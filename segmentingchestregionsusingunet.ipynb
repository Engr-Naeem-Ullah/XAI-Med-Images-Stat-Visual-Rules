{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U-Net model for segmentation with Resnet50 as a backbone "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21165 images belonging to 4 classes.\n",
      "Found 21165 images belonging to 4 classes.\n",
      "Epoch 1/5\n",
      "\u001b[1m2646/2646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11s/step - f1_score: 0.9653 - loss: 0.0363 - mean_io_u: 0.8486 - precision: 0.9657 - recall: 0.9687 \n",
      "Epoch 1: val_mean_io_u improved from -inf to 0.97891, saving model to saved_model/best_model.keras\n",
      "\u001b[1m2646/2646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28379s\u001b[0m 11s/step - f1_score: 0.9653 - loss: 0.0363 - mean_io_u: 0.8486 - precision: 0.9657 - recall: 0.9687 - val_f1_score: 0.9885 - val_loss: 0.0117 - val_mean_io_u: 0.9789 - val_precision: 0.9832 - val_recall: 0.9938\n",
      "Epoch 2/5\n",
      "\u001b[1m2646/2646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11s/step - f1_score: 0.9868 - loss: 0.0133 - mean_io_u: 0.9737 - precision: 0.9875 - recall: 0.9862 \n",
      "Epoch 2: val_mean_io_u improved from 0.97891 to 0.98810, saving model to saved_model/best_model.keras\n",
      "\u001b[1m2646/2646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28081s\u001b[0m 11s/step - f1_score: 0.9868 - loss: 0.0133 - mean_io_u: 0.9737 - precision: 0.9875 - recall: 0.9862 - val_f1_score: 0.9903 - val_loss: 0.0097 - val_mean_io_u: 0.9881 - val_precision: 0.9831 - val_recall: 0.9976\n",
      "Epoch 3/5\n",
      "\u001b[1m2207/2646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1:18:43\u001b[0m 11s/step - f1_score: 0.9881 - loss: 0.0120 - mean_io_u: 0.9797 - precision: 0.9886 - recall: 0.9877"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import Conv2D, UpSampling2D, concatenate\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import MeanIoU\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create directories for model saving\n",
    "model_save_dir = 'saved_model/'\n",
    "if not os.path.exists(model_save_dir):\n",
    "    os.makedirs(model_save_dir)\n",
    "\n",
    "# Dice loss function\n",
    "def dice_loss(y_true, y_pred):\n",
    "    smooth = 1.0\n",
    "    y_true_f = tf.reshape(y_true, [-1])\n",
    "    y_pred_f = tf.reshape(y_pred, [-1])\n",
    "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
    "    return 1 - (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n",
    "\n",
    "# Custom metric functions\n",
    "def precision(y_true, y_pred):\n",
    "    y_pred = tf.round(y_pred)\n",
    "    true_positives = tf.reduce_sum(y_true * y_pred)\n",
    "    predicted_positives = tf.reduce_sum(y_pred)\n",
    "    return true_positives / (predicted_positives + tf.keras.backend.epsilon())\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    y_pred = tf.round(y_pred)\n",
    "    true_positives = tf.reduce_sum(y_true * y_pred)\n",
    "    possible_positives = tf.reduce_sum(y_true)\n",
    "    return true_positives / (possible_positives + tf.keras.backend.epsilon())\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    prec = precision(y_true, y_pred)\n",
    "    rec = recall(y_true, y_pred)\n",
    "    return 2 * (prec * rec) / (prec + rec + tf.keras.backend.epsilon())\n",
    "\n",
    "# Define the U-Net model with ResNet50 backbone\n",
    "def unet_model(input_size=(256, 256, 3)):\n",
    "    inputs = Input(input_size)\n",
    "\n",
    "    # Pre-trained ResNet50 encoder (backbone)\n",
    "    resnet_base = ResNet50(weights='imagenet', include_top=False, input_tensor=inputs)\n",
    "\n",
    "    # Extract specific layers from ResNet50 to use in U-Net\n",
    "    c1 = resnet_base.get_layer('conv1_relu').output  # 128x128, 64 filters\n",
    "    c2 = resnet_base.get_layer('conv2_block3_out').output  # 64x64, 256 filters\n",
    "    c3 = resnet_base.get_layer('conv3_block4_out').output  # 32x32, 512 filters\n",
    "    c4 = resnet_base.get_layer('conv4_block6_out').output  # 16x16, 1024 filters\n",
    "    c5 = resnet_base.get_layer('conv5_block3_out').output  # 8x8, 2048 filters\n",
    "\n",
    "    # Decoder part (upsampling)\n",
    "    u6 = UpSampling2D(size=(2, 2))(c5)\n",
    "    u6 = Conv2D(512, 3, activation='relu', padding='same')(u6)\n",
    "    u6 = concatenate([u6, c4])\n",
    "\n",
    "    u7 = UpSampling2D(size=(2, 2))(u6)\n",
    "    u7 = Conv2D(256, 3, activation='relu', padding='same')(u7)\n",
    "    u7 = concatenate([u7, c3])\n",
    "\n",
    "    u8 = UpSampling2D(size=(2, 2))(u7)\n",
    "    u8 = Conv2D(128, 3, activation='relu', padding='same')(u8)\n",
    "    u8 = concatenate([u8, c2])\n",
    "\n",
    "    u9 = UpSampling2D(size=(2, 2))(u8)\n",
    "    u9 = Conv2D(64, 3, activation='relu', padding='same')(u9)\n",
    "    u9 = concatenate([u9, c1])\n",
    "\n",
    "    u10 = UpSampling2D(size=(2, 2))(u9)\n",
    "    u10 = Conv2D(32, 3, activation='relu', padding='same')(u10)\n",
    "\n",
    "    outputs = Conv2D(1, 1, activation='sigmoid')(u10)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-4), \n",
    "                  loss=dice_loss, \n",
    "                  metrics=[MeanIoU(num_classes=2), precision, recall, f1_score])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Data preprocessing and augmentation\n",
    "def load_data(image_dir, mask_dir, target_size=(256, 256)):\n",
    "    # ImageDataGenerator for loading and augmenting images\n",
    "    image_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    mask_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    # Load images and masks\n",
    "    image_generator = image_datagen.flow_from_directory(\n",
    "        image_dir,\n",
    "        target_size=target_size,\n",
    "        class_mode=None,  # No labels for images (unsupervised for this step)\n",
    "        color_mode='rgb',  # Use 'grayscale' if working with grayscale images\n",
    "        batch_size=8,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    mask_generator = mask_datagen.flow_from_directory(\n",
    "        mask_dir,\n",
    "        target_size=target_size,\n",
    "        class_mode=None,  # Masks are also unlabeled\n",
    "        color_mode='grayscale',  # Masks are typically grayscale\n",
    "        batch_size=8,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    return image_generator, mask_generator\n",
    "\n",
    "# Combine image and mask generators\n",
    "def combine_generator(image_gen, mask_gen):\n",
    "    while True:\n",
    "        imgs = next(image_gen)\n",
    "        masks = next(mask_gen)\n",
    "        yield imgs, masks\n",
    "\n",
    "# Example of loading dataset\n",
    "mask_dir = 'E:/Abroad period research/new idea implementation codes/Original dataset/Masks'  # Path to your mask directory\n",
    "image_dir = 'E:/Abroad period research/new idea implementation codes/Original dataset/Images'  # Path to your image directory\n",
    "\n",
    "# Load images and masks using ImageDataGenerator\n",
    "image_gen, mask_gen = load_data(image_dir, mask_dir)\n",
    "\n",
    "# Combine images and masks for training\n",
    "combined_gen = combine_generator(image_gen, mask_gen)\n",
    "\n",
    "# Load a batch of data for training\n",
    "images, masks = next(combined_gen)  # Get a small batch of images and masks\n",
    "X_train, X_val, y_train, y_val = train_test_split(images, masks, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the U-Net model\n",
    "model = unet_model(input_size=(256, 256, 3))\n",
    "\n",
    "# Create a checkpoint to save the model with the best validation IoU\n",
    "checkpoint = ModelCheckpoint(\n",
    "    os.path.join(model_save_dir, 'best_model.keras'),\n",
    "    monitor='val_mean_io_u',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(combined_gen, \n",
    "                    steps_per_epoch=len(image_gen), \n",
    "                    epochs=5, \n",
    "                    validation_data=(X_val, y_val), \n",
    "                    callbacks=[checkpoint])\n",
    "\n",
    "# Save the final model\n",
    "model.save(os.path.join(model_save_dir, 'final_model.keras'))\n",
    "\n",
    "# Plot training metrics\n",
    "def plot_metrics(history):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for metric in ['loss', 'val_loss', 'mean_io_u', 'val_mean_io_u', 'precision', 'val_precision', 'recall', 'val_recall', 'f1_score', 'val_f1_score']:\n",
    "        if metric in history.history:\n",
    "            plt.plot(history.history[metric], label=metric)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Metric Value\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Training and Validation Metrics\")\n",
    "    plt.show()\n",
    "\n",
    "plot_metrics(history)\n",
    "\n",
    "# Example prediction on validation data\n",
    "predictions = model.predict(X_val)\n",
    "\n",
    "# Display predictions, ground truth, and original images side-by-side\n",
    "def display_predictions(X_val, y_val, predictions):\n",
    "    plt.figure(figsize=(10, 15))\n",
    "    for i in range(3):  # Display a few sample images\n",
    "        plt.subplot(3, 3, i * 3 + 1)\n",
    "        plt.imshow(X_val[i])\n",
    "        plt.title(\"Original Image\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.subplot(3, 3, i * 3 + 2)\n",
    "        plt.imshow(y_val[i].squeeze(), cmap='gray')\n",
    "        plt.title(\"Ground Truth Mask\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.subplot(3, 3, i * 3 + 3)\n",
    "        plt.imshow(predictions[i].squeeze(), cmap='gray')\n",
    "        plt.title(\"Predicted Mask\")\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "display_predictions(X_val, y_val, predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cropping images based on predicted masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define paths\n",
    "model_path = 'E:/Abroad period research/new idea implementation codes/saved_model/unet_resnet50_final.keras'  # Trained model path\n",
    "image_dir = 'E:/Abroad period research/new idea implementation codes/Original dataset/Images'  # Original images\n",
    "output_cropped_dir = 'E:/Abroad period research/new idea implementation codes/Unet_Segmented_Dataset'  # Output cropped images\n",
    "output_mask_dir = 'E:/Abroad period research/new idea implementation codes/Predicted_Masks'  # Output masks\n",
    "class_folders = ['COVID', 'Viral Pneumonia', 'Lung_Opacity', 'Normal']  # Class names in your dataset\n",
    "\n",
    "# Dice loss function\n",
    "def dice_loss(y_true, y_pred):\n",
    "    smooth = 1.0\n",
    "    y_true_f = tf.reshape(y_true, [-1])\n",
    "    y_pred_f = tf.reshape(y_pred, [-1])\n",
    "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
    "    return 1 - (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n",
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model(model_path, custom_objects={'dice_loss': dice_loss})\n",
    "\n",
    "# Function to load images\n",
    "def load_image(image_path, target_size=(256, 256)):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.resize(img, target_size)\n",
    "    img = img / 255.0  # Normalize the image\n",
    "    return img\n",
    "\n",
    "# Function to postprocess mask and crop chest region\n",
    "def crop_chest_region(original_image, predicted_mask, threshold=0.5):\n",
    "    binary_mask = (predicted_mask > threshold).astype(np.uint8)  # Binary mask\n",
    "    contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    if len(contours) == 0:\n",
    "        return original_image  # If no contours, return original image\n",
    "\n",
    "    # Find bounding box of largest contour\n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "    x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "    \n",
    "    # Crop the chest region from the original image\n",
    "    cropped_image = original_image[y:y+h, x:x+w]\n",
    "    return cropped_image\n",
    "\n",
    "# Create output directories for cropped images and masks\n",
    "os.makedirs(output_cropped_dir, exist_ok=True)\n",
    "os.makedirs(output_mask_dir, exist_ok=True)\n",
    "for class_name in class_folders:\n",
    "    os.makedirs(os.path.join(output_cropped_dir, class_name), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_mask_dir, class_name), exist_ok=True)\n",
    "\n",
    "# Process images in each class folder\n",
    "for class_name in class_folders:\n",
    "    class_image_dir = os.path.join(image_dir, class_name)\n",
    "    class_cropped_dir = os.path.join(output_cropped_dir, class_name)\n",
    "    class_mask_dir = os.path.join(output_mask_dir, class_name)\n",
    "    \n",
    "    for img_filename in os.listdir(class_image_dir):\n",
    "        img_path = os.path.join(class_image_dir, img_filename)\n",
    "        original_img = cv2.imread(img_path)\n",
    "        \n",
    "        # Resize and predict mask\n",
    "        img_for_pred = load_image(img_path).reshape(1, 256, 256, 3)\n",
    "        predicted_mask = model.predict(img_for_pred)[0, :, :, 0]\n",
    "        \n",
    "        # Resize mask to original image size and save\n",
    "        predicted_mask_resized = cv2.resize(predicted_mask, (original_img.shape[1], original_img.shape[0]))\n",
    "        mask_output_path = os.path.join(class_mask_dir, img_filename)\n",
    "        cv2.imwrite(mask_output_path, (predicted_mask_resized * 255).astype(np.uint8))  # Save mask as grayscale image\n",
    "        \n",
    "        # Crop chest region based on predicted mask\n",
    "        cropped_image = crop_chest_region(original_img, predicted_mask_resized)\n",
    "        \n",
    "        # Save the cropped image\n",
    "        cropped_output_path = os.path.join(class_cropped_dir, img_filename)\n",
    "        cv2.imwrite(cropped_output_path, cropped_image)\n",
    "\n",
    "print(f\"Segmented and cropped images have been saved to '{output_cropped_dir}'\")\n",
    "print(f\"Predicted masks have been saved to '{output_mask_dir}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To accurately capture the entire chest region in the cropped images based on the mask, the cropping function needs to consider the outer bounds of all mask regions, ensuring both sides of the chest are included. Here’s a modified cropping code to achieve this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Define paths\n",
    "image_dir = 'E:/Abroad period research/new idea implementation codes/Original dataset/Images'  # Original images\n",
    "output_mask_dir = 'E:/Abroad period research/new idea implementation codes/Predicted_Masks'  # Masks directory\n",
    "output_cropped_dir = 'E:/Abroad period research/new idea implementation codes/fullchest_Unet_Segmented_Dataset'  # Cropped output images\n",
    "class_folders = ['COVID', 'Viral Pneumonia', 'Lung_Opacity', 'Normal']  # Class names in your dataset\n",
    "\n",
    "# Function to crop chest region based on the mask\n",
    "def crop_chest_region(original_image, mask, threshold=0.5):\n",
    "    # Binary mask for contours\n",
    "    binary_mask = (mask > threshold).astype(np.uint8)\n",
    "    contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    if len(contours) == 0:\n",
    "        return original_image  # No contours, return original image\n",
    "\n",
    "    # Find the bounding rectangle that covers all contours (chest region)\n",
    "    x, y, w, h = cv2.boundingRect(np.vstack(contours))\n",
    "    cropped_image = original_image[y:y+h, x:x+w]\n",
    "\n",
    "    return cropped_image\n",
    "\n",
    "# Create directories for cropped images\n",
    "for class_name in class_folders:\n",
    "    os.makedirs(os.path.join(output_cropped_dir, class_name), exist_ok=True)\n",
    "\n",
    "# Process each image based on the mask\n",
    "for class_name in class_folders:\n",
    "    class_image_dir = os.path.join(image_dir, class_name)\n",
    "    class_mask_dir = os.path.join(output_mask_dir, class_name)\n",
    "    class_cropped_dir = os.path.join(output_cropped_dir, class_name)\n",
    "\n",
    "    for img_filename in os.listdir(class_image_dir):\n",
    "        # Load the original image and corresponding mask\n",
    "        img_path = os.path.join(class_image_dir, img_filename)\n",
    "        mask_path = os.path.join(class_mask_dir, img_filename)\n",
    "        \n",
    "        original_img = cv2.imread(img_path)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)  # Load mask in grayscale\n",
    "\n",
    "        # Ensure mask and image sizes match\n",
    "        mask_resized = cv2.resize(mask, (original_img.shape[1], original_img.shape[0]))\n",
    "\n",
    "        # Crop chest region based on mask\n",
    "        cropped_image = crop_chest_region(original_img, mask_resized)\n",
    "\n",
    "        # Save the cropped image\n",
    "        cropped_output_path = os.path.join(class_cropped_dir, img_filename)\n",
    "        cv2.imwrite(cropped_output_path, cropped_image)\n",
    "\n",
    "print(f\"Chest regions have been accurately cropped and saved to '{output_cropped_dir}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U-Net model with mobilenetv2 as backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import Conv2D, UpSampling2D, concatenate\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import MeanIoU\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Create directories for model saving\n",
    "model_save_dir = 'saved_model/'\n",
    "if not os.path.exists(model_save_dir):\n",
    "    os.makedirs(model_save_dir)\n",
    "\n",
    "# Dice loss function\n",
    "def dice_loss(y_true, y_pred):\n",
    "    smooth = 1.0\n",
    "    y_true_f = tf.reshape(y_true, [-1])\n",
    "    y_pred_f = tf.reshape(y_pred, [-1])\n",
    "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
    "    return 1 - (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n",
    "\n",
    "# Define the U-Net model with MobileNetV2 backbone\n",
    "def unet_model(input_size=(256, 256, 3)):\n",
    "    inputs = Input(input_size)\n",
    "\n",
    "    # Pre-trained MobileNetV2 encoder (backbone)\n",
    "    mobilenet_base = MobileNetV2(weights='imagenet', include_top=False, input_tensor=inputs)\n",
    "\n",
    "    # Extract specific layers from MobileNetV2 to use in U-Net\n",
    "    c1 = mobilenet_base.get_layer('block_1_expand_relu').output  # 128x128\n",
    "    c2 = mobilenet_base.get_layer('block_3_expand_relu').output  # 64x64\n",
    "    c3 = mobilenet_base.get_layer('block_6_expand_relu').output  # 32x32\n",
    "    c4 = mobilenet_base.get_layer('block_13_expand_relu').output  # 16x16\n",
    "    c5 = mobilenet_base.get_layer('block_16_project').output  # 8x8\n",
    "\n",
    "    # Decoder part (upsampling)\n",
    "    u6 = UpSampling2D(size=(2, 2))(c5)\n",
    "    u6 = Conv2D(512, 3, activation='relu', padding='same')(u6)\n",
    "    u6 = concatenate([u6, c4])\n",
    "\n",
    "    u7 = UpSampling2D(size=(2, 2))(u6)\n",
    "    u7 = Conv2D(256, 3, activation='relu', padding='same')(u7)\n",
    "    u7 = concatenate([u7, c3])\n",
    "\n",
    "    u8 = UpSampling2D(size=(2, 2))(u7)\n",
    "    u8 = Conv2D(128, 3, activation='relu', padding='same')(u8)\n",
    "    u8 = concatenate([u8, c2])\n",
    "\n",
    "    u9 = UpSampling2D(size=(2, 2))(u8)\n",
    "    u9 = Conv2D(64, 3, activation='relu', padding='same')(u9)\n",
    "    u9 = concatenate([u9, c1])\n",
    "\n",
    "    u10 = UpSampling2D(size=(2, 2))(u9)\n",
    "    u10 = Conv2D(32, 3, activation='relu', padding='same')(u10)\n",
    "\n",
    "    outputs = Conv2D(1, 1, activation='sigmoid')(u10)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-4), loss=dice_loss, metrics=[MeanIoU(num_classes=2)])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Data preprocessing and augmentation\n",
    "def load_data(image_dir, mask_dir, target_size=(256, 256)):\n",
    "    # ImageDataGenerator for loading and augmenting images\n",
    "    image_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    mask_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    # Load images and masks\n",
    "    image_generator = image_datagen.flow_from_directory(\n",
    "        image_dir,\n",
    "        target_size=target_size,\n",
    "        class_mode=None,  # No labels for images (unsupervised for this step)\n",
    "        color_mode='rgb',  # Use 'grayscale' if working with grayscale images\n",
    "        batch_size=8,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    mask_generator = mask_datagen.flow_from_directory(\n",
    "        mask_dir,\n",
    "        target_size=target_size,\n",
    "        class_mode=None,  # Masks are also unlabeled\n",
    "        color_mode='grayscale',  # Masks are typically grayscale\n",
    "        batch_size=8,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    return image_generator, mask_generator\n",
    "\n",
    "# Combine image and mask generators\n",
    "def combine_generator(image_gen, mask_gen):\n",
    "    while True:\n",
    "        imgs = next(image_gen)\n",
    "        masks = next(mask_gen)\n",
    "        yield imgs, masks\n",
    "\n",
    "# Example of loading dataset\n",
    "mask_dir = 'E:/Abroad period research/new idea implementation codes/Original dataset/Masks'  # Path to your mask directory\n",
    "image_dir = 'E:/Abroad period research/new idea implementation codes/Original dataset/Images'  # Path to your image directory\n",
    "\n",
    "# Load images and masks using ImageDataGenerator\n",
    "image_gen, mask_gen = load_data(image_dir, mask_dir)\n",
    "\n",
    "# Combine images and masks for training\n",
    "combined_gen = combine_generator(image_gen, mask_gen)\n",
    "\n",
    "# Load a batch of data for training\n",
    "images, masks = next(combined_gen)  # Get a small batch of images and masks\n",
    "X_train, X_val, y_train, y_val = train_test_split(images, masks, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the U-Net model\n",
    "model = unet_model(input_size=(256, 256, 3))\n",
    "\n",
    "# Create a checkpoint to save the model with the best validation IoU\n",
    "checkpoint = ModelCheckpoint(\n",
    "    os.path.join(model_save_dir, 'best_model.keras'),  # Changed the extension to .keras\n",
    "    monitor='val_mean_io_u',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(combined_gen, \n",
    "          steps_per_epoch=len(image_gen), \n",
    "          epochs=1, \n",
    "          validation_data=(X_val, y_val), \n",
    "          callbacks=[checkpoint])\n",
    "\n",
    "# Save the final model\n",
    "model.save(os.path.join(model_save_dir, 'final_model.keras'))\n",
    "\n",
    "# Example prediction on validation data\n",
    "predictions = model.predict(X_val)\n",
    "\n",
    "# Save predictions as images for inspection (optional)\n",
    "pred_dir = 'predictions/'\n",
    "if not os.path.exists(pred_dir):\n",
    "    os.makedirs(pred_dir)\n",
    "\n",
    "for i, pred in enumerate(predictions):\n",
    "    pred_image = (pred * 255).astype(np.uint8)\n",
    "    tf.keras.preprocessing.image.save_img(f'{pred_dir}/pred_{i}.png', pred_image)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
