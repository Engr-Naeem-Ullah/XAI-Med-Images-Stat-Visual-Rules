{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 6000 | Classes: ['Glaucoma', 'Normal']\n",
      "Train samples: 5100 | Test samples: 900\n",
      "Number of classes: 2\n",
      "Epoch 1/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m287s\u001b[0m 2s/step - accuracy: 0.4924 - loss: 0.7378 - val_accuracy: 0.6644 - val_loss: 0.6816\n",
      "Epoch 2/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 1s/step - accuracy: 0.5321 - loss: 0.6897 - val_accuracy: 0.6211 - val_loss: 0.6798\n",
      "Epoch 3/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m245s\u001b[0m 2s/step - accuracy: 0.5659 - loss: 0.6847 - val_accuracy: 0.5656 - val_loss: 0.6753\n",
      "Epoch 4/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m246s\u001b[0m 2s/step - accuracy: 0.5593 - loss: 0.6813 - val_accuracy: 0.6011 - val_loss: 0.6662\n",
      "Epoch 5/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m244s\u001b[0m 2s/step - accuracy: 0.5718 - loss: 0.6758 - val_accuracy: 0.6800 - val_loss: 0.6518\n",
      "Epoch 6/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m249s\u001b[0m 2s/step - accuracy: 0.5835 - loss: 0.6713 - val_accuracy: 0.6767 - val_loss: 0.6463\n",
      "Epoch 7/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m244s\u001b[0m 2s/step - accuracy: 0.5991 - loss: 0.6624 - val_accuracy: 0.6856 - val_loss: 0.6397\n",
      "Epoch 8/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m248s\u001b[0m 2s/step - accuracy: 0.6053 - loss: 0.6563 - val_accuracy: 0.6756 - val_loss: 0.6413\n",
      "Epoch 9/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m248s\u001b[0m 2s/step - accuracy: 0.6219 - loss: 0.6541 - val_accuracy: 0.6744 - val_loss: 0.6237\n",
      "Epoch 10/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m247s\u001b[0m 2s/step - accuracy: 0.6014 - loss: 0.6526 - val_accuracy: 0.6800 - val_loss: 0.6221\n",
      "Epoch 11/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m244s\u001b[0m 2s/step - accuracy: 0.6199 - loss: 0.6477 - val_accuracy: 0.6767 - val_loss: 0.6102\n",
      "Epoch 12/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m243s\u001b[0m 2s/step - accuracy: 0.6240 - loss: 0.6447 - val_accuracy: 0.6778 - val_loss: 0.6133\n",
      "Epoch 13/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 1s/step - accuracy: 0.6347 - loss: 0.6428 - val_accuracy: 0.6956 - val_loss: 0.6117\n",
      "Epoch 14/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 1s/step - accuracy: 0.6364 - loss: 0.6375 - val_accuracy: 0.6911 - val_loss: 0.6058\n",
      "Epoch 15/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 1s/step - accuracy: 0.6390 - loss: 0.6345 - val_accuracy: 0.6633 - val_loss: 0.6164\n",
      "Epoch 16/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 1s/step - accuracy: 0.6249 - loss: 0.6426 - val_accuracy: 0.6844 - val_loss: 0.5988\n",
      "Epoch 17/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m211s\u001b[0m 1s/step - accuracy: 0.6215 - loss: 0.6403 - val_accuracy: 0.6944 - val_loss: 0.6019\n",
      "Epoch 18/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 1s/step - accuracy: 0.6264 - loss: 0.6436 - val_accuracy: 0.6878 - val_loss: 0.6093\n",
      "Epoch 19/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 1s/step - accuracy: 0.6342 - loss: 0.6374 - val_accuracy: 0.6744 - val_loss: 0.6074\n",
      "Epoch 20/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 1s/step - accuracy: 0.6269 - loss: 0.6360 - val_accuracy: 0.6878 - val_loss: 0.5913\n",
      "Epoch 21/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 1s/step - accuracy: 0.6308 - loss: 0.6383 - val_accuracy: 0.6856 - val_loss: 0.5992\n",
      "Epoch 22/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 1s/step - accuracy: 0.6364 - loss: 0.6360 - val_accuracy: 0.6889 - val_loss: 0.5902\n",
      "Epoch 23/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 1s/step - accuracy: 0.6388 - loss: 0.6283 - val_accuracy: 0.6911 - val_loss: 0.5949\n",
      "Epoch 24/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 1s/step - accuracy: 0.6403 - loss: 0.6256 - val_accuracy: 0.6900 - val_loss: 0.5936\n",
      "Epoch 25/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 1s/step - accuracy: 0.6436 - loss: 0.6308 - val_accuracy: 0.6889 - val_loss: 0.5979\n",
      "Epoch 26/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 1s/step - accuracy: 0.6424 - loss: 0.6336 - val_accuracy: 0.7000 - val_loss: 0.5902\n",
      "Epoch 27/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 1s/step - accuracy: 0.6362 - loss: 0.6316 - val_accuracy: 0.6844 - val_loss: 0.5936\n",
      "Epoch 28/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 1s/step - accuracy: 0.6355 - loss: 0.6336 - val_accuracy: 0.6900 - val_loss: 0.5958\n",
      "Epoch 29/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 1s/step - accuracy: 0.6347 - loss: 0.6328 - val_accuracy: 0.6911 - val_loss: 0.5877\n",
      "Epoch 30/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 1s/step - accuracy: 0.6419 - loss: 0.6309 - val_accuracy: 0.6911 - val_loss: 0.5807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 1s/step\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Glaucoma       0.71      0.64      0.67       450\n",
      "      Normal       0.67      0.75      0.71       450\n",
      "\n",
      "    accuracy                           0.69       900\n",
      "   macro avg       0.69      0.69      0.69       900\n",
      "weighted avg       0.69      0.69      0.69       900\n",
      "\n",
      "Final validation accuracy: 0.6911\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Dataset directory\n",
    "data_dir = r'E:\\Abroad period research\\Medical images analysis paper implementation codes\\Second part of the paper\\testing on Generated Eye Dataset for Glaucoma Detection\\10 features results\\Acrima'\n",
    "\n",
    "# Parameters\n",
    "IMG_SIZE = (150, 150)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 30\n",
    "TEST_SIZE = 0.15  # 15% for testing\n",
    "\n",
    "# Load image paths and labels\n",
    "def load_image_paths_and_labels(base_dir):\n",
    "    class_names = sorted([d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))])\n",
    "    all_filepaths = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for idx, class_name in enumerate(class_names):\n",
    "        class_folder = os.path.join(base_dir, class_name)\n",
    "        file_names = [f for f in os.listdir(class_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        \n",
    "        for file_name in file_names:\n",
    "            all_filepaths.append(os.path.join(class_folder, file_name))\n",
    "            all_labels.append(idx)  # numeric label\n",
    "            \n",
    "    return np.array(all_filepaths), np.array(all_labels), class_names\n",
    "\n",
    "filepaths, labels, class_names = load_image_paths_and_labels(data_dir)\n",
    "print(f\"Total images: {len(filepaths)} | Classes: {class_names}\")\n",
    "\n",
    "# Split data\n",
    "train_paths, test_paths, train_labels, test_labels = train_test_split(\n",
    "    filepaths, labels, test_size=TEST_SIZE, stratify=labels, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(train_paths)} | Test samples: {len(test_paths)}\")\n",
    "\n",
    "# Preprocess function\n",
    "def load_and_preprocess_image(path, label):\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)  # handle jpeg/png\n",
    "    image = tf.image.resize(image, IMG_SIZE)\n",
    "    image = image / 255.0  # normalize to [0,1]\n",
    "    return image, label\n",
    "\n",
    "# TF datasets\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n",
    "train_ds = train_ds.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_ds = train_ds.shuffle(buffer_size=1000).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_paths, test_labels))\n",
    "test_ds = test_ds.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# Model setup\n",
    "base_model = tf.keras.applications.ResNet50(\n",
    "    input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3),\n",
    "    include_top=False,\n",
    "    weights='imagenet'\n",
    ")\n",
    "base_model.trainable = False  # freeze\n",
    "\n",
    "# Number of classes\n",
    "num_classes = len(class_names)\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# Final activation and loss\n",
    "if num_classes == 2:\n",
    "    final_activation = 'sigmoid'\n",
    "    final_units = 1\n",
    "    loss_function = 'binary_crossentropy'\n",
    "else:\n",
    "    final_activation = 'softmax'\n",
    "    final_units = num_classes\n",
    "    loss_function = 'sparse_categorical_crossentropy'  # because labels are integers\n",
    "\n",
    "# Build the model\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(final_units, activation=final_activation)\n",
    "])\n",
    "\n",
    "# Compile\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=loss_function,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=test_ds\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save('glaucoma_detection_resnet50.h5')\n",
    "\n",
    "# Evaluate & Predict\n",
    "# Get test data arrays for predictions\n",
    "test_images = []\n",
    "test_labels_list = []\n",
    "\n",
    "for image, label in test_ds.unbatch():\n",
    "    test_images.append(image.numpy())\n",
    "    test_labels_list.append(label.numpy())\n",
    "\n",
    "test_images = np.array(test_images)\n",
    "test_labels_list = np.array(test_labels_list)\n",
    "\n",
    "# Predictions\n",
    "predictions = model.predict(test_images)\n",
    "\n",
    "if num_classes == 2:\n",
    "    predicted_classes = (predictions > 0.5).astype(\"int32\").flatten()\n",
    "else:\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Classification report\n",
    "report = classification_report(test_labels_list, predicted_classes, target_names=class_names)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "# Final validation accuracy\n",
    "val_acc = history.history['val_accuracy'][-1]\n",
    "print(f\"Final validation accuracy: {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 2400 | Classes: ['Benign', 'Malignant']\n",
      "Train samples: 2040 | Test samples: 360\n",
      "Number of classes: 2\n",
      "Epoch 1/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 1s/step - accuracy: 0.5108 - loss: 0.7331 - val_accuracy: 0.7333 - val_loss: 0.6554\n",
      "Epoch 2/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 1s/step - accuracy: 0.6022 - loss: 0.6679 - val_accuracy: 0.6056 - val_loss: 0.6307\n",
      "Epoch 3/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 1s/step - accuracy: 0.6580 - loss: 0.6382 - val_accuracy: 0.7889 - val_loss: 0.5978\n",
      "Epoch 4/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 1s/step - accuracy: 0.6962 - loss: 0.6111 - val_accuracy: 0.7278 - val_loss: 0.5809\n",
      "Epoch 5/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 1s/step - accuracy: 0.7231 - loss: 0.5729 - val_accuracy: 0.7417 - val_loss: 0.5420\n",
      "Epoch 6/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 2s/step - accuracy: 0.7534 - loss: 0.5444 - val_accuracy: 0.7694 - val_loss: 0.5093\n",
      "Epoch 7/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 1s/step - accuracy: 0.7472 - loss: 0.5283 - val_accuracy: 0.8306 - val_loss: 0.4803\n",
      "Epoch 8/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 2s/step - accuracy: 0.7867 - loss: 0.4906 - val_accuracy: 0.7972 - val_loss: 0.4837\n",
      "Epoch 9/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 2s/step - accuracy: 0.7897 - loss: 0.4788 - val_accuracy: 0.8417 - val_loss: 0.4385\n",
      "Epoch 10/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 2s/step - accuracy: 0.7918 - loss: 0.4548 - val_accuracy: 0.8472 - val_loss: 0.4205\n",
      "Epoch 11/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 1s/step - accuracy: 0.8230 - loss: 0.4353 - val_accuracy: 0.8472 - val_loss: 0.4035\n",
      "Epoch 12/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 2s/step - accuracy: 0.8121 - loss: 0.4081 - val_accuracy: 0.8472 - val_loss: 0.3891\n",
      "Epoch 13/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 2s/step - accuracy: 0.8438 - loss: 0.3763 - val_accuracy: 0.8472 - val_loss: 0.3862\n",
      "Epoch 14/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 2s/step - accuracy: 0.8386 - loss: 0.3670 - val_accuracy: 0.8139 - val_loss: 0.3747\n",
      "Epoch 15/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 1s/step - accuracy: 0.8332 - loss: 0.3841 - val_accuracy: 0.8250 - val_loss: 0.3782\n",
      "Epoch 16/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 1s/step - accuracy: 0.8361 - loss: 0.3705 - val_accuracy: 0.8528 - val_loss: 0.3492\n",
      "Epoch 17/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 1s/step - accuracy: 0.8716 - loss: 0.3226 - val_accuracy: 0.8500 - val_loss: 0.3483\n",
      "Epoch 18/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 1s/step - accuracy: 0.8495 - loss: 0.3466 - val_accuracy: 0.8583 - val_loss: 0.3351\n",
      "Epoch 19/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 1s/step - accuracy: 0.8607 - loss: 0.3267 - val_accuracy: 0.8639 - val_loss: 0.3221\n",
      "Epoch 20/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 1s/step - accuracy: 0.8473 - loss: 0.3365 - val_accuracy: 0.8722 - val_loss: 0.3177\n",
      "Epoch 21/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 2s/step - accuracy: 0.8622 - loss: 0.3274 - val_accuracy: 0.8639 - val_loss: 0.3290\n",
      "Epoch 22/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 2s/step - accuracy: 0.8672 - loss: 0.3091 - val_accuracy: 0.8722 - val_loss: 0.3063\n",
      "Epoch 23/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 2s/step - accuracy: 0.8647 - loss: 0.3029 - val_accuracy: 0.8778 - val_loss: 0.2913\n",
      "Epoch 24/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 2s/step - accuracy: 0.8668 - loss: 0.3163 - val_accuracy: 0.8750 - val_loss: 0.2979\n",
      "Epoch 25/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 2s/step - accuracy: 0.8801 - loss: 0.2907 - val_accuracy: 0.8778 - val_loss: 0.2843\n",
      "Epoch 26/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 2s/step - accuracy: 0.8670 - loss: 0.2989 - val_accuracy: 0.8556 - val_loss: 0.3146\n",
      "Epoch 27/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 2s/step - accuracy: 0.8632 - loss: 0.3056 - val_accuracy: 0.8667 - val_loss: 0.3034\n",
      "Epoch 28/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 2s/step - accuracy: 0.8676 - loss: 0.2973 - val_accuracy: 0.8583 - val_loss: 0.3107\n",
      "Epoch 29/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 1s/step - accuracy: 0.8739 - loss: 0.2807 - val_accuracy: 0.8861 - val_loss: 0.2689\n",
      "Epoch 30/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 1s/step - accuracy: 0.8841 - loss: 0.2750 - val_accuracy: 0.8889 - val_loss: 0.2710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 1s/step \n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.85      0.94      0.89       180\n",
      "   Malignant       0.94      0.83      0.88       180\n",
      "\n",
      "    accuracy                           0.89       360\n",
      "   macro avg       0.89      0.89      0.89       360\n",
      "weighted avg       0.89      0.89      0.89       360\n",
      "\n",
      "Final validation accuracy: 0.8889\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Dataset directory\n",
    "data_dir = r'E:\\Abroad period research\\Medical images analysis paper implementation codes\\Second part of the paper\\testing code on brain tumor dataset\\dataset'\n",
    "\n",
    "# Parameters\n",
    "IMG_SIZE = (150, 150)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 30\n",
    "TEST_SIZE = 0.15  # 15% for testing\n",
    "\n",
    "# Load image paths and labels\n",
    "def load_image_paths_and_labels(base_dir):\n",
    "    class_names = sorted([d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))])\n",
    "    all_filepaths = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for idx, class_name in enumerate(class_names):\n",
    "        class_folder = os.path.join(base_dir, class_name)\n",
    "        file_names = [f for f in os.listdir(class_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        \n",
    "        for file_name in file_names:\n",
    "            all_filepaths.append(os.path.join(class_folder, file_name))\n",
    "            all_labels.append(idx)  # numeric label\n",
    "            \n",
    "    return np.array(all_filepaths), np.array(all_labels), class_names\n",
    "\n",
    "filepaths, labels, class_names = load_image_paths_and_labels(data_dir)\n",
    "print(f\"Total images: {len(filepaths)} | Classes: {class_names}\")\n",
    "\n",
    "# Split data\n",
    "train_paths, test_paths, train_labels, test_labels = train_test_split(\n",
    "    filepaths, labels, test_size=TEST_SIZE, stratify=labels, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(train_paths)} | Test samples: {len(test_paths)}\")\n",
    "\n",
    "# Preprocess function\n",
    "def load_and_preprocess_image(path, label):\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)  # handle jpeg/png\n",
    "    image = tf.image.resize(image, IMG_SIZE)\n",
    "    image = image / 255.0  # normalize to [0,1]\n",
    "    return image, label\n",
    "\n",
    "# TF datasets\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n",
    "train_ds = train_ds.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_ds = train_ds.shuffle(buffer_size=1000).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_paths, test_labels))\n",
    "test_ds = test_ds.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# Model setup\n",
    "base_model = tf.keras.applications.ResNet50(\n",
    "    input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3),\n",
    "    include_top=False,\n",
    "    weights='imagenet'\n",
    ")\n",
    "base_model.trainable = False  # freeze\n",
    "\n",
    "# Number of classes\n",
    "num_classes = len(class_names)\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# Final activation and loss\n",
    "if num_classes == 2:\n",
    "    final_activation = 'sigmoid'\n",
    "    final_units = 1\n",
    "    loss_function = 'binary_crossentropy'\n",
    "else:\n",
    "    final_activation = 'softmax'\n",
    "    final_units = num_classes\n",
    "    loss_function = 'sparse_categorical_crossentropy'  # because labels are integers\n",
    "\n",
    "# Build the model\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(final_units, activation=final_activation)\n",
    "])\n",
    "\n",
    "# Compile\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=loss_function,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=test_ds\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save('glaucoma_detection_resnet50.h5')\n",
    "\n",
    "# Evaluate & Predict\n",
    "# Get test data arrays for predictions\n",
    "test_images = []\n",
    "test_labels_list = []\n",
    "\n",
    "for image, label in test_ds.unbatch():\n",
    "    test_images.append(image.numpy())\n",
    "    test_labels_list.append(label.numpy())\n",
    "\n",
    "test_images = np.array(test_images)\n",
    "test_labels_list = np.array(test_labels_list)\n",
    "\n",
    "# Predictions\n",
    "predictions = model.predict(test_images)\n",
    "\n",
    "if num_classes == 2:\n",
    "    predicted_classes = (predictions > 0.5).astype(\"int32\").flatten()\n",
    "else:\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Classification report\n",
    "report = classification_report(test_labels_list, predicted_classes, target_names=class_names, digits=4)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "# Final validation accuracy\n",
    "val_acc = history.history['val_accuracy'][-1]\n",
    "print(f\"Final validation accuracy: {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lung and Colon Cancer Histopathological Images ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 25000 | Classes: ['colon_aca', 'colon_n', 'lung_aca', 'lung_n', 'lung_scc']\n",
      "Train samples: 21250 | Test samples: 3750\n",
      "Number of classes: 5\n",
      "Epoch 1/30\n",
      "\u001b[1m665/665\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1606s\u001b[0m 2s/step - accuracy: 0.2923 - loss: 1.5604 - val_accuracy: 0.5571 - val_loss: 1.2238\n",
      "Epoch 2/30\n",
      "\u001b[1m665/665\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1694s\u001b[0m 3s/step - accuracy: 0.4546 - loss: 1.2542 - val_accuracy: 0.5608 - val_loss: 1.0485\n",
      "Epoch 3/30\n",
      "\u001b[1m665/665\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1000s\u001b[0m 1s/step - accuracy: 0.4847 - loss: 1.1467 - val_accuracy: 0.5893 - val_loss: 0.9634\n",
      "Epoch 4/30\n",
      "\u001b[1m665/665\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m967s\u001b[0m 1s/step - accuracy: 0.5167 - loss: 1.0895 - val_accuracy: 0.5885 - val_loss: 0.9344\n",
      "Epoch 5/30\n",
      "\u001b[1m665/665\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1010s\u001b[0m 2s/step - accuracy: 0.5258 - loss: 1.0539 - val_accuracy: 0.6141 - val_loss: 0.9026\n",
      "Epoch 6/30\n",
      "\u001b[1m665/665\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m935s\u001b[0m 1s/step - accuracy: 0.5492 - loss: 1.0200 - val_accuracy: 0.6064 - val_loss: 0.9279\n",
      "Epoch 7/30\n",
      "\u001b[1m665/665\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m937s\u001b[0m 1s/step - accuracy: 0.5439 - loss: 1.0066 - val_accuracy: 0.6333 - val_loss: 0.8665\n",
      "Epoch 8/30\n",
      "\u001b[1m665/665\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m939s\u001b[0m 1s/step - accuracy: 0.5567 - loss: 0.9809 - val_accuracy: 0.6571 - val_loss: 0.8189\n",
      "Epoch 9/30\n",
      "\u001b[1m665/665\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m943s\u001b[0m 1s/step - accuracy: 0.5682 - loss: 0.9584 - val_accuracy: 0.6635 - val_loss: 0.8053\n",
      "Epoch 10/30\n",
      "\u001b[1m665/665\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m987s\u001b[0m 1s/step - accuracy: 0.5757 - loss: 0.9459 - val_accuracy: 0.6659 - val_loss: 0.8063\n",
      "Epoch 11/30\n",
      "\u001b[1m665/665\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1019s\u001b[0m 2s/step - accuracy: 0.5837 - loss: 0.9319 - val_accuracy: 0.6597 - val_loss: 0.8095\n",
      "Epoch 12/30\n",
      "\u001b[1m665/665\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m960s\u001b[0m 1s/step - accuracy: 0.5853 - loss: 0.9247 - val_accuracy: 0.6776 - val_loss: 0.7735\n",
      "Epoch 13/30\n",
      "\u001b[1m665/665\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m969s\u001b[0m 1s/step - accuracy: 0.5927 - loss: 0.9024 - val_accuracy: 0.6944 - val_loss: 0.7351\n",
      "Epoch 14/30\n",
      "\u001b[1m665/665\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m958s\u001b[0m 1s/step - accuracy: 0.5933 - loss: 0.8961 - val_accuracy: 0.6256 - val_loss: 0.8107\n",
      "Epoch 15/30\n",
      "\u001b[1m665/665\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m957s\u001b[0m 1s/step - accuracy: 0.6024 - loss: 0.8879 - val_accuracy: 0.6816 - val_loss: 0.7415\n",
      "Epoch 16/30\n",
      "\u001b[1m665/665\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m954s\u001b[0m 1s/step - accuracy: 0.5963 - loss: 0.9006 - val_accuracy: 0.6968 - val_loss: 0.7210\n",
      "Epoch 17/30\n",
      "\u001b[1m665/665\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m946s\u001b[0m 1s/step - accuracy: 0.6147 - loss: 0.8646 - val_accuracy: 0.6669 - val_loss: 0.7563\n",
      "Epoch 18/30\n",
      "\u001b[1m665/665\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m943s\u001b[0m 1s/step - accuracy: 0.6129 - loss: 0.8786 - val_accuracy: 0.6827 - val_loss: 0.7445\n",
      "Epoch 19/30\n",
      "\u001b[1m665/665\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m945s\u001b[0m 1s/step - accuracy: 0.6122 - loss: 0.8622 - val_accuracy: 0.6923 - val_loss: 0.7161\n",
      "Epoch 20/30\n",
      "\u001b[1m665/665\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m951s\u001b[0m 1s/step - accuracy: 0.6169 - loss: 0.8592 - val_accuracy: 0.7168 - val_loss: 0.6837\n",
      "Epoch 21/30\n",
      "\u001b[1m665/665\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m952s\u001b[0m 1s/step - accuracy: 0.6152 - loss: 0.8541 - val_accuracy: 0.7197 - val_loss: 0.6996\n",
      "Epoch 22/30\n",
      "\u001b[1m665/665\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m948s\u001b[0m 1s/step - accuracy: 0.6018 - loss: 0.8648 - val_accuracy: 0.6528 - val_loss: 0.7939\n",
      "Epoch 23/30\n",
      "\u001b[1m665/665\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m948s\u001b[0m 1s/step - accuracy: 0.6048 - loss: 0.8670 - val_accuracy: 0.7221 - val_loss: 0.7117\n",
      "Epoch 24/30\n",
      "\u001b[1m665/665\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m958s\u001b[0m 1s/step - accuracy: 0.6118 - loss: 0.8596 - val_accuracy: 0.7083 - val_loss: 0.7271\n",
      "Epoch 25/30\n",
      "\u001b[1m665/665\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m959s\u001b[0m 1s/step - accuracy: 0.6007 - loss: 0.8731 - val_accuracy: 0.7024 - val_loss: 0.7133\n",
      "Epoch 26/30\n",
      "\u001b[1m665/665\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m951s\u001b[0m 1s/step - accuracy: 0.5888 - loss: 0.9073 - val_accuracy: 0.6816 - val_loss: 0.7699\n",
      "Epoch 27/30\n",
      "\u001b[1m665/665\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m939s\u001b[0m 1s/step - accuracy: 0.6025 - loss: 0.8803 - val_accuracy: 0.6829 - val_loss: 0.7562\n",
      "Epoch 28/30\n",
      "\u001b[1m665/665\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m948s\u001b[0m 1s/step - accuracy: 0.5954 - loss: 0.9017 - val_accuracy: 0.7176 - val_loss: 0.7378\n",
      "Epoch 29/30\n",
      "\u001b[1m665/665\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1165s\u001b[0m 2s/step - accuracy: 0.6031 - loss: 0.8978 - val_accuracy: 0.7203 - val_loss: 0.7188\n",
      "Epoch 30/30\n",
      "\u001b[1m665/665\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m972s\u001b[0m 1s/step - accuracy: 0.6141 - loss: 0.8887 - val_accuracy: 0.7237 - val_loss: 0.7082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 1s/step\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   colon_aca     0.7849    0.5693    0.6600       750\n",
      "     colon_n     0.6772    0.8253    0.7440       750\n",
      "    lung_aca     0.6643    0.5013    0.5714       750\n",
      "      lung_n     0.7467    0.8413    0.7912       750\n",
      "    lung_scc     0.7503    0.8813    0.8105       750\n",
      "\n",
      "    accuracy                         0.7237      3750\n",
      "   macro avg     0.7247    0.7237    0.7154      3750\n",
      "weighted avg     0.7247    0.7237    0.7154      3750\n",
      "\n",
      "Final validation accuracy: 0.7237\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Dataset directory\n",
    "data_dir = r'E:\\Abroad period research\\Medical images analysis paper implementation codes\\Second part of the paper\\lung_colon_image_set\\dataset'\n",
    "\n",
    "# Parameters\n",
    "IMG_SIZE = (150, 150)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 30\n",
    "TEST_SIZE = 0.15  # 15% for testing\n",
    "\n",
    "# Load image paths and labels\n",
    "def load_image_paths_and_labels(base_dir):\n",
    "    class_names = sorted([d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))])\n",
    "    all_filepaths = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for idx, class_name in enumerate(class_names):\n",
    "        class_folder = os.path.join(base_dir, class_name)\n",
    "        file_names = [f for f in os.listdir(class_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        \n",
    "        for file_name in file_names:\n",
    "            all_filepaths.append(os.path.join(class_folder, file_name))\n",
    "            all_labels.append(idx)  # numeric label\n",
    "            \n",
    "    return np.array(all_filepaths), np.array(all_labels), class_names\n",
    "\n",
    "filepaths, labels, class_names = load_image_paths_and_labels(data_dir)\n",
    "print(f\"Total images: {len(filepaths)} | Classes: {class_names}\")\n",
    "\n",
    "# Split data\n",
    "train_paths, test_paths, train_labels, test_labels = train_test_split(\n",
    "    filepaths, labels, test_size=TEST_SIZE, stratify=labels, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(train_paths)} | Test samples: {len(test_paths)}\")\n",
    "\n",
    "# Preprocess function\n",
    "def load_and_preprocess_image(path, label):\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)  # handle jpeg/png\n",
    "    image = tf.image.resize(image, IMG_SIZE)\n",
    "    image = image / 255.0  # normalize to [0,1]\n",
    "    return image, label\n",
    "\n",
    "# TF datasets\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n",
    "train_ds = train_ds.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_ds = train_ds.shuffle(buffer_size=1000).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_paths, test_labels))\n",
    "test_ds = test_ds.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# Model setup\n",
    "base_model = tf.keras.applications.ResNet50(\n",
    "    input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3),\n",
    "    include_top=False,\n",
    "    weights='imagenet'\n",
    ")\n",
    "base_model.trainable = False  # freeze\n",
    "\n",
    "# Number of classes\n",
    "num_classes = len(class_names)\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# Final activation and loss\n",
    "if num_classes == 2:\n",
    "    final_activation = 'sigmoid'\n",
    "    final_units = 1\n",
    "    loss_function = 'binary_crossentropy'\n",
    "else:\n",
    "    final_activation = 'softmax'\n",
    "    final_units = num_classes\n",
    "    loss_function = 'sparse_categorical_crossentropy'  # because labels are integers\n",
    "\n",
    "# Build the model\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(final_units, activation=final_activation)\n",
    "])\n",
    "\n",
    "# Compile\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=loss_function,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=test_ds\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save('glaucoma_detection_resnet50.h5')\n",
    "\n",
    "# Evaluate & Predict\n",
    "# Get test data arrays for predictions\n",
    "test_images = []\n",
    "test_labels_list = []\n",
    "\n",
    "for image, label in test_ds.unbatch():\n",
    "    test_images.append(image.numpy())\n",
    "    test_labels_list.append(label.numpy())\n",
    "\n",
    "test_images = np.array(test_images)\n",
    "test_labels_list = np.array(test_labels_list)\n",
    "\n",
    "# Predictions\n",
    "predictions = model.predict(test_images)\n",
    "\n",
    "if num_classes == 2:\n",
    "    predicted_classes = (predictions > 0.5).astype(\"int32\").flatten()\n",
    "else:\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Classification report\n",
    "report = classification_report(test_labels_list, predicted_classes, target_names=class_names, digits=4)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "# Final validation accuracy\n",
    "val_acc = history.history['val_accuracy'][-1]\n",
    "print(f\"Final validation accuracy: {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultrasound Breast Images for Breast Cancer ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 9016 | Classes: ['benign', 'malignant']\n",
      "Train samples: 7663 | Test samples: 1353\n",
      "Number of classes: 2\n",
      "Epoch 1/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m374s\u001b[0m 1s/step - accuracy: 0.5730 - loss: 0.6871 - val_accuracy: 0.7066 - val_loss: 0.6148\n",
      "Epoch 2/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m328s\u001b[0m 1s/step - accuracy: 0.6695 - loss: 0.6144 - val_accuracy: 0.7317 - val_loss: 0.5608\n",
      "Epoch 3/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m334s\u001b[0m 1s/step - accuracy: 0.7141 - loss: 0.5702 - val_accuracy: 0.7243 - val_loss: 0.5457\n",
      "Epoch 4/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m335s\u001b[0m 1s/step - accuracy: 0.7399 - loss: 0.5477 - val_accuracy: 0.7421 - val_loss: 0.5195\n",
      "Epoch 5/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m335s\u001b[0m 1s/step - accuracy: 0.7457 - loss: 0.5333 - val_accuracy: 0.7310 - val_loss: 0.5301\n",
      "Epoch 6/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m338s\u001b[0m 1s/step - accuracy: 0.7486 - loss: 0.5251 - val_accuracy: 0.7664 - val_loss: 0.5018\n",
      "Epoch 7/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m341s\u001b[0m 1s/step - accuracy: 0.7487 - loss: 0.5240 - val_accuracy: 0.7857 - val_loss: 0.4966\n",
      "Epoch 8/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m338s\u001b[0m 1s/step - accuracy: 0.7715 - loss: 0.5066 - val_accuracy: 0.7827 - val_loss: 0.4815\n",
      "Epoch 9/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m337s\u001b[0m 1s/step - accuracy: 0.7706 - loss: 0.4943 - val_accuracy: 0.7443 - val_loss: 0.5020\n",
      "Epoch 10/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m315s\u001b[0m 1s/step - accuracy: 0.7728 - loss: 0.4920 - val_accuracy: 0.7613 - val_loss: 0.4819\n",
      "Epoch 11/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m313s\u001b[0m 1s/step - accuracy: 0.7709 - loss: 0.4903 - val_accuracy: 0.7953 - val_loss: 0.4756\n",
      "Epoch 12/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m311s\u001b[0m 1s/step - accuracy: 0.7766 - loss: 0.4869 - val_accuracy: 0.7849 - val_loss: 0.4732\n",
      "Epoch 13/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m312s\u001b[0m 1s/step - accuracy: 0.7787 - loss: 0.4863 - val_accuracy: 0.8056 - val_loss: 0.4594\n",
      "Epoch 14/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m312s\u001b[0m 1s/step - accuracy: 0.7764 - loss: 0.4867 - val_accuracy: 0.7953 - val_loss: 0.4495\n",
      "Epoch 15/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m310s\u001b[0m 1s/step - accuracy: 0.7883 - loss: 0.4665 - val_accuracy: 0.8019 - val_loss: 0.4481\n",
      "Epoch 16/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m313s\u001b[0m 1s/step - accuracy: 0.7756 - loss: 0.4771 - val_accuracy: 0.7812 - val_loss: 0.4502\n",
      "Epoch 17/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m311s\u001b[0m 1s/step - accuracy: 0.7891 - loss: 0.4674 - val_accuracy: 0.8101 - val_loss: 0.4439\n",
      "Epoch 18/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 1s/step - accuracy: 0.7902 - loss: 0.4672 - val_accuracy: 0.7716 - val_loss: 0.5008\n",
      "Epoch 19/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m312s\u001b[0m 1s/step - accuracy: 0.7844 - loss: 0.4721 - val_accuracy: 0.7953 - val_loss: 0.4459\n",
      "Epoch 20/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 1s/step - accuracy: 0.7928 - loss: 0.4606 - val_accuracy: 0.8160 - val_loss: 0.4365\n",
      "Epoch 21/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m335s\u001b[0m 1s/step - accuracy: 0.7988 - loss: 0.4520 - val_accuracy: 0.7554 - val_loss: 0.4834\n",
      "Epoch 22/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m343s\u001b[0m 1s/step - accuracy: 0.7578 - loss: 0.5005 - val_accuracy: 0.8056 - val_loss: 0.4371\n",
      "Epoch 23/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m363s\u001b[0m 2s/step - accuracy: 0.7774 - loss: 0.4721 - val_accuracy: 0.8108 - val_loss: 0.4472\n",
      "Epoch 24/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m405s\u001b[0m 2s/step - accuracy: 0.7878 - loss: 0.4569 - val_accuracy: 0.8123 - val_loss: 0.4385\n",
      "Epoch 25/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m356s\u001b[0m 1s/step - accuracy: 0.7884 - loss: 0.4614 - val_accuracy: 0.7894 - val_loss: 0.4466\n",
      "Epoch 26/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m317s\u001b[0m 1s/step - accuracy: 0.7855 - loss: 0.4676 - val_accuracy: 0.8189 - val_loss: 0.4329\n",
      "Epoch 27/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m315s\u001b[0m 1s/step - accuracy: 0.7924 - loss: 0.4623 - val_accuracy: 0.8137 - val_loss: 0.4296\n",
      "Epoch 28/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m310s\u001b[0m 1s/step - accuracy: 0.7871 - loss: 0.4567 - val_accuracy: 0.8093 - val_loss: 0.4361\n",
      "Epoch 29/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m311s\u001b[0m 1s/step - accuracy: 0.7893 - loss: 0.4602 - val_accuracy: 0.8204 - val_loss: 0.4304\n",
      "Epoch 30/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 1s/step - accuracy: 0.7827 - loss: 0.4577 - val_accuracy: 0.8211 - val_loss: 0.4274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 1s/step\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      benign     0.8025    0.8586    0.8296       686\n",
      "   malignant     0.8433    0.7826    0.8118       667\n",
      "\n",
      "    accuracy                         0.8211      1353\n",
      "   macro avg     0.8229    0.8206    0.8207      1353\n",
      "weighted avg     0.8226    0.8211    0.8208      1353\n",
      "\n",
      "Final validation accuracy: 0.8211\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Dataset directory\n",
    "data_dir = r'E:\\Abroad period research\\Medical images analysis paper implementation codes\\Second part of the paper\\Ultrasound Breast Images for Breast Cancer\\ultrasound breast classification'\n",
    "\n",
    "# Parameters\n",
    "IMG_SIZE = (150, 150)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 30\n",
    "TEST_SIZE = 0.15  # 15% for testing\n",
    "\n",
    "# Load image paths and labels\n",
    "def load_image_paths_and_labels(base_dir):\n",
    "    class_names = sorted([d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))])\n",
    "    all_filepaths = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for idx, class_name in enumerate(class_names):\n",
    "        class_folder = os.path.join(base_dir, class_name)\n",
    "        file_names = [f for f in os.listdir(class_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        \n",
    "        for file_name in file_names:\n",
    "            all_filepaths.append(os.path.join(class_folder, file_name))\n",
    "            all_labels.append(idx)  # numeric label\n",
    "            \n",
    "    return np.array(all_filepaths), np.array(all_labels), class_names\n",
    "\n",
    "filepaths, labels, class_names = load_image_paths_and_labels(data_dir)\n",
    "print(f\"Total images: {len(filepaths)} | Classes: {class_names}\")\n",
    "\n",
    "# Split data\n",
    "train_paths, test_paths, train_labels, test_labels = train_test_split(\n",
    "    filepaths, labels, test_size=TEST_SIZE, stratify=labels, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(train_paths)} | Test samples: {len(test_paths)}\")\n",
    "\n",
    "# Preprocess function\n",
    "def load_and_preprocess_image(path, label):\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)  # handle jpeg/png\n",
    "    image = tf.image.resize(image, IMG_SIZE)\n",
    "    image = image / 255.0  # normalize to [0,1]\n",
    "    return image, label\n",
    "\n",
    "# TF datasets\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n",
    "train_ds = train_ds.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_ds = train_ds.shuffle(buffer_size=1000).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_paths, test_labels))\n",
    "test_ds = test_ds.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# Model setup\n",
    "base_model = tf.keras.applications.ResNet50(\n",
    "    input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3),\n",
    "    include_top=False,\n",
    "    weights='imagenet'\n",
    ")\n",
    "base_model.trainable = False  # freeze\n",
    "\n",
    "# Number of classes\n",
    "num_classes = len(class_names)\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# Final activation and loss\n",
    "if num_classes == 2:\n",
    "    final_activation = 'sigmoid'\n",
    "    final_units = 1\n",
    "    loss_function = 'binary_crossentropy'\n",
    "else:\n",
    "    final_activation = 'softmax'\n",
    "    final_units = num_classes\n",
    "    loss_function = 'sparse_categorical_crossentropy'  # because labels are integers\n",
    "\n",
    "# Build the model\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(final_units, activation=final_activation)\n",
    "])\n",
    "\n",
    "# Compile\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=loss_function,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=test_ds\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save('glaucoma_detection_resnet50.h5')\n",
    "\n",
    "# Evaluate & Predict\n",
    "# Get test data arrays for predictions\n",
    "test_images = []\n",
    "test_labels_list = []\n",
    "\n",
    "for image, label in test_ds.unbatch():\n",
    "    test_images.append(image.numpy())\n",
    "    test_labels_list.append(label.numpy())\n",
    "\n",
    "test_images = np.array(test_images)\n",
    "test_labels_list = np.array(test_labels_list)\n",
    "\n",
    "# Predictions\n",
    "predictions = model.predict(test_images)\n",
    "\n",
    "if num_classes == 2:\n",
    "    predicted_classes = (predictions > 0.5).astype(\"int32\").flatten()\n",
    "else:\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Classification report\n",
    "report = classification_report(test_labels_list, predicted_classes, target_names=class_names, digits=4)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "# Final validation accuracy\n",
    "val_acc = history.history['val_accuracy'][-1]\n",
    "print(f\"Final validation accuracy: {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COVID-19 Radiography Database ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 40768 | Classes: ['COVID_Augmented', 'Lung_Opacity_Augmented', 'Normal', 'Viral Pneumonia_Augmented']\n",
      "Train samples: 34652 | Test samples: 6116\n",
      "Number of classes: 4\n",
      "Epoch 1/30\n",
      "\u001b[1m1083/1083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1620s\u001b[0m 1s/step - accuracy: 0.3773 - loss: 1.2874 - val_accuracy: 0.5448 - val_loss: 1.0528\n",
      "Epoch 2/30\n",
      "\u001b[1m1083/1083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1487s\u001b[0m 1s/step - accuracy: 0.4953 - loss: 1.0837 - val_accuracy: 0.5878 - val_loss: 0.9812\n",
      "Epoch 3/30\n",
      "\u001b[1m1083/1083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1505s\u001b[0m 1s/step - accuracy: 0.5455 - loss: 1.0005 - val_accuracy: 0.6537 - val_loss: 0.8789\n",
      "Epoch 4/30\n",
      "\u001b[1m1083/1083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1498s\u001b[0m 1s/step - accuracy: 0.5811 - loss: 0.9432 - val_accuracy: 0.6624 - val_loss: 0.8264\n",
      "Epoch 5/30\n",
      "\u001b[1m1083/1083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1603s\u001b[0m 1s/step - accuracy: 0.6016 - loss: 0.9063 - val_accuracy: 0.6687 - val_loss: 0.7979\n",
      "Epoch 6/30\n",
      "\u001b[1m1083/1083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1511s\u001b[0m 1s/step - accuracy: 0.6099 - loss: 0.8861 - val_accuracy: 0.6915 - val_loss: 0.7715\n",
      "Epoch 7/30\n",
      "\u001b[1m1083/1083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1496s\u001b[0m 1s/step - accuracy: 0.6165 - loss: 0.8611 - val_accuracy: 0.6836 - val_loss: 0.7596\n",
      "Epoch 8/30\n",
      "\u001b[1m1083/1083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1492s\u001b[0m 1s/step - accuracy: 0.6324 - loss: 0.8402 - val_accuracy: 0.6902 - val_loss: 0.7215\n",
      "Epoch 9/30\n",
      "\u001b[1m1083/1083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1505s\u001b[0m 1s/step - accuracy: 0.6329 - loss: 0.8346 - val_accuracy: 0.7124 - val_loss: 0.6951\n",
      "Epoch 10/30\n",
      "\u001b[1m1083/1083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1522s\u001b[0m 1s/step - accuracy: 0.6302 - loss: 0.8294 - val_accuracy: 0.7112 - val_loss: 0.6986\n",
      "Epoch 11/30\n",
      "\u001b[1m1083/1083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1496s\u001b[0m 1s/step - accuracy: 0.6449 - loss: 0.8146 - val_accuracy: 0.6990 - val_loss: 0.7563\n",
      "Epoch 12/30\n",
      "\u001b[1m1083/1083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1486s\u001b[0m 1s/step - accuracy: 0.6334 - loss: 0.8255 - val_accuracy: 0.6947 - val_loss: 0.6783\n",
      "Epoch 13/30\n",
      "\u001b[1m1083/1083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1502s\u001b[0m 1s/step - accuracy: 0.6337 - loss: 0.8138 - val_accuracy: 0.7083 - val_loss: 0.6649\n",
      "Epoch 14/30\n",
      "\u001b[1m1083/1083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1546s\u001b[0m 1s/step - accuracy: 0.6420 - loss: 0.7967 - val_accuracy: 0.6911 - val_loss: 0.6970\n",
      "Epoch 15/30\n",
      "\u001b[1m1083/1083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1596s\u001b[0m 1s/step - accuracy: 0.6488 - loss: 0.7920 - val_accuracy: 0.6902 - val_loss: 0.6836\n",
      "Epoch 16/30\n",
      "\u001b[1m1083/1083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1834s\u001b[0m 2s/step - accuracy: 0.6474 - loss: 0.7913 - val_accuracy: 0.6810 - val_loss: 0.6587\n",
      "Epoch 17/30\n",
      "\u001b[1m1083/1083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1369s\u001b[0m 1s/step - accuracy: 0.6452 - loss: 0.7926 - val_accuracy: 0.7227 - val_loss: 0.6653\n",
      "Epoch 18/30\n",
      "\u001b[1m1083/1083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1308s\u001b[0m 1s/step - accuracy: 0.6487 - loss: 0.7815 - val_accuracy: 0.6844 - val_loss: 0.6973\n",
      "Epoch 19/30\n",
      "\u001b[1m1083/1083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1291s\u001b[0m 1s/step - accuracy: 0.6379 - loss: 0.8031 - val_accuracy: 0.7271 - val_loss: 0.6400\n",
      "Epoch 20/30\n",
      "\u001b[1m1083/1083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1315s\u001b[0m 1s/step - accuracy: 0.6494 - loss: 0.7851 - val_accuracy: 0.6833 - val_loss: 0.6746\n",
      "Epoch 21/30\n",
      "\u001b[1m1083/1083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1480s\u001b[0m 1s/step - accuracy: 0.6506 - loss: 0.7765 - val_accuracy: 0.7281 - val_loss: 0.6643\n",
      "Epoch 22/30\n",
      "\u001b[1m1083/1083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2094s\u001b[0m 2s/step - accuracy: 0.6564 - loss: 0.7758 - val_accuracy: 0.7016 - val_loss: 0.6752\n",
      "Epoch 23/30\n",
      "\u001b[1m1083/1083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2086s\u001b[0m 2s/step - accuracy: 0.6525 - loss: 0.7849 - val_accuracy: 0.7081 - val_loss: 0.6409\n",
      "Epoch 24/30\n",
      "\u001b[1m1083/1083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1727s\u001b[0m 2s/step - accuracy: 0.6502 - loss: 0.7823 - val_accuracy: 0.7194 - val_loss: 0.6615\n",
      "Epoch 25/30\n",
      "\u001b[1m1083/1083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2460s\u001b[0m 2s/step - accuracy: 0.6550 - loss: 0.7718 - val_accuracy: 0.7266 - val_loss: 0.6476\n",
      "Epoch 26/30\n",
      "\u001b[1m1083/1083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1411s\u001b[0m 1s/step - accuracy: 0.6440 - loss: 0.7884 - val_accuracy: 0.7063 - val_loss: 0.6616\n",
      "Epoch 27/30\n",
      "\u001b[1m1083/1083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1405s\u001b[0m 1s/step - accuracy: 0.6420 - loss: 0.7904 - val_accuracy: 0.7158 - val_loss: 0.6746\n",
      "Epoch 28/30\n",
      "\u001b[1m1083/1083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1402s\u001b[0m 1s/step - accuracy: 0.6490 - loss: 0.7894 - val_accuracy: 0.7368 - val_loss: 0.6338\n",
      "Epoch 29/30\n",
      "\u001b[1m1083/1083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1401s\u001b[0m 1s/step - accuracy: 0.6523 - loss: 0.7743 - val_accuracy: 0.7201 - val_loss: 0.6707\n",
      "Epoch 30/30\n",
      "\u001b[1m1083/1083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1553s\u001b[0m 1s/step - accuracy: 0.6531 - loss: 0.7757 - val_accuracy: 0.7075 - val_loss: 0.6568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 1s/step\n",
      "Classification Report:\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      "          COVID_Augmented     0.6632    0.3774    0.4810      1529\n",
      "   Lung_Opacity_Augmented     0.6154    0.5736    0.5938      1529\n",
      "                   Normal     0.8036    0.9529    0.8719      1529\n",
      "Viral Pneumonia_Augmented     0.7052    0.9261    0.8007      1529\n",
      "\n",
      "                 accuracy                         0.7075      6116\n",
      "                macro avg     0.6969    0.7075    0.6869      6116\n",
      "             weighted avg     0.6969    0.7075    0.6869      6116\n",
      "\n",
      "Final validation accuracy: 0.7075\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Dataset directory\n",
    "data_dir = r'E:\\Abroad period research\\Medical images analysis paper implementation codes\\Second part of the paper\\Results on augmented just chest dataset\\justchest_Unet_Segmented_Dataset'\n",
    "\n",
    "# Parameters\n",
    "IMG_SIZE = (150, 150)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 30\n",
    "TEST_SIZE = 0.15  # 15% for testing\n",
    "\n",
    "# Load image paths and labels\n",
    "def load_image_paths_and_labels(base_dir):\n",
    "    class_names = sorted([d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))])\n",
    "    all_filepaths = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for idx, class_name in enumerate(class_names):\n",
    "        class_folder = os.path.join(base_dir, class_name)\n",
    "        file_names = [f for f in os.listdir(class_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        \n",
    "        for file_name in file_names:\n",
    "            all_filepaths.append(os.path.join(class_folder, file_name))\n",
    "            all_labels.append(idx)  # numeric label\n",
    "            \n",
    "    return np.array(all_filepaths), np.array(all_labels), class_names\n",
    "\n",
    "filepaths, labels, class_names = load_image_paths_and_labels(data_dir)\n",
    "print(f\"Total images: {len(filepaths)} | Classes: {class_names}\")\n",
    "\n",
    "# Split data\n",
    "train_paths, test_paths, train_labels, test_labels = train_test_split(\n",
    "    filepaths, labels, test_size=TEST_SIZE, stratify=labels, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(train_paths)} | Test samples: {len(test_paths)}\")\n",
    "\n",
    "# Preprocess function\n",
    "def load_and_preprocess_image(path, label):\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)  # handle jpeg/png\n",
    "    image = tf.image.resize(image, IMG_SIZE)\n",
    "    image = image / 255.0  # normalize to [0,1]\n",
    "    return image, label\n",
    "\n",
    "# TF datasets\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n",
    "train_ds = train_ds.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_ds = train_ds.shuffle(buffer_size=1000).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_paths, test_labels))\n",
    "test_ds = test_ds.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# Model setup\n",
    "base_model = tf.keras.applications.ResNet50(\n",
    "    input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3),\n",
    "    include_top=False,\n",
    "    weights='imagenet'\n",
    ")\n",
    "base_model.trainable = False  # freeze\n",
    "\n",
    "# Number of classes\n",
    "num_classes = len(class_names)\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# Final activation and loss\n",
    "if num_classes == 2:\n",
    "    final_activation = 'sigmoid'\n",
    "    final_units = 1\n",
    "    loss_function = 'binary_crossentropy'\n",
    "else:\n",
    "    final_activation = 'softmax'\n",
    "    final_units = num_classes\n",
    "    loss_function = 'sparse_categorical_crossentropy'  # because labels are integers\n",
    "\n",
    "# Build the model\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(final_units, activation=final_activation)\n",
    "])\n",
    "\n",
    "# Compile\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=loss_function,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=test_ds\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save('glaucoma_detection_resnet50.h5')\n",
    "\n",
    "# Evaluate & Predict\n",
    "# Get test data arrays for predictions\n",
    "test_images = []\n",
    "test_labels_list = []\n",
    "\n",
    "for image, label in test_ds.unbatch():\n",
    "    test_images.append(image.numpy())\n",
    "    test_labels_list.append(label.numpy())\n",
    "\n",
    "test_images = np.array(test_images)\n",
    "test_labels_list = np.array(test_labels_list)\n",
    "\n",
    "# Predictions\n",
    "predictions = model.predict(test_images)\n",
    "\n",
    "if num_classes == 2:\n",
    "    predicted_classes = (predictions > 0.5).astype(\"int32\").flatten()\n",
    "else:\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Classification report\n",
    "report = classification_report(test_labels_list, predicted_classes, target_names=class_names, digits=4)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "# Final validation accuracy\n",
    "val_acc = history.history['val_accuracy'][-1]\n",
    "print(f\"Final validation accuracy: {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resnet50"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
