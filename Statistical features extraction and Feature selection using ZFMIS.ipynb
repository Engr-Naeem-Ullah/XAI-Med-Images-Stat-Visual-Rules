{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Feature extraction using trained custom MobileNetV2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28537 validated image filenames belonging to 4 classes.\n",
      "Found 6115 validated image filenames belonging to 4 classes.\n",
      "Found 6116 validated image filenames belonging to 4 classes.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 878ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 746ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 774ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 791ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 736ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 719ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 724ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 722ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 712ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 709ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 699ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 707ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 805ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 752ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 708ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 706ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 726ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 711ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 719ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 710ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 730ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 758ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 703ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 695ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 699ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 705ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 809ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 707ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 694ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 692ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 710ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 785ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 695ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 692ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 706ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 695ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 699ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 707ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 709ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 708ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 699ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 694ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 695ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 695ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 774ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 699ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 706ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 690ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 704ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 693ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 695ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 694ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 696ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 743ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 703ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 705ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 704ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 769ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 690ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 690ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 696ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 699ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 727ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 835ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 829ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 720ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 748ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 709ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 780ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 717ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 712ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 704ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 695ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 713ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 692ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 732ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 690ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 701ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 707ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 696ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 771ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 704ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 718ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 732ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 734ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 725ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 705ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 696ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 701ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 772ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 693ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 695ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 692ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 696ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 694ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 696ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 708ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 701ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 701ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 753ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 747ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 718ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 712ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 709ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 721ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 694ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 695ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 703ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 703ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 710ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 699ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 774ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 703ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 695ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 729ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 704ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 694ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 693ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 704ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 708ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 713ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 709ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 731ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 707ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 705ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 792ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 706ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 707ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 756ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 759ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 727ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 712ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 751ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 705ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 715ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 707ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 704ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 722ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 794ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 709ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 713ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 724ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 713ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 712ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 713ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 714ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 718ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 714ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 720ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 715ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 752ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 844ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 720ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 718ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 719ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 729ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 709ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 710ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 720ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 735ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 711ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 707ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 709ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 800ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 703ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 714ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 759ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 714ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 723ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 716ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 723ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 715ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 713ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 755ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 746ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 854ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 735ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 747ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 739ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 742ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 723ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 706ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 714ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 712ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 708ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 709ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 708ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 791ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 726ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 711ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 704ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 719ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 706ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 709ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 707ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 722ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 706ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 709ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 709ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 781ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 704ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 693ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 704ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 701ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 701ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 693ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 691ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 740ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 707ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 787ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 703ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 704ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 711ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 699ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 722ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 775ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 739ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 780ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 704ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 701ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 757ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 693ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 694ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 694ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 737ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 699ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 708ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 705ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 723ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 711ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 690ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 701ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 708ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 711ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 772ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 705ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 701ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 727ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 704ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 704ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 699ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 701ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 788ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 692ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 710ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 691ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 704ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 778ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 786ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 732ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 693ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 707ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 720ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 696ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 701ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 699ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 707ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 704ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 699ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 762ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 690ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 723ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 693ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 694ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 713ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 695ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 784ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 701ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 701ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 694ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 708ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 705ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 777ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 693ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 709ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 696ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 701ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 695ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 693ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 793ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 715ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 707ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 701ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 713ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 693ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 696ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 734ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 695ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 694ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 719ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 737ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 706ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 695ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 705ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 693ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 706ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 774ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 696ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 704ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 701ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 703ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 706ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 710ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 696ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 704ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 699ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 710ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 692ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 711ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 705ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 701ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 787ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 703ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 693ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 706ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 707ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 704ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 703ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 796ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 708ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 708ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 710ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 716ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 704ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 714ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 706ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 720ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 736ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 705ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 709ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 719ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 714ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 705ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 718ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 705ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 716ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 743ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 794ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 711ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 711ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 724ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 705ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 717ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 712ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 711ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 717ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 796ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 713ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 715ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 719ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 708ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 707ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 718ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 709ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 707ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 711ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 712ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 705ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 704ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 780ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 695ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 696ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 704ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 713ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 777ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 696ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 703ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 703ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 699ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 706ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 710ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 706ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 706ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 696ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 705ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 692ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 694ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 790ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 707ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 703ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 707ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 752ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 779ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 726ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 795ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 707ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 704ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 701ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 703ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 699ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 783ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 703ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 705ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 693ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 696ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 768ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 704ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 706ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 710ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 744ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 714ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 706ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 724ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 695ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 706ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 740ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 692ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 711ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 785ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 707ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 695ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 696ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 720ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 705ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 710ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 792ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 696ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 710ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 705ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 707ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 699ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 816ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 706ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 701ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 707ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 710ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 709ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 705ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 777ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 706ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 704ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 706ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 699ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 705ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 785ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 703ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 784ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 703ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 713ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 692ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 705ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 710ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 713ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 779ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 696ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 699ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 699ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 689ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 701ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 706ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 696ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 696ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 785ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 706ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 696ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 713ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 704ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 690ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 701ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 701ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 707ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 705ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 710ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 734ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 725ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 717ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 719ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 708ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 705ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 709ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 708ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 708ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 714ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 724ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 711ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 712ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 709ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 715ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 709ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 707ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 723ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 709ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 720ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 708ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 711ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 714ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 715ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 795ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 730ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 750ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 717ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 719ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 709ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 708ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 715ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 787ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 713ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 709ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 713ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 714ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 720ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 707ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 707ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 790ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 711ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 717ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 714ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 717ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 710ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 714ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 707ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 792ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 713ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 715ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 699ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 708ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 710ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 710ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 710ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 800ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 715ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 717ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 739ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 742ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 710ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 719ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 715ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 713ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 717ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 716ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 712ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 713ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 712ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 704ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 715ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 809ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 716ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 724ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 709ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 716ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 716ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 712ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 709ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 793ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 707ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 752ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 717ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 712ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 710ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 784ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 695ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 707ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 696ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 706ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 701ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 724ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 758ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 718ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 695ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 699ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 776ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 804ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 829ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 706ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 769ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 695ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 707ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 699ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 714ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 699ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 707ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 696ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 703ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 708ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 703ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 708ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 708ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 707ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 696ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 699ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 701ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 699ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 705ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 703ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 695ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 699ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 704ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 708ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 701ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 695ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 699ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 701ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 703ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 703ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 780ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 709ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 705ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 695ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 725ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 716ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 694ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 695ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 707ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 719ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 705ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 712ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 688ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 719ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 724ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 696ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 713ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 705ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 703ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 701ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 701ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 751ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 710ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 699ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 712ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 692ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 770ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 703ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 694ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 780ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 699ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 711ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 691ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 707ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 704ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 704ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 705ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 706ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 755ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 709ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 716ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 781ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 712ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 785ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 696ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 708ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 686ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 704ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 782ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 691ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 711ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 714ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 721ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 717ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 750ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 746ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 712ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 708ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 695ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 836ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 711ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 741ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 744ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 724ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 693ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 703ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 780ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 727ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 730ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 703ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 695ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 720ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 707ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 722ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 720ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 707ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 712ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 744ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 711ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 718ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 701ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 711ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 728ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 712ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 711ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 789ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 715ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 717ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 722ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 727ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 714ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 724ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 791ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 738ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 727ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 711ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 724ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 746ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 722ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 710ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 720ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 714ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 711ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 717ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 722ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 708ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 825ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 744ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 743ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 712ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 733ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 706ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 703ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 703ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 711ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 706ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 705ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 773ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 701ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 699ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 708ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 703ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 693ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 695ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 692ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 704ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 695ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 712ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 713ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 708ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 769ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 722ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 690ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 703ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 725ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 696ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 696ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 691ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 692ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 692ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 694ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 694ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 696ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 690ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 696ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 696ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 701ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 684ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 718ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 742ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 720ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 709ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 783ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 699ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 694ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 696ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 694ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 704ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 750ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 723ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 729ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 746ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 726ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 731ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 711ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 723ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 780ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 715ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 692ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 713ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 696ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 692ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 695ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 684ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 696ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 689ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 695ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 713ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 687ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 696ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 687ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 693ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 692ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 689ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 696ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 705ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 686ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 689ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 703ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 689ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 694ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 696ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 691ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 699ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 694ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 701ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 691ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 693ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 725ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 701ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 693ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 696ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 711ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 687ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 776ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 726ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 699ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 710ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 699ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 699ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 701ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 710ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 688ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 770ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 696ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 707ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 717ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 715ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 759ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 701ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 704ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 701ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 695ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 776ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 692ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 690ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 691ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 689ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 685ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 790ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 693ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 701ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 696ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 699ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 694ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 695ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 693ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 704ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 715ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 740ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 710ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 694ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 688ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 688ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 701ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 690ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 704ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 695ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 696ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 703ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 772ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 709ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 705ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 705ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 706ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 701ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 707ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 707ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 735ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 726ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 708ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 710ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 708ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 720ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 703ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 699ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 710ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 708ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 706ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 715ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 715ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 710ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 703ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 705ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 707ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 712ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 705ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 711ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 709ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 708ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 705ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 719ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 714ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 711ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 706ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 701ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 704ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 728ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 703ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 707ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 711ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 709ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 708ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 701ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 711ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 705ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 708ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 706ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 705ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 704ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 701ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 705ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 727ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 703ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 701ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 707ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 714ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 705ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 786ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 709ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 713ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 718ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 710ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 712ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 714ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 708ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 708ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 706ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 706ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 705ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 711ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 730ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 703ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 704ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 748ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 750ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 712ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 705ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 713ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 704ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 752ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 745ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 711ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 803ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 705ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 708ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 704ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 715ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 705ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 710ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 709ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 703ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 705ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 719ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 711ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 699ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 686ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 699ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 689ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 704ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 687ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 694ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 704ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 690ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 699ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 689ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 694ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 689ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 692ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 714ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 691ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 690ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 694ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 701ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 762ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 689ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 695ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 709ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 717ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 686ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 695ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 690ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 692ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 704ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 693ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 693ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 690ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 691ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 694ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 701ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 705ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 695ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 699ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 689ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 688ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 703ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 689ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 688ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 703ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 708ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 692ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 689ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 699ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 718ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 694ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 692ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 689ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 699ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 689ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 692ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 689ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 696ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 686ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 694ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 725ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 710ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 690ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 687ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 692ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 694ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 696ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 688ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 693ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 694ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 760ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 686ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 705ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 695ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 691ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 699ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 692ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 692ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 688ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 695ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 695ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 695ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 694ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 692ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 686ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 693ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 699ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 699ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 692ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 690ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 696ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 693ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 691ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 687ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 694ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 686ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 692ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 718ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 723ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 724ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 718ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 709ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 709ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 692ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step\n",
      "Features extracted and saved.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Markdown\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, auc\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Function to print with Markdown\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "\n",
    "# Load and preprocess dataset\n",
    "image_dir = Path(r'E:\\Abroad period research\\new idea implementation codes\\Second part of the paper\\justchest_Unet_Segmented_Dataset')\n",
    "\n",
    "filepaths = list(image_dir.glob(r'**/*.png'))\n",
    "labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], filepaths))\n",
    "\n",
    "filepaths = pd.Series(filepaths, name='Filepath').astype(str)\n",
    "labels = pd.Series(labels, name='Label')\n",
    "\n",
    "image_df = pd.concat([filepaths, labels], axis=1)\n",
    "image_df = image_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "train_df, temp_df = train_test_split(image_df, train_size=0.7, shuffle=True, random_state=1)\n",
    "val_df, test_df = train_test_split(temp_df, train_size=0.5, shuffle=True, random_state=1)\n",
    "\n",
    "# Create data generators with augmentation\n",
    "def create_gen():\n",
    "    train_generator = ImageDataGenerator(\n",
    "        preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    val_test_generator = ImageDataGenerator(\n",
    "        preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "    )\n",
    "\n",
    "    train_images = train_generator.flow_from_dataframe(\n",
    "        dataframe=train_df,\n",
    "        x_col='Filepath',\n",
    "        y_col='Label',\n",
    "        target_size=(224, 224),\n",
    "        color_mode='rgb',\n",
    "        class_mode='categorical',\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        seed=0\n",
    "    )\n",
    "\n",
    "    val_images = val_test_generator.flow_from_dataframe(\n",
    "        dataframe=val_df,\n",
    "        x_col='Filepath',\n",
    "        y_col='Label',\n",
    "        target_size=(224, 224),\n",
    "        color_mode='rgb',\n",
    "        class_mode='categorical',\n",
    "        batch_size=32,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    test_images = val_test_generator.flow_from_dataframe(\n",
    "        dataframe=test_df,\n",
    "        x_col='Filepath',\n",
    "        y_col='Label',\n",
    "        target_size=(224, 224),\n",
    "        color_mode='rgb',\n",
    "        class_mode='categorical',\n",
    "        batch_size=32,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    return train_images, val_images, test_images\n",
    "\n",
    "\n",
    "# Load the pre-trained model\n",
    "model_path = r'E:\\Abroad period research\\new idea implementation codes\\Second part of the paper\\26 features results\\Mobilenetv2_finetuned_with_CLR_and_GradientAccum.h5'\n",
    "loaded_model = load_model(model_path)\n",
    "\n",
    "# Define the feature extraction model\n",
    "# Adjust the layer as needed; here, we use the fourth-last layer's output\n",
    "feature_extractor = Model(inputs=loaded_model.input, outputs=loaded_model.layers[-4].output)\n",
    "\n",
    "# Directory to save extracted features\n",
    "feature_dir = r'E:\\Abroad period research\\new idea implementation codes\\Second part of the paper\\26 features results\\extracted_features'\n",
    "os.makedirs(feature_dir, exist_ok=True)\n",
    "\n",
    "def extract_features(data_gen, set_name):\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    # Loop through batches in the data generator\n",
    "    for batch_images, batch_labels in data_gen:\n",
    "        # Extract features for the batch\n",
    "        batch_features = feature_extractor.predict(batch_images)\n",
    "        features.extend(batch_features)\n",
    "        labels.extend(batch_labels)\n",
    "        \n",
    "        # Break if we’ve covered all images in the generator\n",
    "        if data_gen.batch_index == 0:\n",
    "            break\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    features = np.array(features)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    # Save extracted features and labels to a file\n",
    "    with open(os.path.join(feature_dir, f\"{set_name}_features.pkl\"), 'wb') as f:\n",
    "        pickle.dump((features, labels), f)\n",
    "\n",
    "# Create data generators with augmentation\n",
    "train_images, val_images, test_images = create_gen()\n",
    "\n",
    "# Extract features for training, validation, and test sets\n",
    "extract_features(train_images, \"train\")\n",
    "extract_features(val_images, \"val\")\n",
    "extract_features(test_images, \"test\")\n",
    "\n",
    "print(\"Features extracted and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification of features using saved extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 100.00%\n",
      "Train Confusion Matrix:\n",
      " [[7161    0    0    0]\n",
      " [   0 7095    0    0]\n",
      " [   0    0 7189    0]\n",
      " [   0    0    0 7092]]\n",
      "Train Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      7161\n",
      "           1       1.00      1.00      1.00      7095\n",
      "           2       1.00      1.00      1.00      7189\n",
      "           3       1.00      1.00      1.00      7092\n",
      "\n",
      "    accuracy                           1.00     28537\n",
      "   macro avg       1.00      1.00      1.00     28537\n",
      "weighted avg       1.00      1.00      1.00     28537\n",
      "\n",
      "Validation Accuracy: 86.35%\n",
      "Validation Confusion Matrix:\n",
      " [[1280  160   89    4]\n",
      " [ 227 1100  129   74]\n",
      " [   0    0 1454    0]\n",
      " [   5    6  141 1446]]\n",
      "Validation Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.83      0.84      1533\n",
      "           1       0.87      0.72      0.79      1530\n",
      "           2       0.80      1.00      0.89      1454\n",
      "           3       0.95      0.90      0.93      1598\n",
      "\n",
      "    accuracy                           0.86      6115\n",
      "   macro avg       0.87      0.86      0.86      6115\n",
      "weighted avg       0.87      0.86      0.86      6115\n",
      "\n",
      "Test Accuracy: 85.92%\n",
      "Test Confusion Matrix:\n",
      " [[1255  161   80    2]\n",
      " [ 231 1098  139   99]\n",
      " [   0    0 1549    0]\n",
      " [   5    4  140 1353]]\n",
      "Test Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.84      0.84      1498\n",
      "           1       0.87      0.70      0.78      1567\n",
      "           2       0.81      1.00      0.90      1549\n",
      "           3       0.93      0.90      0.92      1502\n",
      "\n",
      "    accuracy                           0.86      6116\n",
      "   macro avg       0.86      0.86      0.86      6116\n",
      "weighted avg       0.86      0.86      0.86      6116\n",
      "\n",
      "Decision Tree model saved to: saved_models\\decision_tree_model.pkl\n"
     ]
    }
   ],
   "source": [
    "# # Import necessary libraries\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Directory to save extracted features and models\n",
    "feature_dir = r'E:\\Abroad period research\\new idea implementation codes\\Second part of the paper\\26 features results\\extracted_features'\n",
    "model_dir = 'saved_models'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Load extracted features\n",
    "with open(os.path.join(feature_dir, \"train_features.pkl\"), 'rb') as f:\n",
    "    train_features, train_labels = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(feature_dir, \"val_features.pkl\"), 'rb') as f:\n",
    "    val_features, val_labels = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(feature_dir, \"test_features.pkl\"), 'rb') as f:\n",
    "    test_features, test_labels = pickle.load(f)\n",
    "\n",
    "# Convert one-hot encoded labels to single-class labels\n",
    "train_labels = np.argmax(train_labels, axis=1)\n",
    "val_labels = np.argmax(val_labels, axis=1)\n",
    "test_labels = np.argmax(test_labels, axis=1)\n",
    "\n",
    "# Train Decision Tree Classifier\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(train_features, train_labels)\n",
    "\n",
    "# Save the trained Decision Tree model\n",
    "tree_model_path = os.path.join(model_dir, 'decision_tree_model.pkl')\n",
    "with open(tree_model_path, 'wb') as f:\n",
    "    pickle.dump(clf, f)\n",
    "\n",
    "# Evaluate on training, validation, and test sets\n",
    "for set_name, features, labels in [(\"Train\", train_features, train_labels), (\"Validation\", val_features, val_labels), (\"Test\", test_features, test_labels)]:\n",
    "    predictions = clf.predict(features)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    print(f\"{set_name} Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"{set_name} Confusion Matrix:\\n\", confusion_matrix(labels, predictions))\n",
    "    print(f\"{set_name} Classification Report:\\n\", classification_report(labels, predictions))\n",
    "\n",
    "print(f\"Decision Tree model saved to: {tree_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Decision Tree Rules:\n",
      "\n",
      "|--- feature_70 <= 0.39\n",
      "|   |--- feature_96 <= 2.19\n",
      "|   |   |--- feature_37 <= 0.58\n",
      "|   |   |   |--- feature_125 <= 1.24\n",
      "|   |   |   |   |--- feature_23 <= 0.18\n",
      "|   |   |   |   |   |--- feature_101 <= 1.32\n",
      "|   |   |   |   |   |   |--- feature_33 <= 0.40\n",
      "|   |   |   |   |   |   |   |--- feature_112 <= 1.08\n",
      "|   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |--- feature_112 >  1.08\n",
      "|   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |--- feature_33 >  0.40\n",
      "|   |   |   |   |   |   |   |--- feature_4 <= 0.32\n",
      "|   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |--- feature_4 >  0.32\n",
      "|   |   |   |   |   |   |   |   |--- class: 3\n",
      "|   |   |   |   |   |--- feature_101 >  1.32\n",
      "|   |   |   |   |   |   |--- feature_88 <= 0.31\n",
      "|   |   |   |   |   |   |   |--- feature_45 <= 1.74\n",
      "|   |   |   |   |   |   |   |   |--- feature_7 <= 3.29\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_8 <= 1.06\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_8 >  1.06\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_116 <= 0.98\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 8\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_116 >  0.98\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |--- feature_7 >  3.29\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_85 <= 3.60\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_85 >  3.60\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |--- feature_45 >  1.74\n",
      "|   |   |   |   |   |   |   |   |--- feature_51 <= 1.77\n",
      "|   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |--- feature_51 >  1.77\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_85 <= 2.71\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_107 <= 1.32\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 9\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_107 >  1.32\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 5\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_85 >  2.71\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_36 <= 3.14\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_36 >  3.14\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 8\n",
      "|   |   |   |   |   |   |--- feature_88 >  0.31\n",
      "|   |   |   |   |   |   |   |--- feature_110 <= 0.05\n",
      "|   |   |   |   |   |   |   |   |--- feature_4 <= 0.71\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_115 <= 1.47\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_108 <= 0.47\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_108 >  0.47\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_115 >  1.47\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_8 <= 1.57\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_8 >  1.57\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 2\n",
      "|   |   |   |   |   |   |   |   |--- feature_4 >  0.71\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_7 <= 2.80\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_4 <= 1.16\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 2\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_4 >  1.16\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_7 >  2.80\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_83 <= 0.76\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 10\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_83 >  0.76\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 6\n",
      "|   |   |   |   |   |   |   |--- feature_110 >  0.05\n",
      "|   |   |   |   |   |   |   |   |--- feature_108 <= 0.43\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_114 <= 1.87\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_2 <= 0.37\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 4\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_2 >  0.37\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_114 >  1.87\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |--- feature_108 >  0.43\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_86 <= 1.87\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_122 <= 1.78\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 3\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_122 >  1.78\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 5\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_86 >  1.87\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_75 <= 2.98\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_75 >  2.98\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |--- feature_23 >  0.18\n",
      "|   |   |   |   |   |--- feature_79 <= 0.44\n",
      "|   |   |   |   |   |   |--- feature_63 <= 0.00\n",
      "|   |   |   |   |   |   |   |--- feature_37 <= 0.05\n",
      "|   |   |   |   |   |   |   |   |--- feature_82 <= 2.59\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_36 <= 3.09\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_44 <= 3.03\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_44 >  3.03\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_36 >  3.09\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_108 <= 1.30\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_108 >  1.30\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |--- feature_82 >  2.59\n",
      "|   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |--- feature_37 >  0.05\n",
      "|   |   |   |   |   |   |   |   |--- feature_72 <= 0.01\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_127 <= 3.13\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_127 >  3.13\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_39 <= 1.71\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 2\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_39 >  1.71\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |--- feature_72 >  0.01\n",
      "|   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |--- feature_63 >  0.00\n",
      "|   |   |   |   |   |   |   |--- feature_92 <= 1.76\n",
      "|   |   |   |   |   |   |   |   |--- feature_93 <= 0.04\n",
      "|   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |--- feature_93 >  0.04\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_102 <= 1.51\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_58 <= 2.37\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 10\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_58 >  2.37\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_102 >  1.51\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |--- feature_92 >  1.76\n",
      "|   |   |   |   |   |   |   |   |--- feature_12 <= 0.17\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_67 <= 1.25\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 3\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_67 >  1.25\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_92 <= 1.82\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_92 >  1.82\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |--- feature_12 >  0.17\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_79 <= 0.21\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_26 <= 3.13\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_26 >  3.13\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 3\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_79 >  0.21\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_108 <= 1.04\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 2\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_108 >  1.04\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |--- feature_79 >  0.44\n",
      "|   |   |   |   |   |   |--- feature_26 <= 3.41\n",
      "|   |   |   |   |   |   |   |--- feature_73 <= 2.46\n",
      "|   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |--- feature_73 >  2.46\n",
      "|   |   |   |   |   |   |   |   |--- feature_111 <= 1.15\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_27 <= 1.59\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_27 >  1.59\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |--- feature_111 >  1.15\n",
      "|   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |--- feature_26 >  3.41\n",
      "|   |   |   |   |   |   |   |--- feature_33 <= 0.91\n",
      "|   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |--- feature_33 >  0.91\n",
      "|   |   |   |   |   |   |   |   |--- class: 3\n",
      "|   |   |   |--- feature_125 >  1.24\n",
      "|   |   |   |   |--- feature_58 <= 0.00\n",
      "|   |   |   |   |   |--- feature_4 <= 0.44\n",
      "|   |   |   |   |   |   |--- class: 3\n",
      "|   |   |   |   |   |--- feature_4 >  0.44\n",
      "|   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |--- feature_58 >  0.00\n",
      "|   |   |   |   |   |--- feature_65 <= 2.89\n",
      "|   |   |   |   |   |   |--- feature_75 <= 3.38\n",
      "|   |   |   |   |   |   |   |--- feature_95 <= 3.65\n",
      "|   |   |   |   |   |   |   |   |--- feature_65 <= 2.89\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_24 <= 3.05\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_22 <= 4.07\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 13\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_22 >  4.07\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_24 >  3.05\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_47 <= 1.57\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 7\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_47 >  1.57\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 5\n",
      "|   |   |   |   |   |   |   |   |--- feature_65 >  2.89\n",
      "|   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |--- feature_95 >  3.65\n",
      "|   |   |   |   |   |   |   |   |--- class: 3\n",
      "|   |   |   |   |   |   |--- feature_75 >  3.38\n",
      "|   |   |   |   |   |   |   |--- class: 3\n",
      "|   |   |   |   |   |--- feature_65 >  2.89\n",
      "|   |   |   |   |   |   |--- feature_98 <= 0.55\n",
      "|   |   |   |   |   |   |   |--- class: 3\n",
      "|   |   |   |   |   |   |--- feature_98 >  0.55\n",
      "|   |   |   |   |   |   |   |--- feature_126 <= 1.50\n",
      "|   |   |   |   |   |   |   |   |--- feature_24 <= 1.10\n",
      "|   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |--- feature_24 >  1.10\n",
      "|   |   |   |   |   |   |   |   |   |--- class: 3\n",
      "|   |   |   |   |   |   |   |--- feature_126 >  1.50\n",
      "|   |   |   |   |   |   |   |   |--- feature_122 <= 0.96\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_82 <= 5.07\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 3\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_82 >  5.07\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |--- feature_122 >  0.96\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_73 <= 1.74\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_32 <= 1.73\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 2\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_32 >  1.73\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 12\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_73 >  1.74\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_41 <= 2.06\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 2\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_41 >  2.06\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |--- feature_37 >  0.58\n",
      "|   |   |   |--- feature_17 <= 0.08\n",
      "|   |   |   |   |--- feature_37 <= 1.17\n",
      "|   |   |   |   |   |--- feature_62 <= 0.27\n",
      "|   |   |   |   |   |   |--- feature_48 <= 1.77\n",
      "|   |   |   |   |   |   |   |--- feature_107 <= 0.80\n",
      "|   |   |   |   |   |   |   |   |--- feature_23 <= 1.19\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_44 <= 2.40\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_44 >  2.40\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_44 <= 2.73\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 3\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_44 >  2.73\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 8\n",
      "|   |   |   |   |   |   |   |   |--- feature_23 >  1.19\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_57 <= 0.26\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_57 >  0.26\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_4 <= 2.13\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_4 >  2.13\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |--- feature_107 >  0.80\n",
      "|   |   |   |   |   |   |   |   |--- feature_65 <= 1.10\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_24 <= 3.30\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_85 <= 3.91\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 8\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_85 >  3.91\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_24 >  3.30\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_7 <= 3.05\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 2\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_7 >  3.05\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 4\n",
      "|   |   |   |   |   |   |   |   |--- feature_65 >  1.10\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_1 <= 2.05\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_1 >  2.05\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_126 <= 2.69\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_126 >  2.69\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |--- feature_48 >  1.77\n",
      "|   |   |   |   |   |   |   |--- feature_43 <= 1.29\n",
      "|   |   |   |   |   |   |   |   |--- feature_51 <= 2.03\n",
      "|   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |--- feature_51 >  2.03\n",
      "|   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |--- feature_43 >  1.29\n",
      "|   |   |   |   |   |   |   |   |--- feature_19 <= 0.05\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_32 <= 2.71\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_83 <= 1.07\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 5\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_83 >  1.07\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 3\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_32 >  2.71\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_11 <= 1.89\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 3\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_11 >  1.89\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 4\n",
      "|   |   |   |   |   |   |   |   |--- feature_19 >  0.05\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_48 <= 1.86\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_10 <= 1.29\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_10 >  1.29\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_48 >  1.86\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |--- feature_62 >  0.27\n",
      "|   |   |   |   |   |   |--- feature_101 <= 2.82\n",
      "|   |   |   |   |   |   |   |--- feature_127 <= 1.84\n",
      "|   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |--- feature_127 >  1.84\n",
      "|   |   |   |   |   |   |   |   |--- feature_26 <= 2.56\n",
      "|   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |--- feature_26 >  2.56\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_82 <= 1.35\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_16 <= 1.72\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_16 >  1.72\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_82 >  1.35\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |--- feature_101 >  2.82\n",
      "|   |   |   |   |   |   |   |--- feature_90 <= 0.35\n",
      "|   |   |   |   |   |   |   |   |--- feature_22 <= 1.20\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_108 <= 0.21\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_108 >  0.21\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_62 <= 0.28\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_62 >  0.28\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |--- feature_22 >  1.20\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_80 <= 1.02\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_116 <= 0.13\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 6\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_116 >  0.13\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_80 >  1.02\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |--- feature_90 >  0.35\n",
      "|   |   |   |   |   |   |   |   |--- feature_82 <= 1.91\n",
      "|   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |--- feature_82 >  1.91\n",
      "|   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |--- feature_37 >  1.17\n",
      "|   |   |   |   |   |--- feature_56 <= 0.90\n",
      "|   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |--- feature_56 >  0.90\n",
      "|   |   |   |   |   |   |--- feature_92 <= 2.42\n",
      "|   |   |   |   |   |   |   |--- feature_63 <= 1.24\n",
      "|   |   |   |   |   |   |   |   |--- feature_53 <= 1.88\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_80 <= 1.38\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_4 <= 1.35\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 3\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_4 >  1.35\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_80 >  1.38\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_92 <= 2.28\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 3\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_92 >  2.28\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |--- feature_53 >  1.88\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_36 <= 3.29\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_102 <= 1.21\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 2\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_102 >  1.21\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 5\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_36 >  3.29\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_44 <= 3.66\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 2\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_44 >  3.66\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |--- feature_63 >  1.24\n",
      "|   |   |   |   |   |   |   |   |--- feature_63 <= 1.35\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_116 <= 0.05\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_6 <= 1.13\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_6 >  1.13\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_116 >  0.05\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_117 <= 1.91\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_117 >  1.91\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |--- feature_63 >  1.35\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_43 <= 1.95\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_43 >  1.95\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_11 <= 2.04\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 5\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_11 >  2.04\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |--- feature_92 >  2.42\n",
      "|   |   |   |   |   |   |   |--- feature_56 <= 1.68\n",
      "|   |   |   |   |   |   |   |   |--- feature_57 <= 0.40\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_39 <= 0.15\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_39 >  0.15\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_16 <= 1.60\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_16 >  1.60\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |--- feature_57 >  0.40\n",
      "|   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |--- feature_56 >  1.68\n",
      "|   |   |   |   |   |   |   |   |--- feature_1 <= 1.30\n",
      "|   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |--- feature_1 >  1.30\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_27 <= 3.07\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_24 <= 2.92\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 8\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_24 >  2.92\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 7\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_27 >  3.07\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_19 <= 0.46\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_19 >  0.46\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |--- feature_17 >  0.08\n",
      "|   |   |   |   |--- feature_85 <= 1.61\n",
      "|   |   |   |   |   |--- feature_57 <= 1.56\n",
      "|   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |--- feature_57 >  1.56\n",
      "|   |   |   |   |   |   |--- feature_10 <= 1.76\n",
      "|   |   |   |   |   |   |   |--- class: 3\n",
      "|   |   |   |   |   |   |--- feature_10 >  1.76\n",
      "|   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |--- feature_85 >  1.61\n",
      "|   |   |   |   |   |--- feature_12 <= 2.32\n",
      "|   |   |   |   |   |   |--- feature_15 <= 1.25\n",
      "|   |   |   |   |   |   |   |--- feature_84 <= 4.39\n",
      "|   |   |   |   |   |   |   |   |--- feature_102 <= 1.18\n",
      "|   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |--- feature_102 >  1.18\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_51 <= 0.98\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_4 <= 1.10\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_4 >  1.10\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 21\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_51 >  0.98\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_37 <= 1.60\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 2\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_37 >  1.60\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 4\n",
      "|   |   |   |   |   |   |   |--- feature_84 >  4.39\n",
      "|   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |--- feature_15 >  1.25\n",
      "|   |   |   |   |   |   |   |--- feature_50 <= 1.14\n",
      "|   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |--- feature_50 >  1.14\n",
      "|   |   |   |   |   |   |   |   |--- class: 3\n",
      "|   |   |   |   |   |--- feature_12 >  2.32\n",
      "|   |   |   |   |   |   |--- feature_106 <= 0.27\n",
      "|   |   |   |   |   |   |   |--- feature_54 <= 0.62\n",
      "|   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |--- feature_54 >  0.62\n",
      "|   |   |   |   |   |   |   |   |--- class: 3\n",
      "|   |   |   |   |   |   |--- feature_106 >  0.27\n",
      "|   |   |   |   |   |   |   |--- feature_102 <= 1.76\n",
      "|   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |--- feature_102 >  1.76\n",
      "|   |   |   |   |   |   |   |   |--- feature_23 <= 2.95\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_23 <= 2.95\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_34 <= 4.31\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 21\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_34 >  4.31\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_23 >  2.95\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |--- feature_23 >  2.95\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_49 <= 2.53\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_15 <= 1.05\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_15 >  1.05\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_49 >  2.53\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_7 <= 3.24\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 16\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_7 >  3.24\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 9\n",
      "|   |--- feature_96 >  2.19\n",
      "|   |   |--- feature_100 <= 0.70\n",
      "|   |   |   |--- feature_11 <= 2.54\n",
      "|   |   |   |   |--- class: 3\n",
      "|   |   |   |--- feature_11 >  2.54\n",
      "|   |   |   |   |--- feature_7 <= 0.64\n",
      "|   |   |   |   |   |--- class: 3\n",
      "|   |   |   |   |--- feature_7 >  0.64\n",
      "|   |   |   |   |   |--- feature_84 <= 0.84\n",
      "|   |   |   |   |   |   |--- feature_49 <= 0.36\n",
      "|   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |--- feature_49 >  0.36\n",
      "|   |   |   |   |   |   |   |--- feature_120 <= 4.18\n",
      "|   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |--- feature_120 >  4.18\n",
      "|   |   |   |   |   |   |   |   |--- class: 3\n",
      "|   |   |   |   |   |--- feature_84 >  0.84\n",
      "|   |   |   |   |   |   |--- class: 3\n",
      "|   |   |--- feature_100 >  0.70\n",
      "|   |   |   |--- feature_96 <= 2.39\n",
      "|   |   |   |   |--- feature_68 <= 0.59\n",
      "|   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |--- feature_68 >  0.59\n",
      "|   |   |   |   |   |--- feature_112 <= 2.41\n",
      "|   |   |   |   |   |   |--- class: 3\n",
      "|   |   |   |   |   |--- feature_112 >  2.41\n",
      "|   |   |   |   |   |   |--- feature_17 <= 1.32\n",
      "|   |   |   |   |   |   |   |--- class: 3\n",
      "|   |   |   |   |   |   |--- feature_17 >  1.32\n",
      "|   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |--- feature_96 >  2.39\n",
      "|   |   |   |   |--- feature_26 <= 3.72\n",
      "|   |   |   |   |   |--- feature_7 <= 1.13\n",
      "|   |   |   |   |   |   |--- feature_98 <= 0.05\n",
      "|   |   |   |   |   |   |   |--- feature_26 <= 3.34\n",
      "|   |   |   |   |   |   |   |   |--- feature_75 <= 2.54\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_57 <= 2.88\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_57 <= 2.88\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 3\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_57 >  2.88\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_57 >  2.88\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 3\n",
      "|   |   |   |   |   |   |   |   |--- feature_75 >  2.54\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_75 <= 2.54\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_75 >  2.54\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 3\n",
      "|   |   |   |   |   |   |   |--- feature_26 >  3.34\n",
      "|   |   |   |   |   |   |   |   |--- feature_112 <= 2.81\n",
      "|   |   |   |   |   |   |   |   |   |--- class: 3\n",
      "|   |   |   |   |   |   |   |   |--- feature_112 >  2.81\n",
      "|   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |--- feature_98 >  0.05\n",
      "|   |   |   |   |   |   |   |--- feature_55 <= 0.86\n",
      "|   |   |   |   |   |   |   |   |--- class: 3\n",
      "|   |   |   |   |   |   |   |--- feature_55 >  0.86\n",
      "|   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |--- feature_7 >  1.13\n",
      "|   |   |   |   |   |   |--- feature_29 <= 1.79\n",
      "|   |   |   |   |   |   |   |--- feature_11 <= 0.75\n",
      "|   |   |   |   |   |   |   |   |--- feature_90 <= 2.81\n",
      "|   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |--- feature_90 >  2.81\n",
      "|   |   |   |   |   |   |   |   |   |--- class: 3\n",
      "|   |   |   |   |   |   |   |--- feature_11 >  0.75\n",
      "|   |   |   |   |   |   |   |   |--- feature_112 <= 2.73\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_49 <= 1.29\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_90 <= 1.85\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 2\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_90 >  1.85\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 3\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_49 >  1.29\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_49 <= 1.33\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 2\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_49 >  1.33\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 2\n",
      "|   |   |   |   |   |   |   |   |--- feature_112 >  2.73\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_49 <= 0.34\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_95 <= 1.40\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_95 >  1.40\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_49 >  0.34\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 3\n",
      "|   |   |   |   |   |   |--- feature_29 >  1.79\n",
      "|   |   |   |   |   |   |   |--- feature_24 <= 0.58\n",
      "|   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |--- feature_24 >  0.58\n",
      "|   |   |   |   |   |   |   |   |--- class: 3\n",
      "|   |   |   |   |--- feature_26 >  3.72\n",
      "|   |   |   |   |   |--- class: 0\n",
      "|--- feature_70 >  0.39\n",
      "|   |--- feature_101 <= 1.64\n",
      "|   |   |--- class: 2\n",
      "|   |--- feature_101 >  1.64\n",
      "|   |   |--- class: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import export_text\n",
    "import pickle\n",
    "\n",
    "# Load the trained decision tree model\n",
    "with open(tree_model_path, 'rb') as f:\n",
    "    clf = pickle.load(f)\n",
    "\n",
    "# Extract decision rules\n",
    "tree_rules = export_text(clf, feature_names=[f'feature_{i}' for i in range(train_features.shape[1])])\n",
    "\n",
    "# Display decision rules\n",
    "print(\"Extracted Decision Tree Rules:\\n\")\n",
    "print(tree_rules)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USing Rulefit on the extracted deep features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Features Shape: (28537, 128)\n",
      "Train Labels Shape: (28537, 4)\n",
      "Validation Features Shape: (6115, 128)\n",
      "Validation Labels Shape: (6115, 4)\n",
      "Combined Training + Validation Features Shape: (34652, 128)\n",
      "Combined Training + Validation Labels Shape: (34652,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.010e+02, tolerance: 4.343e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.9472\n",
      "Confusion Matrix:\n",
      "[[1360  124   14    0]\n",
      " [  94 1399   72    2]\n",
      " [   0    0 1549    0]\n",
      " [   0    2   15 1485]]\n",
      "Test Confusion Matrix:\n",
      " [[1360  124   14    0]\n",
      " [  94 1399   72    2]\n",
      " [   0    0 1549    0]\n",
      " [   0    2   15 1485]]\n",
      "Test Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.92      1498\n",
      "           1       0.92      0.89      0.90      1567\n",
      "           2       0.94      1.00      0.97      1549\n",
      "           3       1.00      0.99      0.99      1502\n",
      "\n",
      "    accuracy                           0.95      6116\n",
      "   macro avg       0.95      0.95      0.95      6116\n",
      "weighted avg       0.95      0.95      0.95      6116\n",
      "\n",
      "Top Rules from RuleFit Model:\n",
      "                                                  rule  type      coef  \\\n",
      "518  feature_27 <= 1.4795796871185303 & feature_57 ...  rule  0.247160   \n",
      "424  feature_90 > 1.8816329836845398 & feature_115 ...  rule  0.240747   \n",
      "186                  feature_127 <= 1.3167153000831604  rule  0.130553   \n",
      "185                    feature_127 > 1.194348156452179  rule -0.111971   \n",
      "550  feature_57 > 1.6212771534919739 & feature_27 <...  rule  0.112541   \n",
      "\n",
      "      support  importance  \n",
      "518  0.243421    0.106068  \n",
      "424  0.227796    0.100972  \n",
      "186  0.511513    0.065259  \n",
      "185  0.514803    0.055961  \n",
      "550  0.229441    0.047320  \n",
      "Rules have been saved to E:/Abroad period research/new idea implementation codes/Second part of the paper/26 features results/rulefit_rules_on_deep_features.txt.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from rulefit import RuleFit\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Directory to save extracted features and models\n",
    "feature_dir = r'E:\\Abroad period research\\new idea implementation codes\\Second part of the paper\\26 features results\\extracted_features'\n",
    "model_dir = 'saved_models'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Load extracted features\n",
    "with open(os.path.join(feature_dir, \"train_features.pkl\"), 'rb') as f:\n",
    "    train_features, train_labels = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(feature_dir, \"val_features.pkl\"), 'rb') as f:\n",
    "    val_features, val_labels = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(feature_dir, \"test_features.pkl\"), 'rb') as f:\n",
    "    test_features, test_labels = pickle.load(f)\n",
    "\n",
    "# Check shapes of features and labels\n",
    "print(\"Train Features Shape:\", train_features.shape)\n",
    "print(\"Train Labels Shape:\", train_labels.shape)\n",
    "print(\"Validation Features Shape:\", val_features.shape)\n",
    "print(\"Validation Labels Shape:\", val_labels.shape)\n",
    "\n",
    "# Ensure labels are aligned with features\n",
    "assert train_features.shape[0] == train_labels.shape[0], \"Mismatch between train features and labels.\"\n",
    "assert val_features.shape[0] == val_labels.shape[0], \"Mismatch between validation features and labels.\"\n",
    "\n",
    "# Convert multi-dimensional labels to single class labels (if needed)\n",
    "train_labels = np.argmax(train_labels, axis=1)  # Assuming one-hot encoding\n",
    "val_labels = np.argmax(val_labels, axis=1)      # Assuming one-hot encoding\n",
    "test_labels = np.argmax(test_labels, axis=1)    # If test labels are also one-hot encoded\n",
    "\n",
    "# Combine training and validation data for final training\n",
    "train_val_features = np.vstack([train_features, val_features])\n",
    "train_val_labels = np.hstack([train_labels, val_labels])\n",
    "\n",
    "print(f\"Combined Training + Validation Features Shape: {train_val_features.shape}\")\n",
    "print(f\"Combined Training + Validation Labels Shape: {train_val_labels.shape}\")\n",
    "\n",
    "# Initialize RuleFit model\n",
    "rf = RuleFit(tree_size=3, sample_fract=1.0, max_rules=2000, random_state=42)\n",
    "\n",
    "# Fit the RuleFit model to combined training and validation data\n",
    "rf.fit(train_val_features, train_val_labels)\n",
    "\n",
    "# Predict on the test set (continuous values)\n",
    "test_predictions = rf.predict(test_features)\n",
    "\n",
    "# Convert continuous predictions to discrete class labels by rounding or using a threshold\n",
    "test_predictions_discrete = np.round(test_predictions).astype(int)\n",
    "\n",
    "# Ensure the predicted labels are within the valid range of classes\n",
    "test_predictions_discrete = np.clip(test_predictions_discrete, np.min(train_val_labels), np.max(train_val_labels))\n",
    "\n",
    "# Calculate testing accuracy\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions_discrete)\n",
    "print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(test_labels, test_predictions_discrete)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(f\"{set_name} Confusion Matrix:\\n\", confusion_matrix(test_labels, test_predictions_discrete))\n",
    "print(f\"{set_name} Classification Report:\\n\", classification_report(test_labels, test_predictions_discrete))\n",
    "\n",
    "# Extract rules from the RuleFit model\n",
    "rules = rf.get_rules()\n",
    "rules = rules[rules.coef != 0].sort_values(\"importance\", ascending=False)\n",
    "\n",
    "# Display the top rules\n",
    "print(\"Top Rules from RuleFit Model:\")\n",
    "print(rules.head())\n",
    "\n",
    "# Save rules to a text file\n",
    "output_file_path = 'E:/Abroad period research/new idea implementation codes/Second part of the paper/26 features results/rulefit_rules_on_deep_features.txt'\n",
    "rules.to_csv(output_file_path, index=False)\n",
    "print(f\"Rules have been saved to {output_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Statistical Features for Training, Validation, and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistical features calculated and saved in CSV format.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from scipy.stats import skew, entropy, kurtosis, variation, iqr\n",
    "import os\n",
    "\n",
    "# Load extracted features\n",
    "feature_dir = 'E:/Abroad period research/new idea implementation codes/Second part of the paper/26 features results/extracted_features'\n",
    "stat_feature_dir = 'E:/Abroad period research/new idea implementation codes/Second part of the paper/26 features results/statistical_features'\n",
    "os.makedirs(stat_feature_dir, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(feature_dir, \"train_features.pkl\"), 'rb') as f:\n",
    "    train_features, train_labels = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(feature_dir, \"val_features.pkl\"), 'rb') as f:\n",
    "    val_features, val_labels = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(feature_dir, \"test_features.pkl\"), 'rb') as f:\n",
    "    test_features, test_labels = pickle.load(f)\n",
    "\n",
    "# Convert one-hot encoded labels to single class labels, if needed\n",
    "if len(train_labels.shape) > 1 and train_labels.shape[1] > 1:\n",
    "    train_labels = np.argmax(train_labels, axis=1)\n",
    "\n",
    "if len(val_labels.shape) > 1 and val_labels.shape[1] > 1:\n",
    "    val_labels = np.argmax(val_labels, axis=1)\n",
    "\n",
    "if len(test_labels.shape) > 1 and test_labels.shape[1] > 1:\n",
    "    test_labels = np.argmax(test_labels, axis=1)\n",
    "\n",
    "# Function to calculate signal-to-noise ratio\n",
    "def signal_to_noise(f):\n",
    "    mean = np.mean(f)\n",
    "    std = np.std(f)\n",
    "    return mean / (std + 1e-6)  # Adding small constant to avoid division by zero\n",
    "\n",
    "# Function to calculate more advanced statistical features from deep features\n",
    "def calculate_statistical_features(features):\n",
    "    stats_features = []\n",
    "    for f in features:\n",
    "        stats = {\n",
    "            'mean': np.mean(f),\n",
    "            'std_dev': np.std(f),\n",
    "            'variance': np.var(f),\n",
    "            'median': np.median(f),\n",
    "            'range': np.ptp(f),  # Peak-to-peak range\n",
    "            'skewness': skew(f),\n",
    "            'kurtosis': kurtosis(f),\n",
    "            'entropy': entropy(np.abs(f) + 1e-6),  # Add small constant to avoid log(0)\n",
    "            'energy': np.sum(f ** 2),  # Sum of squared elements\n",
    "            'contrast': np.std(f) ** 2,  # Contrast as variance\n",
    "            'mean_abs_dev': np.mean(np.abs(f - np.mean(f))),\n",
    "            'min_value': np.min(f),\n",
    "            'max_value': np.max(f),\n",
    "            'iqr': iqr(f),  # Interquartile range\n",
    "            'percentile_25': np.percentile(f, 25),\n",
    "            'percentile_50': np.percentile(f, 50),  # Median\n",
    "            'percentile_75': np.percentile(f, 75),\n",
    "            'signal_to_noise': signal_to_noise(f),\n",
    "            'coef_of_var': variation(f),  # Coefficient of variation\n",
    "            'autocorrelation': np.corrcoef(f[:-1], f[1:])[0, 1] if len(f) > 1 else 0,  # Lag-1 autocorrelation\n",
    "            'shannon_entropy': -np.sum(f * np.log2(f + 1e-6)),  # Shannon entropy for diversity measure\n",
    "            'root_mean_square': np.sqrt(np.mean(f ** 2)),  # Root mean square\n",
    "            'harmonic_mean': len(f) / np.sum(1.0 / (f + 1e-6)),  # Harmonic mean\n",
    "            'geometric_mean': np.exp(np.mean(np.log(f + 1e-6))),  # Geometric mean\n",
    "            'std_error_mean': np.std(f) / np.sqrt(len(f)),  # Standard error of the mean\n",
    "            'median_abs_dev': np.median(np.abs(f - np.median(f))),  # Median absolute deviation\n",
    "        }\n",
    "        stats_features.append(stats)\n",
    "    return stats_features\n",
    "\n",
    "# Function to save features and labels as CSV\n",
    "def save_statistical_features_as_csv(features, labels, set_name):\n",
    "    df = pd.DataFrame(features)\n",
    "    df['label'] = labels  # Ensure labels are a 1D array\n",
    "    df.to_csv(os.path.join(stat_feature_dir, f\"{set_name}_stat_features.csv\"), index=False)\n",
    "\n",
    "# Calculate and save statistical features for each dataset\n",
    "for set_name, features, labels in [(\"train\", train_features, train_labels), \n",
    "                                   (\"val\", val_features, val_labels), \n",
    "                                   (\"test\", test_features, test_labels)]:\n",
    "    stats_features = calculate_statistical_features(features)\n",
    "    save_statistical_features_as_csv(stats_features, labels, set_name)\n",
    "\n",
    "print(\"Statistical features calculated and saved in CSV format.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Statistical Features and Train & Evaluate Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Training Accuracy: 91.9283%\n",
      "Combined Training Confusion Matrix:\n",
      " [[7373 1236   70   15]\n",
      " [ 663 7857   94   11]\n",
      " [  43   90 8214  296]\n",
      " [   6   40  233 8411]]\n",
      "Combined Training Classification Report:\n",
      "  Class 0: Precision: 0.9119, Recall: 0.8481, F1-Score: 0.8788\n",
      "  Class 1: Precision: 0.8519, Recall: 0.9110, F1-Score: 0.8804\n",
      "  Class 2: Precision: 0.9539, Recall: 0.9504, F1-Score: 0.9521\n",
      "  Class 3: Precision: 0.9631, Recall: 0.9679, F1-Score: 0.9655\n",
      "  Accuracy: 0.9193\n",
      "  Class macro avg: Precision: 0.9202, Recall: 0.9193, F1-Score: 0.9192\n",
      "  Class weighted avg: Precision: 0.9203, Recall: 0.9193, F1-Score: 0.9192\n",
      "\n",
      "Test Accuracy: 88.3257%\n",
      "Test Confusion Matrix:\n",
      " [[1194  262   40    2]\n",
      " [ 157 1307   93   10]\n",
      " [   0    0 1537   12]\n",
      " [   2    7  129 1364]]\n",
      "Test Classification Report:\n",
      "  Class 0: Precision: 0.8825, Recall: 0.7971, F1-Score: 0.8376\n",
      "  Class 1: Precision: 0.8293, Recall: 0.8341, F1-Score: 0.8317\n",
      "  Class 2: Precision: 0.8544, Recall: 0.9923, F1-Score: 0.9182\n",
      "  Class 3: Precision: 0.9827, Recall: 0.9081, F1-Score: 0.9439\n",
      "  Accuracy: 0.8833\n",
      "  Class macro avg: Precision: 0.8872, Recall: 0.8829, F1-Score: 0.8828\n",
      "  Class weighted avg: Precision: 0.8864, Recall: 0.8833, F1-Score: 0.8826\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "stat_feature_dir = 'E:/Abroad period research/new idea implementation codes/Second part of the paper/26 features results/statistical_features'\n",
    "\n",
    "# Load the CSV files\n",
    "train_df = pd.read_csv(os.path.join(stat_feature_dir, \"train_stat_features.csv\"))\n",
    "val_df = pd.read_csv(os.path.join(stat_feature_dir, \"val_stat_features.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(stat_feature_dir, \"test_stat_features.csv\"))\n",
    "\n",
    "# Separate features and labels\n",
    "train_stat_features = train_df.drop(columns=['label']).values\n",
    "train_labels = train_df['label'].values\n",
    "\n",
    "val_stat_features = val_df.drop(columns=['label']).values\n",
    "val_labels = val_df['label'].values\n",
    "\n",
    "test_stat_features = test_df.drop(columns=['label']).values\n",
    "test_labels = test_df['label'].values\n",
    "\n",
    "# Combine training and validation data for final training\n",
    "combined_features = np.vstack([train_stat_features, val_stat_features])\n",
    "combined_labels = np.hstack([train_labels, val_labels])\n",
    "\n",
    "# Train Decision Tree Classifier with specified hyperparameters\n",
    "clf = DecisionTreeClassifier(\n",
    "    random_state=0,\n",
    "    max_depth=10,\n",
    "    min_samples_leaf=4,\n",
    "    min_samples_split=2\n",
    ")\n",
    "clf.fit(combined_features, combined_labels)\n",
    "\n",
    "# Function to print classification report with four decimal points\n",
    "def print_classification_report(set_name, labels, predictions):\n",
    "    report = classification_report(labels, predictions, output_dict=True)\n",
    "    print(f\"{set_name} Classification Report:\")\n",
    "    for label, metrics in report.items():\n",
    "        if label == 'accuracy':\n",
    "            print(f\"  Accuracy: {metrics:.4f}\")\n",
    "        else:\n",
    "            print(f\"  Class {label}: Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, F1-Score: {metrics['f1-score']:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Evaluate on the combined training set\n",
    "train_predictions = clf.predict(combined_features)\n",
    "train_accuracy = accuracy_score(combined_labels, train_predictions)\n",
    "print(f\"Combined Training Accuracy: {train_accuracy * 100:.4f}%\")\n",
    "print(f\"Combined Training Confusion Matrix:\\n\", confusion_matrix(combined_labels, train_predictions))\n",
    "print_classification_report(\"Combined Training\", combined_labels, train_predictions)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_predictions = clf.predict(test_stat_features)\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.4f}%\")\n",
    "print(f\"Test Confusion Matrix:\\n\", confusion_matrix(test_labels, test_predictions))\n",
    "print_classification_report(\"Test\", test_labels, test_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Rulefit on the statistical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Training + Validation Features Shape: (34652, 26)\n",
      "Combined Training + Validation Labels Shape: (34652,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.572e+02, tolerance: 4.343e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.8726\n",
      "Confusion Matrix:\n",
      "[[1154  320   24    0]\n",
      " [ 136 1308  119    4]\n",
      " [   0    0 1540    9]\n",
      " [   0    1  166 1335]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8946    0.7704    0.8278      1498\n",
      "           1     0.8029    0.8347    0.8185      1567\n",
      "           2     0.8329    0.9942    0.9064      1549\n",
      "           3     0.9904    0.8888    0.9368      1502\n",
      "\n",
      "    accuracy                         0.8726      6116\n",
      "   macro avg     0.8802    0.8720    0.8724      6116\n",
      "weighted avg     0.8790    0.8726    0.8721      6116\n",
      "\n",
      "Top Rules from RuleFit Model:\n",
      "                                                   rule  type      coef  \\\n",
      "2362  autocorrelation > -0.0024353615008294582 & sig...  rule -0.147717   \n",
      "1609  max_value > 2.8537813425064087 & kurtosis > -0...  rule  0.127445   \n",
      "2304  mean > 1.0546014308929443 & geometric_mean > 0...  rule -0.112176   \n",
      "1304  geometric_mean <= 0.0011798902414739132 & perc...  rule  0.121715   \n",
      "978   autocorrelation > -0.08615213632583618 & skewn...  rule  0.118250   \n",
      "\n",
      "       support  importance  \n",
      "2362  0.259868    0.064783  \n",
      "1609  0.349507    0.060768  \n",
      "2304  0.420230    0.055369  \n",
      "1304  0.277961    0.054527  \n",
      "978   0.305099    0.054448  \n",
      "Rules have been saved to E:/Abroad period research/new idea implementation codes/Second part of the paper/26 features results/rulefit_rules_on_statistical_features.txt.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rulefit import RuleFit\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "input_dir = 'E:/Abroad period research/new idea implementation codes/Second part of the paper/26 features results/statistical_features'\n",
    "\n",
    "# Load the CSV files\n",
    "train_data = pd.read_csv(os.path.join(input_dir, \"train_stat_features.csv\"))\n",
    "val_data = pd.read_csv(os.path.join(input_dir, \"val_stat_features.csv\"))\n",
    "test_data = pd.read_csv(os.path.join(input_dir, \"test_stat_features.csv\"))\n",
    "\n",
    "# Separate features and labels for each dataset\n",
    "train_features = train_data.drop(columns=['label']).values\n",
    "train_labels = train_data['label'].values\n",
    "\n",
    "val_features = val_data.drop(columns=['label']).values\n",
    "val_labels = val_data['label'].values\n",
    "\n",
    "test_features = test_data.drop(columns=['label']).values\n",
    "test_labels = test_data['label'].values\n",
    "\n",
    "# Combine training and validation data for final training\n",
    "train_val_features = np.vstack([train_features, val_features])\n",
    "train_val_labels = np.hstack([train_labels, val_labels])\n",
    "\n",
    "print(f\"Combined Training + Validation Features Shape: {train_val_features.shape}\")\n",
    "print(f\"Combined Training + Validation Labels Shape: {train_val_labels.shape}\")\n",
    "\n",
    "# Define feature names dynamically based on the training data\n",
    "feature_names = train_data.columns[:-1].tolist()  # Exclude the label column\n",
    "\n",
    "# Initialize RuleFit model\n",
    "rf = RuleFit(tree_size=3, sample_fract=0.7, max_rules=3000, random_state=42)\n",
    "\n",
    "# Fit the RuleFit model to combined training and validation data\n",
    "rf.fit(train_val_features, train_val_labels, feature_names=feature_names)\n",
    "\n",
    "# Predict on the test set (continuous values)\n",
    "test_predictions = rf.predict(test_features)\n",
    "\n",
    "# Convert continuous predictions to discrete class labels by rounding\n",
    "test_predictions_discrete = np.round(test_predictions).astype(int)\n",
    "\n",
    "# Ensure the predicted labels are within the valid range of classes\n",
    "test_predictions_discrete = np.clip(test_predictions_discrete, np.min(train_val_labels), np.max(train_val_labels))\n",
    "\n",
    "# Calculate testing accuracy\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions_discrete)\n",
    "print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(test_labels, test_predictions_discrete)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(test_labels, test_predictions_discrete, digits=4)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Extract rules from the RuleFit model\n",
    "rules = rf.get_rules()\n",
    "rules = rules[rules.coef != 0].sort_values(\"importance\", ascending=False)\n",
    "\n",
    "# Display the top rules\n",
    "print(\"Top Rules from RuleFit Model:\")\n",
    "print(rules.head())\n",
    "\n",
    "# Save rules to a text file\n",
    "output_file_path = 'E:/Abroad period research/new idea implementation codes/Second part of the paper/26 features results/rulefit_rules_on_statistical_features.txt'\n",
    "rules.to_csv(output_file_path, index=False)\n",
    "print(f\"Rules have been saved to {output_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for evaluating the statistical features (Mutual information and feature importance appraoch)\n",
    "I used Mutual Information and Feature Importance to directly assess how well your statistical features relate to the class labels. If many features have low MI scores or feature importance values close to zero, they may not be contributing to the classification, and we might need to generate new features or revisit the feature extraction method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Feature  MI Score\n",
      "22     harmonic_mean  0.606683\n",
      "5           skewness  0.598683\n",
      "18       coef_of_var  0.573632\n",
      "17   signal_to_noise  0.573415\n",
      "23    geometric_mean  0.552168\n",
      "6           kurtosis  0.542938\n",
      "7            entropy  0.533806\n",
      "16     percentile_75  0.402670\n",
      "13               iqr  0.402660\n",
      "0               mean  0.375342\n",
      "19   autocorrelation  0.358632\n",
      "21  root_mean_square  0.322491\n",
      "8             energy  0.322483\n",
      "20   shannon_entropy  0.313630\n",
      "12         max_value  0.306126\n",
      "4              range  0.306105\n",
      "2           variance  0.298579\n",
      "10      mean_abs_dev  0.298576\n",
      "9           contrast  0.298568\n",
      "1            std_dev  0.298555\n",
      "24    std_error_mean  0.298528\n",
      "3             median  0.266166\n",
      "25    median_abs_dev  0.265641\n",
      "15     percentile_50  0.260394\n",
      "14     percentile_25  0.008844\n",
      "11         min_value  0.000000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# Load the statistical features and labels from CSV\n",
    "stat_feature_dir = 'E:/Abroad period research/new idea implementation codes/Second part of the paper/26 features results/statistical_features'\n",
    "\n",
    "# Load the CSV file\n",
    "train_df = pd.read_csv(f\"{stat_feature_dir}/train_stat_features.csv\")\n",
    "\n",
    "# Separate features and labels\n",
    "train_features = train_df.drop(columns=['label']).values  # Drop the 'label' column to get features\n",
    "train_labels = train_df['label'].values  # Extract the labels\n",
    "\n",
    "# Names of the statistical features (based on the columns in the CSV)\n",
    "stat_feature_names = train_df.columns[:-1]  # All columns except 'label' are feature names\n",
    "\n",
    "# 1. Mutual Information (MI) for each feature\n",
    "# Mutual Information helps to see which features carry the most information about the target\n",
    "mi_scores = mutual_info_classif(train_features, train_labels, discrete_features=False)\n",
    "mi_scores_df = pd.DataFrame({'Feature': stat_feature_names, 'MI Score': mi_scores})\n",
    "mi_scores_df.sort_values(by='MI Score', ascending=False, inplace=True)\n",
    "\n",
    "# Display MI scores for each feature\n",
    "print(mi_scores_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop features with more than 50% of data 0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with more than 50% zeros in any dataset: ['median', 'median_abs_dev', 'min_value', 'percentile_25', 'percentile_50']\n",
      "Filtered features have been saved in the 'filtered_statistical_features' directory.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define directories\n",
    "input_dir = 'E:/Abroad period research/new idea implementation codes/Second part of the paper/26 features results/statistical_features'\n",
    "output_dir = 'E:/Abroad period research/new idea implementation codes/Second part of the paper/26 features results/filtered_statistical_features'\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load datasets\n",
    "train_data = pd.read_csv(os.path.join(input_dir, \"train_stat_features.csv\"))\n",
    "val_data = pd.read_csv(os.path.join(input_dir, \"val_stat_features.csv\"))\n",
    "test_data = pd.read_csv(os.path.join(input_dir, \"test_stat_features.csv\"))\n",
    "\n",
    "# Function to find features with more than 50% zeros\n",
    "def find_zero_features(df):\n",
    "    zero_features = [col for col in df.columns if (df[col] == 0).mean() > 0.5]\n",
    "    return zero_features\n",
    "\n",
    "# Identify features with more than 50% zeros in each dataset\n",
    "train_zero_features = find_zero_features(train_data)\n",
    "val_zero_features = find_zero_features(val_data)\n",
    "test_zero_features = find_zero_features(test_data)\n",
    "\n",
    "# Union of features with more than 50% zeros across all datasets\n",
    "all_zero_features = set(train_zero_features).union(set(val_zero_features)).union(set(test_zero_features))\n",
    "\n",
    "# Display features with more than 50% zeros in any dataset\n",
    "print(f\"Features with more than 50% zeros in any dataset: {list(all_zero_features)}\")\n",
    "\n",
    "# Drop these features from all datasets\n",
    "train_filtered = train_data.drop(columns=all_zero_features)\n",
    "val_filtered = val_data.drop(columns=all_zero_features)\n",
    "test_filtered = test_data.drop(columns=all_zero_features)\n",
    "\n",
    "# Save filtered datasets\n",
    "train_filtered.to_csv(os.path.join(output_dir, \"filtered_train_stat_features.csv\"), index=False)\n",
    "val_filtered.to_csv(os.path.join(output_dir, \"filtered_val_stat_features.csv\"), index=False)\n",
    "test_filtered.to_csv(os.path.join(output_dir, \"filtered_test_stat_features.csv\"), index=False)\n",
    "\n",
    "print(\"Filtered features have been saved in the 'filtered_statistical_features' directory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tree evaluation using filtered_statistical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Training Accuracy: 91.6801%\n",
      "Combined Training Confusion Matrix:\n",
      " [[7332 1283   61   18]\n",
      " [ 663 7857   88   17]\n",
      " [  43  113 8102  385]\n",
      " [  10   24  178 8478]]\n",
      "Combined Training Classification Report:\n",
      "  Class 0: Precision: 0.9110, Recall: 0.8433, F1-Score: 0.8759\n",
      "  Class 1: Precision: 0.8469, Recall: 0.9110, F1-Score: 0.8778\n",
      "  Class 2: Precision: 0.9612, Recall: 0.9374, F1-Score: 0.9492\n",
      "  Class 3: Precision: 0.9528, Recall: 0.9756, F1-Score: 0.9641\n",
      "  Accuracy: 0.9168\n",
      "  Class macro avg: Precision: 0.9180, Recall: 0.9168, F1-Score: 0.9167\n",
      "  Class weighted avg: Precision: 0.9181, Recall: 0.9168, F1-Score: 0.9167\n",
      "\n",
      "Test Accuracy: 88.9307%\n",
      "Test Confusion Matrix:\n",
      " [[1204  253   38    3]\n",
      " [ 159 1310   84   14]\n",
      " [   0    0 1537   12]\n",
      " [   4    3  107 1388]]\n",
      "Test Classification Report:\n",
      "  Class 0: Precision: 0.8808, Recall: 0.8037, F1-Score: 0.8405\n",
      "  Class 1: Precision: 0.8365, Recall: 0.8360, F1-Score: 0.8363\n",
      "  Class 2: Precision: 0.8703, Recall: 0.9923, F1-Score: 0.9273\n",
      "  Class 3: Precision: 0.9795, Recall: 0.9241, F1-Score: 0.9510\n",
      "  Accuracy: 0.8893\n",
      "  Class macro avg: Precision: 0.8918, Recall: 0.8890, F1-Score: 0.8888\n",
      "  Class weighted avg: Precision: 0.8910, Recall: 0.8893, F1-Score: 0.8885\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "stat_feature_dir = 'E:/Abroad period research/new idea implementation codes/Second part of the paper/26 features results/filtered_statistical_features'\n",
    "\n",
    "# Load the CSV files\n",
    "train_df = pd.read_csv(os.path.join(stat_feature_dir, \"filtered_train_stat_features.csv\"))\n",
    "val_df = pd.read_csv(os.path.join(stat_feature_dir, \"filtered_val_stat_features.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(stat_feature_dir, \"filtered_test_stat_features.csv\"))\n",
    "\n",
    "# Separate features and labels\n",
    "train_stat_features = train_df.drop(columns=['label']).values\n",
    "train_labels = train_df['label'].values\n",
    "\n",
    "val_stat_features = val_df.drop(columns=['label']).values\n",
    "val_labels = val_df['label'].values\n",
    "\n",
    "test_stat_features = test_df.drop(columns=['label']).values\n",
    "test_labels = test_df['label'].values\n",
    "\n",
    "# Combine training and validation data for final training\n",
    "combined_features = np.vstack([train_stat_features, val_stat_features])\n",
    "combined_labels = np.hstack([train_labels, val_labels])\n",
    "\n",
    "# Train Decision Tree Classifier with specified hyperparameters\n",
    "clf = DecisionTreeClassifier(\n",
    "    random_state=0,\n",
    "    max_depth=10,\n",
    "    min_samples_leaf=4,\n",
    "    min_samples_split=2\n",
    ")\n",
    "clf.fit(combined_features, combined_labels)\n",
    "\n",
    "# Function to print classification report with four decimal points\n",
    "def print_classification_report(set_name, labels, predictions):\n",
    "    report = classification_report(labels, predictions, output_dict=True)\n",
    "    print(f\"{set_name} Classification Report:\")\n",
    "    for label, metrics in report.items():\n",
    "        if label == 'accuracy':\n",
    "            print(f\"  Accuracy: {metrics:.4f}\")\n",
    "        else:\n",
    "            print(f\"  Class {label}: Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, F1-Score: {metrics['f1-score']:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Evaluate on the combined training set\n",
    "train_predictions = clf.predict(combined_features)\n",
    "train_accuracy = accuracy_score(combined_labels, train_predictions)\n",
    "print(f\"Combined Training Accuracy: {train_accuracy * 100:.4f}%\")\n",
    "print(f\"Combined Training Confusion Matrix:\\n\", confusion_matrix(combined_labels, train_predictions))\n",
    "print_classification_report(\"Combined Training\", combined_labels, train_predictions)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_predictions = clf.predict(test_stat_features)\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.4f}%\")\n",
    "print(f\"Test Confusion Matrix:\\n\", confusion_matrix(test_labels, test_predictions))\n",
    "print_classification_report(\"Test\", test_labels, test_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rulefit evaluation on filtered statistical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Training + Validation Features Shape: (34652, 21)\n",
      "Combined Training + Validation Labels Shape: (34652,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.504e+02, tolerance: 4.343e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.8710\n",
      "Confusion Matrix:\n",
      "[[1145  327   26    0]\n",
      " [ 151 1307  106    3]\n",
      " [   0    2 1538    9]\n",
      " [   0    1  164 1337]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8835    0.7644    0.8196      1498\n",
      "           1     0.7984    0.8341    0.8159      1567\n",
      "           2     0.8386    0.9929    0.9093      1549\n",
      "           3     0.9911    0.8901    0.9379      1502\n",
      "\n",
      "    accuracy                         0.8710      6116\n",
      "   macro avg     0.8779    0.8704    0.8707      6116\n",
      "weighted avg     0.8768    0.8710    0.8704      6116\n",
      "\n",
      "Top Rules from RuleFit Model:\n",
      "                                                   rule  type      coef  \\\n",
      "1057  autocorrelation > -0.048355357721447945 & coef...  rule  0.120761   \n",
      "691   mean > 0.8756935596466064 & coef_of_var <= 1.2...  rule -0.113350   \n",
      "826   mean > 1.0546014308929443 & geometric_mean > 0...  rule -0.106838   \n",
      "58    iqr <= 3.206506848335266 & geometric_mean <= 0...  rule  0.117154   \n",
      "1550  autocorrelation <= 0.10110809653997421 & mean ...  rule  0.107728   \n",
      "\n",
      "       support  importance  \n",
      "1057  0.411184    0.059420  \n",
      "691   0.470395    0.056575  \n",
      "826   0.420230    0.052735  \n",
      "58    0.277961    0.052484  \n",
      "1550  0.349507    0.051366  \n",
      "Rules have been saved to E:/Abroad period research/new idea implementation codes/Second part of the paper/26 features results/rulefit_rules_on_filtered_statistical_features.txt.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rulefit import RuleFit\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "input_dir = 'E:/Abroad period research/new idea implementation codes/Second part of the paper/26 features results/filtered_statistical_features'\n",
    "\n",
    "# Load the CSV files\n",
    "train_data = pd.read_csv(os.path.join(stat_feature_dir, \"filtered_train_stat_features.csv\"))\n",
    "val_data = pd.read_csv(os.path.join(stat_feature_dir, \"filtered_val_stat_features.csv\"))\n",
    "test_data = pd.read_csv(os.path.join(stat_feature_dir, \"filtered_test_stat_features.csv\"))\n",
    "\n",
    "# Separate features and labels for each dataset\n",
    "train_features = train_data.drop(columns=['label']).values\n",
    "train_labels = train_data['label'].values\n",
    "\n",
    "val_features = val_data.drop(columns=['label']).values\n",
    "val_labels = val_data['label'].values\n",
    "\n",
    "test_features = test_data.drop(columns=['label']).values\n",
    "test_labels = test_data['label'].values\n",
    "\n",
    "# Combine training and validation data for final training\n",
    "train_val_features = np.vstack([train_features, val_features])\n",
    "train_val_labels = np.hstack([train_labels, val_labels])\n",
    "\n",
    "print(f\"Combined Training + Validation Features Shape: {train_val_features.shape}\")\n",
    "print(f\"Combined Training + Validation Labels Shape: {train_val_labels.shape}\")\n",
    "\n",
    "# Define feature names dynamically based on the training data\n",
    "feature_names = train_data.columns[:-1].tolist()  # Exclude the label column\n",
    "\n",
    "# Initialize RuleFit model\n",
    "rf = RuleFit(tree_size=3, sample_fract=0.7, max_rules=3000, random_state=42)\n",
    "\n",
    "# Fit the RuleFit model to combined training and validation data\n",
    "rf.fit(train_val_features, train_val_labels, feature_names=feature_names)\n",
    "\n",
    "# Predict on the test set (continuous values)\n",
    "test_predictions = rf.predict(test_features)\n",
    "\n",
    "# Convert continuous predictions to discrete class labels by rounding\n",
    "test_predictions_discrete = np.round(test_predictions).astype(int)\n",
    "\n",
    "# Ensure the predicted labels are within the valid range of classes\n",
    "test_predictions_discrete = np.clip(test_predictions_discrete, np.min(train_val_labels), np.max(train_val_labels))\n",
    "\n",
    "# Calculate testing accuracy\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions_discrete)\n",
    "print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(test_labels, test_predictions_discrete)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(test_labels, test_predictions_discrete, digits=4)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Extract rules from the RuleFit model\n",
    "rules = rf.get_rules()\n",
    "rules = rules[rules.coef != 0].sort_values(\"importance\", ascending=False)\n",
    "\n",
    "# Display the top rules\n",
    "print(\"Top Rules from RuleFit Model:\")\n",
    "print(rules.head())\n",
    "\n",
    "# Save rules to a text file\n",
    "output_file_path = 'E:/Abroad period research/new idea implementation codes/Second part of the paper/26 features results/rulefit_rules_on_filtered_statistical_features.txt'\n",
    "rules.to_csv(output_file_path, index=False)\n",
    "print(f\"Rules have been saved to {output_file_path}.\")\n",
    "\t\t\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "applying mutual information based approach to select 18 best best features among filtered_statistical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features based on training set: Index(['mean', 'std_dev', 'range', 'skewness', 'kurtosis', 'entropy', 'energy',\n",
      "       'mean_abs_dev', 'max_value', 'iqr', 'percentile_75', 'signal_to_noise',\n",
      "       'coef_of_var', 'autocorrelation', 'shannon_entropy', 'root_mean_square',\n",
      "       'harmonic_mean', 'geometric_mean'],\n",
      "      dtype='object')\n",
      "Feature selection completed and saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# Define the input directory where the selected statistical features are saved\n",
    "input_dir = 'E:/Abroad period research/new idea implementation codes/Second part of the paper/26 features results/filtered_statistical_features'\n",
    "\n",
    "# Define the output directory for saving selected features after CFS\n",
    "output_dir = 'E:/Abroad period research/new idea implementation codes/Second part of the paper/26 features results/18 best features'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Function to apply CFS and save selected features\n",
    "def apply_mic_and_save(data, selected_columns, output_name):\n",
    "    # Separate features and labels\n",
    "    X = data[selected_columns]  # Use only the selected features\n",
    "    y = data['label']  # Labels\n",
    "    \n",
    "    # Add the label back to the selected features\n",
    "    selected_data = pd.concat([X, y], axis=1)\n",
    "    \n",
    "    # Save the selected features with labels\n",
    "    selected_data.to_csv(os.path.join(output_dir, output_name), index=False)\n",
    "\n",
    "# Step 1: Perform feature selection on the training set\n",
    "train_data = pd.read_csv(os.path.join(input_dir, \"filtered_train_stat_features.csv\"))\n",
    "\n",
    "# Separate features and labels for the training set\n",
    "X_train = train_data.drop(columns=['label'])  # Features\n",
    "y_train = train_data['label']  # Labels\n",
    "\n",
    "# Use mutual information on training data to select top 18 features\n",
    "selector = SelectKBest(score_func=mutual_info_classif, k=18)  # Select top 18 features\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "# Get the selected feature names based on training set\n",
    "selected_columns = X_train.columns[selector.get_support()]\n",
    "print(f\"Selected features based on training set: {selected_columns}\")\n",
    "\n",
    "# Step 2: Apply the same selected features to the training, validation, and testing datasets\n",
    "\n",
    "# Apply CFS to the training set (using selected features)\n",
    "apply_mic_and_save(train_data, selected_columns, \"18_training_selected_features.csv\")\n",
    "\n",
    "# Apply the same selected features to the validation set\n",
    "val_data = pd.read_csv(os.path.join(input_dir, \"filtered_val_stat_features.csv\"))\n",
    "apply_mic_and_save(val_data, selected_columns, \"18_validation_selected_features.csv\")\n",
    "\n",
    "# Apply the same selected features to the testing set\n",
    "test_data = pd.read_csv(os.path.join(input_dir, \"filtered_test_stat_features.csv\"))\n",
    "apply_mic_and_save(test_data, selected_columns, \"18_testing_selected_features.csv\")\n",
    "\n",
    "print(\"Feature selection completed and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating decision tree using 18 selected statistical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Training Accuracy: 91.6744%\n",
      "Combined Training Confusion Matrix:\n",
      " [[7329 1286   61   18]\n",
      " [ 662 7858   88   17]\n",
      " [  43  113 8102  385]\n",
      " [  10   24  178 8478]]\n",
      "Combined Training Classification Report:\n",
      "  Class 0: Precision: 0.9111, Recall: 0.8430, F1-Score: 0.8757\n",
      "  Class 1: Precision: 0.8467, Recall: 0.9111, F1-Score: 0.8777\n",
      "  Class 2: Precision: 0.9612, Recall: 0.9374, F1-Score: 0.9492\n",
      "  Class 3: Precision: 0.9528, Recall: 0.9756, F1-Score: 0.9641\n",
      "  Accuracy: 0.9167\n",
      "  Class macro avg: Precision: 0.9179, Recall: 0.9168, F1-Score: 0.9167\n",
      "  Class weighted avg: Precision: 0.9180, Recall: 0.9167, F1-Score: 0.9167\n",
      "\n",
      "Test Accuracy: 88.8816%\n",
      "Test Confusion Matrix:\n",
      " [[1202  256   37    3]\n",
      " [ 160 1310   83   14]\n",
      " [   0    0 1537   12]\n",
      " [   4    3  108 1387]]\n",
      "Test Classification Report:\n",
      "  Class 0: Precision: 0.8799, Recall: 0.8024, F1-Score: 0.8394\n",
      "  Class 1: Precision: 0.8349, Recall: 0.8360, F1-Score: 0.8355\n",
      "  Class 2: Precision: 0.8708, Recall: 0.9923, F1-Score: 0.9276\n",
      "  Class 3: Precision: 0.9795, Recall: 0.9234, F1-Score: 0.9507\n",
      "  Accuracy: 0.8888\n",
      "  Class macro avg: Precision: 0.8913, Recall: 0.8885, F1-Score: 0.8883\n",
      "  Class weighted avg: Precision: 0.8906, Recall: 0.8888, F1-Score: 0.8880\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# # Define the directory containing the CSV files\n",
    "# stat_feature_dir = 'E:/Abroad period research/new idea implementation codes/Second part of the paper/26 features results/18 best features'\n",
    "\n",
    "# # Load the CSV files\n",
    "# train_df = pd.read_csv(os.path.join(stat_feature_dir, \"18_training_selected_features.csv\"))\n",
    "# val_df = pd.read_csv(os.path.join(stat_feature_dir, \"18_validation_selected_features.csv\"))\n",
    "# test_df = pd.read_csv(os.path.join(stat_feature_dir, \"18_testing_selected_features.csv\"))\n",
    "\n",
    "# # Separate features and labels\n",
    "# train_stat_features = train_df.drop(columns=['label']).values\n",
    "# train_labels = train_df['label'].values\n",
    "\n",
    "# val_stat_features = val_df.drop(columns=['label']).values\n",
    "# val_labels = val_df['label'].values\n",
    "\n",
    "# test_stat_features = test_df.drop(columns=['label']).values\n",
    "# test_labels = test_df['label'].values\n",
    "\n",
    "# # Combine training and validation data for final training\n",
    "# combined_features = np.vstack([train_stat_features, val_stat_features])\n",
    "# combined_labels = np.hstack([train_labels, val_labels])\n",
    "\n",
    "# # Train Decision Tree Classifier on combined training and validation data\n",
    "# clf = DecisionTreeClassifier(random_state=0)\n",
    "# clf.fit(combined_features, combined_labels)\n",
    "\n",
    "# # Function to print classification report with four decimal points\n",
    "# def print_classification_report(set_name, labels, predictions):\n",
    "#     report = classification_report(labels, predictions, output_dict=True)\n",
    "#     print(f\"{set_name} Classification Report:\")\n",
    "#     for label, metrics in report.items():\n",
    "#         if label == 'accuracy':\n",
    "#             print(f\"  Accuracy: {metrics:.4f}\")\n",
    "#         else:\n",
    "#             print(f\"  Class {label}: Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, F1-Score: {metrics['f1-score']:.4f}\")\n",
    "#     print()\n",
    "\n",
    "# # Evaluate on the combined training set\n",
    "# train_predictions = clf.predict(combined_features)\n",
    "# train_accuracy = accuracy_score(combined_labels, train_predictions)\n",
    "# print(f\"Combined Training Accuracy: {train_accuracy * 100:.4f}%\")\n",
    "# print(f\"Combined Training Confusion Matrix:\\n\", confusion_matrix(combined_labels, train_predictions))\n",
    "# print_classification_report(\"Combined Training\", combined_labels, train_predictions)\n",
    "\n",
    "# # Evaluate on the test set\n",
    "# test_predictions = clf.predict(test_stat_features)\n",
    "# test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "# print(f\"Test Accuracy: {test_accuracy * 100:.4f}%\")\n",
    "# print(f\"Test Confusion Matrix:\\n\", confusion_matrix(test_labels, test_predictions))\n",
    "# print_classification_report(\"Test\", test_labels, test_predictions)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "stat_feature_dir = 'E:/Abroad period research/new idea implementation codes/Second part of the paper/26 features results/18 best features'\n",
    "\n",
    "# Load the CSV files\n",
    "train_df = pd.read_csv(os.path.join(stat_feature_dir, \"18_training_selected_features.csv\"))\n",
    "val_df = pd.read_csv(os.path.join(stat_feature_dir, \"18_validation_selected_features.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(stat_feature_dir, \"18_testing_selected_features.csv\"))\n",
    "\n",
    "# Separate features and labels\n",
    "train_stat_features = train_df.drop(columns=['label']).values\n",
    "train_labels = train_df['label'].values\n",
    "\n",
    "val_stat_features = val_df.drop(columns=['label']).values\n",
    "val_labels = val_df['label'].values\n",
    "\n",
    "test_stat_features = test_df.drop(columns=['label']).values\n",
    "test_labels = test_df['label'].values\n",
    "\n",
    "# Combine training and validation data for final training\n",
    "combined_features = np.vstack([train_stat_features, val_stat_features])\n",
    "combined_labels = np.hstack([train_labels, val_labels])\n",
    "\n",
    "# Train Decision Tree Classifier with specified hyperparameters on combined training and validation data\n",
    "clf = DecisionTreeClassifier(\n",
    "    random_state=0,\n",
    "    max_depth=10,\n",
    "    min_samples_leaf=4,\n",
    "    min_samples_split=2\n",
    ")\n",
    "clf.fit(combined_features, combined_labels)\n",
    "\n",
    "# Function to print classification report with four decimal points\n",
    "def print_classification_report(set_name, labels, predictions):\n",
    "    report = classification_report(labels, predictions, output_dict=True)\n",
    "    print(f\"{set_name} Classification Report:\")\n",
    "    for label, metrics in report.items():\n",
    "        if label == 'accuracy':\n",
    "            print(f\"  Accuracy: {metrics:.4f}\")\n",
    "        else:\n",
    "            print(f\"  Class {label}: Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, F1-Score: {metrics['f1-score']:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Evaluate on the combined training set\n",
    "train_predictions = clf.predict(combined_features)\n",
    "train_accuracy = accuracy_score(combined_labels, train_predictions)\n",
    "print(f\"Combined Training Accuracy: {train_accuracy * 100:.4f}%\")\n",
    "print(f\"Combined Training Confusion Matrix:\\n\", confusion_matrix(combined_labels, train_predictions))\n",
    "print_classification_report(\"Combined Training\", combined_labels, train_predictions)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_predictions = clf.predict(test_stat_features)\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.4f}%\")\n",
    "print(f\"Test Confusion Matrix:\\n\", confusion_matrix(test_labels, test_predictions))\n",
    "print_classification_report(\"Test\", test_labels, test_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Rulefit on 18 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features Shape: (28537, 18)\n",
      "Validation Features Shape: (6115, 18)\n",
      "Testing Features Shape: (6116, 18)\n",
      "Combined Training + Validation Features Shape: (34652, 18)\n",
      "Combined Training + Validation Labels Shape: (34652,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.845e+02, tolerance: 4.343e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.8698\n",
      "Confusion Matrix:\n",
      "[[1145  329   24    0]\n",
      " [ 159 1296  109    3]\n",
      " [   0    0 1540    9]\n",
      " [   0    1  162 1339]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8781    0.7644    0.8173      1498\n",
      "           1     0.7970    0.8271    0.8118      1567\n",
      "           2     0.8392    0.9942    0.9102      1549\n",
      "           3     0.9911    0.8915    0.9387      1502\n",
      "\n",
      "    accuracy                         0.8698      6116\n",
      "   macro avg     0.8764    0.8693    0.8695      6116\n",
      "weighted avg     0.8752    0.8698    0.8692      6116\n",
      "\n",
      "Top Rules from RuleFit Model:\n",
      "                                                   rule  type      coef  \\\n",
      "242           root_mean_square <= 0.0009085230121854693  rule  0.164117   \n",
      "322                       kurtosis > 3.9242721796035767  rule -0.130895   \n",
      "2473  root_mean_square <= 0.0011798902414739132 & me...  rule  0.130629   \n",
      "390   percentile_75 <= 1.2989734411239624 & mean > 0...  rule -0.116706   \n",
      "2321  percentile_75 > 1.3156236410140991 & signal_to...  rule  0.112444   \n",
      "\n",
      "       support  importance  \n",
      "242   0.384868    0.079853  \n",
      "322   0.606908    0.063934  \n",
      "2473  0.277961    0.058521  \n",
      "390   0.470395    0.058250  \n",
      "2321  0.411184    0.055328  \n",
      "Rules have been saved to E:/Abroad period research/new idea implementation codes/Second part of the paper/26 features results/rulefit_rules_on_18_statistical_features.txt.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rulefit import RuleFit\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "input_dir = 'E:/Abroad period research/new idea implementation codes/Second part of the paper/26 features results/18 best features'\n",
    "\n",
    "# Load the CSV files\n",
    "train_df = pd.read_csv(os.path.join(input_dir, \"18_training_selected_features.csv\"))\n",
    "val_df = pd.read_csv(os.path.join(input_dir, \"18_validation_selected_features.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(input_dir, \"18_testing_selected_features.csv\"))\n",
    "\n",
    "\n",
    "# Separate features and labels for each dataset\n",
    "train_features = train_df.drop(columns=['label']).values\n",
    "train_labels = train_df['label'].values\n",
    "print(f\"Training Features Shape: {train_features.shape}\")\n",
    "\n",
    "val_features = val_df.drop(columns=['label']).values\n",
    "val_labels = val_df['label'].values\n",
    "print(f\"Validation Features Shape: {val_features.shape}\")\n",
    "\n",
    "test_features = test_df.drop(columns=['label']).values\n",
    "test_labels = test_df['label'].values\n",
    "print(f\"Testing Features Shape: {test_features.shape}\")\n",
    "\n",
    "# Combine training and validation data for final training\n",
    "train_val_features = np.vstack([train_features, val_features])\n",
    "train_val_labels = np.hstack([train_labels, val_labels])\n",
    "\n",
    "print(f\"Combined Training + Validation Features Shape: {train_val_features.shape}\")\n",
    "print(f\"Combined Training + Validation Labels Shape: {train_val_labels.shape}\")\n",
    "\n",
    "# Define feature names dynamically based on the training data\n",
    "feature_names = train_data.columns[:-1].tolist()  # Exclude the label column\n",
    "\n",
    "# Initialize RuleFit model\n",
    "rf = RuleFit(tree_size=3, sample_fract=0.7, max_rules=3000, random_state=42)\n",
    "\n",
    "# Fit the RuleFit model to combined training and validation data\n",
    "rf.fit(train_val_features, train_val_labels, feature_names=feature_names)\n",
    "\n",
    "# Predict on the test set (continuous values)\n",
    "test_predictions = rf.predict(test_features)\n",
    "\n",
    "# Convert continuous predictions to discrete class labels by rounding\n",
    "test_predictions_discrete = np.round(test_predictions).astype(int)\n",
    "\n",
    "# Ensure the predicted labels are within the valid range of classes\n",
    "test_predictions_discrete = np.clip(test_predictions_discrete, np.min(train_val_labels), np.max(train_val_labels))\n",
    "\n",
    "# Calculate testing accuracy\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions_discrete)\n",
    "print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(test_labels, test_predictions_discrete)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(test_labels, test_predictions_discrete, digits=4)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Extract rules from the RuleFit model\n",
    "rules = rf.get_rules()\n",
    "rules = rules[rules.coef != 0].sort_values(\"importance\", ascending=False)\n",
    "\n",
    "# Display the top rules\n",
    "print(\"Top Rules from RuleFit Model:\")\n",
    "print(rules.head())\n",
    "\n",
    "# Save rules to a text file\n",
    "output_file_path = 'E:/Abroad period research/new idea implementation codes/Second part of the paper/26 features results/rulefit_rules_on_18_statistical_features.txt'\n",
    "rules.to_csv(output_file_path, index=False)\n",
    "print(f\"Rules have been saved to {output_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting 15 statistical features based on mutual importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features based on training set: Index(['mean', 'range', 'skewness', 'kurtosis', 'entropy', 'energy', 'iqr',\n",
      "       'percentile_75', 'signal_to_noise', 'coef_of_var', 'autocorrelation',\n",
      "       'shannon_entropy', 'root_mean_square', 'harmonic_mean',\n",
      "       'geometric_mean'],\n",
      "      dtype='object')\n",
      "Feature selection completed and saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# Define the input directory where the selected statistical features are saved\n",
    "input_dir = 'E:/Abroad period research/new idea implementation codes/Second part of the paper/26 features results/filtered_statistical_features'\n",
    "\n",
    "# Define the output directory for saving selected features after CFS\n",
    "output_dir = 'E:/Abroad period research/new idea implementation codes/Second part of the paper/26 features results/15 best features'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Function to apply CFS and save selected features\n",
    "def apply_mic_and_save(data, selected_columns, output_name):\n",
    "    # Separate features and labels\n",
    "    X = data[selected_columns]  # Use only the selected features\n",
    "    y = data['label']  # Labels\n",
    "    \n",
    "    # Add the label back to the selected features\n",
    "    selected_data = pd.concat([X, y], axis=1)\n",
    "    \n",
    "    # Save the selected features with labels\n",
    "    selected_data.to_csv(os.path.join(output_dir, output_name), index=False)\n",
    "\n",
    "# Step 1: Perform feature selection on the training set\n",
    "train_data = pd.read_csv(os.path.join(input_dir, \"filtered_train_stat_features.csv\"))\n",
    "\n",
    "# Separate features and labels for the training set\n",
    "X_train = train_data.drop(columns=['label'])  # Features\n",
    "y_train = train_data['label']  # Labels\n",
    "\n",
    "# Use mutual information on training data to select top 3 features\n",
    "selector = SelectKBest(score_func=mutual_info_classif, k=15)  # Select top 3 features\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "# Get the selected feature names based on training set\n",
    "selected_columns = X_train.columns[selector.get_support()]\n",
    "print(f\"Selected features based on training set: {selected_columns}\")\n",
    "\n",
    "# Step 2: Apply the same selected features to the training, validation, and testing datasets\n",
    "\n",
    "# Apply CFS to the training set (using selected features)\n",
    "apply_mic_and_save(train_data, selected_columns, \"15_training_selected_features.csv\")\n",
    "\n",
    "# Apply the same selected features to the validation set\n",
    "val_data = pd.read_csv(os.path.join(input_dir, \"filtered_val_stat_features.csv\"))\n",
    "apply_mic_and_save(val_data, selected_columns, \"15_validation_selected_features.csv\")\n",
    "\n",
    "# Apply the same selected features to the testing set\n",
    "test_data = pd.read_csv(os.path.join(input_dir, \"filtered_test_stat_features.csv\"))\n",
    "apply_mic_and_save(test_data, selected_columns, \"15_testing_selected_features.csv\")\n",
    "\n",
    "print(\"Feature selection completed and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree evaluation using 15 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Training Accuracy: 91.8966%\n",
      "Combined Training Confusion Matrix:\n",
      " [[7465 1128   77   24]\n",
      " [ 763 7717  123   22]\n",
      " [  43   56 8235  309]\n",
      " [   8   18  237 8427]]\n",
      "Combined Training Classification Report:\n",
      "  Class 0: Precision: 0.9017, Recall: 0.8586, F1-Score: 0.8796\n",
      "  Class 1: Precision: 0.8652, Recall: 0.8947, F1-Score: 0.8797\n",
      "  Class 2: Precision: 0.9496, Recall: 0.9528, F1-Score: 0.9512\n",
      "  Class 3: Precision: 0.9596, Recall: 0.9697, F1-Score: 0.9646\n",
      "  Accuracy: 0.9190\n",
      "  Class macro avg: Precision: 0.9190, Recall: 0.9190, F1-Score: 0.9188\n",
      "  Class weighted avg: Precision: 0.9191, Recall: 0.9190, F1-Score: 0.9188\n",
      "\n",
      "Test Accuracy: 88.4565%\n",
      "Test Confusion Matrix:\n",
      " [[1216  234   42    6]\n",
      " [ 167 1282   99   19]\n",
      " [   1    0 1537   11]\n",
      " [   3    3  121 1375]]\n",
      "Test Classification Report:\n",
      "  Class 0: Precision: 0.8767, Recall: 0.8117, F1-Score: 0.8430\n",
      "  Class 1: Precision: 0.8440, Recall: 0.8181, F1-Score: 0.8308\n",
      "  Class 2: Precision: 0.8544, Recall: 0.9923, F1-Score: 0.9182\n",
      "  Class 3: Precision: 0.9745, Recall: 0.9154, F1-Score: 0.9440\n",
      "  Accuracy: 0.8846\n",
      "  Class macro avg: Precision: 0.8874, Recall: 0.8844, F1-Score: 0.8840\n",
      "  Class weighted avg: Precision: 0.8867, Recall: 0.8846, F1-Score: 0.8837\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "stat_feature_dir = 'E:/Abroad period research/new idea implementation codes/Second part of the paper/26 features results/15 best features'\n",
    "\n",
    "# Load the CSV files\n",
    "train_df = pd.read_csv(os.path.join(stat_feature_dir, \"15_training_selected_features.csv\"))\n",
    "val_df = pd.read_csv(os.path.join(stat_feature_dir, \"15_validation_selected_features.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(stat_feature_dir, \"15_testing_selected_features.csv\"))\n",
    "\n",
    "# Separate features and labels\n",
    "train_stat_features = train_df.drop(columns=['label']).values\n",
    "train_labels = train_df['label'].values\n",
    "\n",
    "val_stat_features = val_df.drop(columns=['label']).values\n",
    "val_labels = val_df['label'].values\n",
    "\n",
    "test_stat_features = test_df.drop(columns=['label']).values\n",
    "test_labels = test_df['label'].values\n",
    "\n",
    "# Combine training and validation data for final training\n",
    "combined_features = np.vstack([train_stat_features, val_stat_features])\n",
    "combined_labels = np.hstack([train_labels, val_labels])\n",
    "\n",
    "# Train Decision Tree Classifier with specified hyperparameters on combined training and validation data\n",
    "clf = DecisionTreeClassifier(\n",
    "    random_state=0,\n",
    "    max_depth=10,\n",
    "    min_samples_leaf=4,\n",
    "    min_samples_split=2\n",
    ")\n",
    "clf.fit(combined_features, combined_labels)\n",
    "\n",
    "# Function to print classification report with four decimal points\n",
    "def print_classification_report(set_name, labels, predictions):\n",
    "    report = classification_report(labels, predictions, output_dict=True)\n",
    "    print(f\"{set_name} Classification Report:\")\n",
    "    for label, metrics in report.items():\n",
    "        if label == 'accuracy':\n",
    "            print(f\"  Accuracy: {metrics:.4f}\")\n",
    "        else:\n",
    "            print(f\"  Class {label}: Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, F1-Score: {metrics['f1-score']:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Evaluate on the combined training set\n",
    "train_predictions = clf.predict(combined_features)\n",
    "train_accuracy = accuracy_score(combined_labels, train_predictions)\n",
    "print(f\"Combined Training Accuracy: {train_accuracy * 100:.4f}%\")\n",
    "print(f\"Combined Training Confusion Matrix:\\n\", confusion_matrix(combined_labels, train_predictions))\n",
    "print_classification_report(\"Combined Training\", combined_labels, train_predictions)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_predictions = clf.predict(test_stat_features)\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.4f}%\")\n",
    "print(f\"Test Confusion Matrix:\\n\", confusion_matrix(test_labels, test_predictions))\n",
    "print_classification_report(\"Test\", test_labels, test_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rulefit evaluation using 15 best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features Shape: (28537, 15)\n",
      "Validation Features Shape: (6115, 15)\n",
      "Testing Features Shape: (6116, 15)\n",
      "Combined Training + Validation Features Shape: (34652, 15)\n",
      "Combined Training + Validation Labels Shape: (34652,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.368e+02, tolerance: 4.343e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.8721\n",
      "Confusion Matrix:\n",
      "[[1155  319   24    0]\n",
      " [ 154 1299  111    3]\n",
      " [   0    0 1540    9]\n",
      " [   0    1  161 1340]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8824    0.7710    0.8229      1498\n",
      "           1     0.8023    0.8290    0.8154      1567\n",
      "           2     0.8388    0.9942    0.9099      1549\n",
      "           3     0.9911    0.8921    0.9390      1502\n",
      "\n",
      "    accuracy                         0.8721      6116\n",
      "   macro avg     0.8787    0.8716    0.8718      6116\n",
      "weighted avg     0.8775    0.8721    0.8716      6116\n",
      "\n",
      "Top Rules from RuleFit Model:\n",
      "                                                   rule  type      coef  \\\n",
      "2270  energy <= 3.206506848335266 & coef_of_var <= 0...  rule  0.136075   \n",
      "731   range > -0.3833502531051636 & mean > 0.8013095...  rule  0.117491   \n",
      "419   mean > 0.9142641723155975 & contrast > 0.76917...  rule -0.111519   \n",
      "1000                coef_of_var > 0.0009085230121854693  rule -0.107847   \n",
      "315                       contrast > 0.7662046849727631  rule -0.096561   \n",
      "\n",
      "       support  importance  \n",
      "2270  0.277961    0.060961  \n",
      "731   0.349507    0.056021  \n",
      "419   0.434211    0.055275  \n",
      "1000  0.615132    0.052474  \n",
      "315   0.571546    0.047784  \n",
      "Rules have been saved to E:/Abroad period research/new idea implementation codes/Second part of the paper/26 features results/rulefit_rules_on_15_statistical_features.txt.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rulefit import RuleFit\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "input_dir = 'E:/Abroad period research/new idea implementation codes/Second part of the paper/26 features results/15 best features'\n",
    "\n",
    "# Load the CSV files\n",
    "train_df = pd.read_csv(os.path.join(input_dir, \"15_training_selected_features.csv\"))\n",
    "val_df = pd.read_csv(os.path.join(input_dir, \"15_validation_selected_features.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(input_dir, \"15_testing_selected_features.csv\"))\n",
    "\n",
    "\n",
    "# Separate features and labels for each dataset\n",
    "train_features = train_df.drop(columns=['label']).values\n",
    "train_labels = train_df['label'].values\n",
    "print(f\"Training Features Shape: {train_features.shape}\")\n",
    "\n",
    "val_features = val_df.drop(columns=['label']).values\n",
    "val_labels = val_df['label'].values\n",
    "print(f\"Validation Features Shape: {val_features.shape}\")\n",
    "\n",
    "test_features = test_df.drop(columns=['label']).values\n",
    "test_labels = test_df['label'].values\n",
    "print(f\"Testing Features Shape: {test_features.shape}\")\n",
    "\n",
    "# Combine training and validation data for final training\n",
    "train_val_features = np.vstack([train_features, val_features])\n",
    "train_val_labels = np.hstack([train_labels, val_labels])\n",
    "\n",
    "print(f\"Combined Training + Validation Features Shape: {train_val_features.shape}\")\n",
    "print(f\"Combined Training + Validation Labels Shape: {train_val_labels.shape}\")\n",
    "\n",
    "# Define feature names dynamically based on the training data\n",
    "feature_names = train_data.columns[:-1].tolist()  # Exclude the label column\n",
    "\n",
    "# Initialize RuleFit model\n",
    "rf = RuleFit(tree_size=3, sample_fract=0.7, max_rules=3000, random_state=42)\n",
    "\n",
    "# Fit the RuleFit model to combined training and validation data\n",
    "rf.fit(train_val_features, train_val_labels, feature_names=feature_names)\n",
    "\n",
    "# Predict on the test set (continuous values)\n",
    "test_predictions = rf.predict(test_features)\n",
    "\n",
    "# Convert continuous predictions to discrete class labels by rounding\n",
    "test_predictions_discrete = np.round(test_predictions).astype(int)\n",
    "\n",
    "# Ensure the predicted labels are within the valid range of classes\n",
    "test_predictions_discrete = np.clip(test_predictions_discrete, np.min(train_val_labels), np.max(train_val_labels))\n",
    "\n",
    "# Calculate testing accuracy\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions_discrete)\n",
    "print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(test_labels, test_predictions_discrete)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(test_labels, test_predictions_discrete, digits=4)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Extract rules from the RuleFit model\n",
    "rules = rf.get_rules()\n",
    "rules = rules[rules.coef != 0].sort_values(\"importance\", ascending=False)\n",
    "\n",
    "# Display the top rules\n",
    "print(\"Top Rules from RuleFit Model:\")\n",
    "print(rules.head())\n",
    "\n",
    "# Save rules to a text file\n",
    "output_file_path = 'E:/Abroad period research/new idea implementation codes/Second part of the paper/26 features results/rulefit_rules_on_15_statistical_features.txt'\n",
    "rules.to_csv(output_file_path, index=False)\n",
    "print(f\"Rules have been saved to {output_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "selecting 12 most important features among filtered features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features based on training set: Index(['mean', 'skewness', 'kurtosis', 'entropy', 'iqr', 'percentile_75',\n",
      "       'signal_to_noise', 'coef_of_var', 'autocorrelation', 'root_mean_square',\n",
      "       'harmonic_mean', 'geometric_mean'],\n",
      "      dtype='object')\n",
      "Feature selection completed and saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# Define the input directory where the selected statistical features are saved\n",
    "input_dir = 'E:/Abroad period research/new idea implementation codes/Second part of the paper/26 features results/filtered_statistical_features'\n",
    "\n",
    "# Define the output directory for saving selected features after CFS\n",
    "output_dir = 'E:/Abroad period research/new idea implementation codes/Second part of the paper/26 features results/12 best features'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Function to apply CFS and save selected features\n",
    "def apply_mic_and_save(data, selected_columns, output_name):\n",
    "    # Separate features and labels\n",
    "    X = data[selected_columns]  # Use only the selected features\n",
    "    y = data['label']  # Labels\n",
    "    \n",
    "    # Add the label back to the selected features\n",
    "    selected_data = pd.concat([X, y], axis=1)\n",
    "    \n",
    "    # Save the selected features with labels\n",
    "    selected_data.to_csv(os.path.join(output_dir, output_name), index=False)\n",
    "\n",
    "# Step 1: Perform feature selection on the training set\n",
    "train_data = pd.read_csv(os.path.join(input_dir, \"filtered_train_stat_features.csv\"))\n",
    "\n",
    "# Separate features and labels for the training set\n",
    "X_train = train_data.drop(columns=['label'])  # Features\n",
    "y_train = train_data['label']  # Labels\n",
    "\n",
    "# Use mutual information on training data to select top 3 features\n",
    "selector = SelectKBest(score_func=mutual_info_classif, k=12)  # Select top 3 features\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "# Get the selected feature names based on training set\n",
    "selected_columns = X_train.columns[selector.get_support()]\n",
    "print(f\"Selected features based on training set: {selected_columns}\")\n",
    "\n",
    "# Step 2: Apply the same selected features to the training, validation, and testing datasets\n",
    "\n",
    "# Apply CFS to the training set (using selected features)\n",
    "apply_mic_and_save(train_data, selected_columns, \"12_training_selected_features.csv\")\n",
    "\n",
    "# Apply the same selected features to the validation set\n",
    "val_data = pd.read_csv(os.path.join(input_dir, \"filtered_val_stat_features.csv\"))\n",
    "apply_mic_and_save(val_data, selected_columns, \"12_validation_selected_features.csv\")\n",
    "\n",
    "# Apply the same selected features to the testing set\n",
    "test_data = pd.read_csv(os.path.join(input_dir, \"filtered_test_stat_features.csv\"))\n",
    "apply_mic_and_save(test_data, selected_columns, \"12_testing_selected_features.csv\")\n",
    "\n",
    "print(\"Feature selection completed and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluating decision tree on 12 most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Training Accuracy: 91.8446%\n",
      "Combined Training Confusion Matrix:\n",
      " [[7473 1120   77   24]\n",
      " [ 769 7700  131   25]\n",
      " [  44   47 8207  345]\n",
      " [   6   16  222 8446]]\n",
      "Combined Training Classification Report:\n",
      "  Class 0: Precision: 0.9012, Recall: 0.8596, F1-Score: 0.8799\n",
      "  Class 1: Precision: 0.8668, Recall: 0.8928, F1-Score: 0.8796\n",
      "  Class 2: Precision: 0.9502, Recall: 0.9496, F1-Score: 0.9499\n",
      "  Class 3: Precision: 0.9554, Recall: 0.9719, F1-Score: 0.9636\n",
      "  Accuracy: 0.9184\n",
      "  Class macro avg: Precision: 0.9184, Recall: 0.9184, F1-Score: 0.9182\n",
      "  Class weighted avg: Precision: 0.9185, Recall: 0.9184, F1-Score: 0.9183\n",
      "\n",
      "Test Accuracy: 88.2276%\n",
      "Test Confusion Matrix:\n",
      " [[1210  241   41    6]\n",
      " [ 167 1273  106   21]\n",
      " [   1    0 1537   11]\n",
      " [   3    2  121 1376]]\n",
      "Test Classification Report:\n",
      "  Class 0: Precision: 0.8762, Recall: 0.8077, F1-Score: 0.8406\n",
      "  Class 1: Precision: 0.8397, Recall: 0.8124, F1-Score: 0.8258\n",
      "  Class 2: Precision: 0.8515, Recall: 0.9923, F1-Score: 0.9165\n",
      "  Class 3: Precision: 0.9731, Recall: 0.9161, F1-Score: 0.9438\n",
      "  Accuracy: 0.8823\n",
      "  Class macro avg: Precision: 0.8851, Recall: 0.8821, F1-Score: 0.8817\n",
      "  Class weighted avg: Precision: 0.8844, Recall: 0.8823, F1-Score: 0.8814\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "stat_feature_dir = 'E:/Abroad period research/new idea implementation codes/Second part of the paper/26 features results/12 best features'\n",
    "\n",
    "# Load the CSV files\n",
    "train_df = pd.read_csv(os.path.join(stat_feature_dir, \"12_training_selected_features.csv\"))\n",
    "val_df = pd.read_csv(os.path.join(stat_feature_dir, \"12_validation_selected_features.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(stat_feature_dir, \"12_testing_selected_features.csv\"))\n",
    "\n",
    "# Separate features and labels\n",
    "train_stat_features = train_df.drop(columns=['label']).values\n",
    "train_labels = train_df['label'].values\n",
    "\n",
    "val_stat_features = val_df.drop(columns=['label']).values\n",
    "val_labels = val_df['label'].values\n",
    "\n",
    "test_stat_features = test_df.drop(columns=['label']).values\n",
    "test_labels = test_df['label'].values\n",
    "\n",
    "# Combine training and validation data for final training\n",
    "combined_features = np.vstack([train_stat_features, val_stat_features])\n",
    "combined_labels = np.hstack([train_labels, val_labels])\n",
    "\n",
    "# Train Decision Tree Classifier with specified hyperparameters on combined training and validation data\n",
    "clf = DecisionTreeClassifier(\n",
    "    random_state=0,\n",
    "    max_depth=10,\n",
    "    min_samples_leaf=4,\n",
    "    min_samples_split=2\n",
    ")\n",
    "clf.fit(combined_features, combined_labels)\n",
    "\n",
    "# Function to print classification report with four decimal points\n",
    "def print_classification_report(set_name, labels, predictions):\n",
    "    report = classification_report(labels, predictions, output_dict=True)\n",
    "    print(f\"{set_name} Classification Report:\")\n",
    "    for label, metrics in report.items():\n",
    "        if label == 'accuracy':\n",
    "            print(f\"  Accuracy: {metrics:.4f}\")\n",
    "        else:\n",
    "            print(f\"  Class {label}: Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, F1-Score: {metrics['f1-score']:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Evaluate on the combined training set\n",
    "train_predictions = clf.predict(combined_features)\n",
    "train_accuracy = accuracy_score(combined_labels, train_predictions)\n",
    "print(f\"Combined Training Accuracy: {train_accuracy * 100:.4f}%\")\n",
    "print(f\"Combined Training Confusion Matrix:\\n\", confusion_matrix(combined_labels, train_predictions))\n",
    "print_classification_report(\"Combined Training\", combined_labels, train_predictions)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_predictions = clf.predict(test_stat_features)\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.4f}%\")\n",
    "print(f\"Test Confusion Matrix:\\n\", confusion_matrix(test_labels, test_predictions))\n",
    "print_classification_report(\"Test\", test_labels, test_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evalutaion rulefit on 12 most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features Shape: (28537, 12)\n",
      "Validation Features Shape: (6115, 12)\n",
      "Testing Features Shape: (6116, 12)\n",
      "Combined Training + Validation Features Shape: (34652, 12)\n",
      "Combined Training + Validation Labels Shape: (34652,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.969e+02, tolerance: 4.343e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.8659\n",
      "Confusion Matrix:\n",
      "[[1107  363   28    0]\n",
      " [ 128 1318  118    3]\n",
      " [   0    2 1541    6]\n",
      " [   0    0  172 1330]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8964    0.7390    0.8101      1498\n",
      "           1     0.7831    0.8411    0.8111      1567\n",
      "           2     0.8289    0.9948    0.9043      1549\n",
      "           3     0.9933    0.8855    0.9363      1502\n",
      "\n",
      "    accuracy                         0.8659      6116\n",
      "   macro avg     0.8754    0.8651    0.8655      6116\n",
      "weighted avg     0.8741    0.8659    0.8652      6116\n",
      "\n",
      "Top Rules from RuleFit Model:\n",
      "                                                  rule  type      coef  \\\n",
      "183  mean > 0.9056200385093689 & skewness <= 3.2065...  rule  0.171093   \n",
      "998  contrast > -0.0024353615008294582 & energy <= ...  rule -0.146447   \n",
      "299                         energy > 1.299041211605072  rule  0.097731   \n",
      "842                        range <= 3.9242721796035767  rule  0.095917   \n",
      "275  iqr > 0.0014559084665961564 & mean_abs_dev > 1...  rule -0.093948   \n",
      "\n",
      "      support  importance  \n",
      "183  0.277961    0.076649  \n",
      "998  0.259868    0.064226  \n",
      "299  0.464638    0.048743  \n",
      "842  0.393092    0.046850  \n",
      "275  0.384868    0.045712  \n",
      "Rules have been saved to E:/Abroad period research/new idea implementation codes/Second part of the paper/26 features results/rulefit_rules_on_12_statistical_features.txt.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rulefit import RuleFit\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "input_dir = 'E:/Abroad period research/new idea implementation codes/Second part of the paper/26 features results/12 best features'\n",
    "\n",
    "# Load the CSV files\n",
    "train_df = pd.read_csv(os.path.join(input_dir, \"12_training_selected_features.csv\"))\n",
    "val_df = pd.read_csv(os.path.join(input_dir, \"12_validation_selected_features.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(input_dir, \"12_testing_selected_features.csv\"))\n",
    "\n",
    "\n",
    "# Separate features and labels for each dataset\n",
    "train_features = train_df.drop(columns=['label']).values\n",
    "train_labels = train_df['label'].values\n",
    "print(f\"Training Features Shape: {train_features.shape}\")\n",
    "\n",
    "val_features = val_df.drop(columns=['label']).values\n",
    "val_labels = val_df['label'].values\n",
    "print(f\"Validation Features Shape: {val_features.shape}\")\n",
    "\n",
    "test_features = test_df.drop(columns=['label']).values\n",
    "test_labels = test_df['label'].values\n",
    "print(f\"Testing Features Shape: {test_features.shape}\")\n",
    "\n",
    "# Combine training and validation data for final training\n",
    "train_val_features = np.vstack([train_features, val_features])\n",
    "train_val_labels = np.hstack([train_labels, val_labels])\n",
    "\n",
    "print(f\"Combined Training + Validation Features Shape: {train_val_features.shape}\")\n",
    "print(f\"Combined Training + Validation Labels Shape: {train_val_labels.shape}\")\n",
    "\n",
    "# Define feature names dynamically based on the training data\n",
    "feature_names = train_data.columns[:-1].tolist()  # Exclude the label column\n",
    "\n",
    "# Initialize RuleFit model\n",
    "rf = RuleFit(tree_size=3, sample_fract=0.7, max_rules=3000, random_state=42)\n",
    "\n",
    "# Fit the RuleFit model to combined training and validation data\n",
    "rf.fit(train_val_features, train_val_labels, feature_names=feature_names)\n",
    "\n",
    "# Predict on the test set (continuous values)\n",
    "test_predictions = rf.predict(test_features)\n",
    "\n",
    "# Convert continuous predictions to discrete class labels by rounding\n",
    "test_predictions_discrete = np.round(test_predictions).astype(int)\n",
    "\n",
    "# Ensure the predicted labels are within the valid range of classes\n",
    "test_predictions_discrete = np.clip(test_predictions_discrete, np.min(train_val_labels), np.max(train_val_labels))\n",
    "\n",
    "# Calculate testing accuracy\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions_discrete)\n",
    "print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(test_labels, test_predictions_discrete)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(test_labels, test_predictions_discrete, digits=4)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Extract rules from the RuleFit model\n",
    "rules = rf.get_rules()\n",
    "rules = rules[rules.coef != 0].sort_values(\"importance\", ascending=False)\n",
    "\n",
    "# Display the top rules\n",
    "print(\"Top Rules from RuleFit Model:\")\n",
    "print(rules.head())\n",
    "\n",
    "# Save rules to a text file\n",
    "output_file_path = 'E:/Abroad period research/new idea implementation codes/Second part of the paper/26 features results/rulefit_rules_on_12_statistical_features.txt'\n",
    "rules.to_csv(output_file_path, index=False)\n",
    "print(f\"Rules have been saved to {output_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selection 9 best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features based on training set: Index(['skewness', 'kurtosis', 'entropy', 'iqr', 'percentile_75',\n",
      "       'signal_to_noise', 'coef_of_var', 'harmonic_mean', 'geometric_mean'],\n",
      "      dtype='object')\n",
      "Feature selection completed and saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# Define the input directory where the selected statistical features are saved\n",
    "input_dir = 'E:/Abroad period research/new idea implementation codes/Second part of the paper/26 features results/filtered_statistical_features'\n",
    "\n",
    "# Define the output directory for saving selected features after CFS\n",
    "output_dir = 'E:/Abroad period research/new idea implementation codes/Second part of the paper/26 features results/9 best features'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Function to apply CFS and save selected features\n",
    "def apply_mic_and_save(data, selected_columns, output_name):\n",
    "    # Separate features and labels\n",
    "    X = data[selected_columns]  # Use only the selected features\n",
    "    y = data['label']  # Labels\n",
    "    \n",
    "    # Add the label back to the selected features\n",
    "    selected_data = pd.concat([X, y], axis=1)\n",
    "    \n",
    "    # Save the selected features with labels\n",
    "    selected_data.to_csv(os.path.join(output_dir, output_name), index=False)\n",
    "\n",
    "# Step 1: Perform feature selection on the training set\n",
    "train_data = pd.read_csv(os.path.join(input_dir, \"filtered_train_stat_features.csv\"))\n",
    "\n",
    "# Separate features and labels for the training set\n",
    "X_train = train_data.drop(columns=['label'])  # Features\n",
    "y_train = train_data['label']  # Labels\n",
    "\n",
    "# Use mutual information on training data to select top 3 features\n",
    "selector = SelectKBest(score_func=mutual_info_classif, k=9)  # Select top 3 features\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "# Get the selected feature names based on training set\n",
    "selected_columns = X_train.columns[selector.get_support()]\n",
    "print(f\"Selected features based on training set: {selected_columns}\")\n",
    "\n",
    "# Step 2: Apply the same selected features to the training, validation, and testing datasets\n",
    "\n",
    "# Apply CFS to the training set (using selected features)\n",
    "apply_mic_and_save(train_data, selected_columns, \"9_training_selected_features.csv\")\n",
    "\n",
    "# Apply the same selected features to the validation set\n",
    "val_data = pd.read_csv(os.path.join(input_dir, \"filtered_val_stat_features.csv\"))\n",
    "apply_mic_and_save(val_data, selected_columns, \"9_validation_selected_features.csv\")\n",
    "\n",
    "# Apply the same selected features to the testing set\n",
    "test_data = pd.read_csv(os.path.join(input_dir, \"filtered_test_stat_features.csv\"))\n",
    "apply_mic_and_save(test_data, selected_columns, \"9_testing_selected_features.csv\")\n",
    "\n",
    "print(\"Feature selection completed and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree evaluation on 9 selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Training Accuracy: 86.2778%\n",
      "Combined Training Confusion Matrix:\n",
      " [[7295 1242  149    8]\n",
      " [2086 6311  208   20]\n",
      " [  76  137 7969  461]\n",
      " [   5   32  331 8322]]\n",
      "Combined Training Classification Report:\n",
      "  Class 0: Precision: 0.7710, Recall: 0.8391, F1-Score: 0.8036\n",
      "  Class 1: Precision: 0.8173, Recall: 0.7317, F1-Score: 0.7721\n",
      "  Class 2: Precision: 0.9205, Recall: 0.9220, F1-Score: 0.9213\n",
      "  Class 3: Precision: 0.9445, Recall: 0.9577, F1-Score: 0.9510\n",
      "  Accuracy: 0.8628\n",
      "  Class macro avg: Precision: 0.8633, Recall: 0.8626, F1-Score: 0.8620\n",
      "  Class weighted avg: Precision: 0.8633, Recall: 0.8628, F1-Score: 0.8621\n",
      "\n",
      "Test Accuracy: 83.8129%\n",
      "Test Confusion Matrix:\n",
      " [[1251  182   64    1]\n",
      " [ 373 1008  168   18]\n",
      " [   0    0 1535   14]\n",
      " [   1    8  161 1332]]\n",
      "Test Classification Report:\n",
      "  Class 0: Precision: 0.7698, Recall: 0.8351, F1-Score: 0.8012\n",
      "  Class 1: Precision: 0.8414, Recall: 0.6433, F1-Score: 0.7291\n",
      "  Class 2: Precision: 0.7962, Recall: 0.9910, F1-Score: 0.8829\n",
      "  Class 3: Precision: 0.9758, Recall: 0.8868, F1-Score: 0.9292\n",
      "  Accuracy: 0.8381\n",
      "  Class macro avg: Precision: 0.8458, Recall: 0.8390, F1-Score: 0.8356\n",
      "  Class weighted avg: Precision: 0.8454, Recall: 0.8381, F1-Score: 0.8349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "stat_feature_dir = 'E:/Abroad period research/new idea implementation codes/Second part of the paper/26 features results/9 best features'\n",
    "\n",
    "# Load the CSV files\n",
    "train_df = pd.read_csv(os.path.join(stat_feature_dir, \"9_training_selected_features.csv\"))\n",
    "val_df = pd.read_csv(os.path.join(stat_feature_dir, \"9_validation_selected_features.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(stat_feature_dir, \"9_testing_selected_features.csv\"))\n",
    "\n",
    "# Separate features and labels\n",
    "train_stat_features = train_df.drop(columns=['label']).values\n",
    "train_labels = train_df['label'].values\n",
    "\n",
    "val_stat_features = val_df.drop(columns=['label']).values\n",
    "val_labels = val_df['label'].values\n",
    "\n",
    "test_stat_features = test_df.drop(columns=['label']).values\n",
    "test_labels = test_df['label'].values\n",
    "\n",
    "# Combine training and validation data for final training\n",
    "combined_features = np.vstack([train_stat_features, val_stat_features])\n",
    "combined_labels = np.hstack([train_labels, val_labels])\n",
    "\n",
    "# Train Decision Tree Classifier with specified hyperparameters on combined training and validation data\n",
    "clf = DecisionTreeClassifier(\n",
    "    random_state=0,\n",
    "    max_depth=10,\n",
    "    min_samples_leaf=4,\n",
    "    min_samples_split=2\n",
    ")\n",
    "clf.fit(combined_features, combined_labels)\n",
    "\n",
    "# Function to print classification report with four decimal points\n",
    "def print_classification_report(set_name, labels, predictions):\n",
    "    report = classification_report(labels, predictions, output_dict=True)\n",
    "    print(f\"{set_name} Classification Report:\")\n",
    "    for label, metrics in report.items():\n",
    "        if label == 'accuracy':\n",
    "            print(f\"  Accuracy: {metrics:.4f}\")\n",
    "        else:\n",
    "            print(f\"  Class {label}: Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, F1-Score: {metrics['f1-score']:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Evaluate on the combined training set\n",
    "train_predictions = clf.predict(combined_features)\n",
    "train_accuracy = accuracy_score(combined_labels, train_predictions)\n",
    "print(f\"Combined Training Accuracy: {train_accuracy * 100:.4f}%\")\n",
    "print(f\"Combined Training Confusion Matrix:\\n\", confusion_matrix(combined_labels, train_predictions))\n",
    "print_classification_report(\"Combined Training\", combined_labels, train_predictions)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_predictions = clf.predict(test_stat_features)\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.4f}%\")\n",
    "print(f\"Test Confusion Matrix:\\n\", confusion_matrix(test_labels, test_predictions))\n",
    "print_classification_report(\"Test\", test_labels, test_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rulefit evaluation using 9 best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features Shape: (28537, 9)\n",
      "Validation Features Shape: (6115, 9)\n",
      "Testing Features Shape: (6116, 9)\n",
      "Combined Training + Validation Features Shape: (34652, 9)\n",
      "Combined Training + Validation Labels Shape: (34652,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.325042266757009, tolerance: 2.902724756504043\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.1577732085852404, tolerance: 2.902724756504043\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.89943237832631, tolerance: 2.902724756504043\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.0877216961798695, tolerance: 2.8783463596225434\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.919872609891627, tolerance: 2.8783463596225434\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.695e+02, tolerance: 4.343e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.8229\n",
      "Confusion Matrix:\n",
      "[[1053  424   21    0]\n",
      " [ 234 1216  110    7]\n",
      " [   0    3 1521   25]\n",
      " [   0    0  259 1243]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8182    0.7029    0.7562      1498\n",
      "           1     0.7401    0.7760    0.7576      1567\n",
      "           2     0.7959    0.9819    0.8792      1549\n",
      "           3     0.9749    0.8276    0.8952      1502\n",
      "\n",
      "    accuracy                         0.8229      6116\n",
      "   macro avg     0.8323    0.8221    0.8221      6116\n",
      "weighted avg     0.8310    0.8229    0.8219      6116\n",
      "\n",
      "Top Rules from RuleFit Model:\n",
      "                                                   rule    type  \\\n",
      "0                                                  mean  linear   \n",
      "7                                                energy  linear   \n",
      "221   kurtosis <= 0.721629410982132 & contrast <= 0....    rule   \n",
      "1048  skewness > 2.124513626098633 & variance > 3.90...    rule   \n",
      "2135  mean > 0.9969204664230347 & contrast <= 0.0068...    rule   \n",
      "\n",
      "               coef   support  importance  \n",
      "0         -0.854395  1.000000    0.244837  \n",
      "7     572619.868437  1.000000    0.122467  \n",
      "221        0.182381  0.154605    0.065936  \n",
      "1048      -0.125587  0.416941    0.061921  \n",
      "2135      -0.122247  0.439967    0.060682  \n",
      "Rules have been saved to E:/Abroad period research/new idea implementation codes/Second part of the paper/26 features results/rulefit_rules_on_9_statistical_features.txt.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rulefit import RuleFit\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "input_dir = 'E:/Abroad period research/new idea implementation codes/Second part of the paper/26 features results/9 best features'\n",
    "\n",
    "# Load the CSV files\n",
    "train_df = pd.read_csv(os.path.join(input_dir, \"9_training_selected_features.csv\"))\n",
    "val_df = pd.read_csv(os.path.join(input_dir, \"9_validation_selected_features.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(input_dir, \"9_testing_selected_features.csv\"))\n",
    "\n",
    "\n",
    "# Separate features and labels for each dataset\n",
    "train_features = train_df.drop(columns=['label']).values\n",
    "train_labels = train_df['label'].values\n",
    "print(f\"Training Features Shape: {train_features.shape}\")\n",
    "\n",
    "val_features = val_df.drop(columns=['label']).values\n",
    "val_labels = val_df['label'].values\n",
    "print(f\"Validation Features Shape: {val_features.shape}\")\n",
    "\n",
    "test_features = test_df.drop(columns=['label']).values\n",
    "test_labels = test_df['label'].values\n",
    "print(f\"Testing Features Shape: {test_features.shape}\")\n",
    "\n",
    "# Combine training and validation data for final training\n",
    "train_val_features = np.vstack([train_features, val_features])\n",
    "train_val_labels = np.hstack([train_labels, val_labels])\n",
    "\n",
    "print(f\"Combined Training + Validation Features Shape: {train_val_features.shape}\")\n",
    "print(f\"Combined Training + Validation Labels Shape: {train_val_labels.shape}\")\n",
    "\n",
    "# Define feature names dynamically based on the training data\n",
    "feature_names = train_data.columns[:-1].tolist()  # Exclude the label column\n",
    "\n",
    "# Initialize RuleFit model\n",
    "rf = RuleFit(tree_size=3, sample_fract=0.7, max_rules=3000, random_state=42)\n",
    "\n",
    "# Fit the RuleFit model to combined training and validation data\n",
    "rf.fit(train_val_features, train_val_labels, feature_names=feature_names)\n",
    "\n",
    "# Predict on the test set (continuous values)\n",
    "test_predictions = rf.predict(test_features)\n",
    "\n",
    "# Convert continuous predictions to discrete class labels by rounding\n",
    "test_predictions_discrete = np.round(test_predictions).astype(int)\n",
    "\n",
    "# Ensure the predicted labels are within the valid range of classes\n",
    "test_predictions_discrete = np.clip(test_predictions_discrete, np.min(train_val_labels), np.max(train_val_labels))\n",
    "\n",
    "# Calculate testing accuracy\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions_discrete)\n",
    "print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(test_labels, test_predictions_discrete)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(test_labels, test_predictions_discrete, digits=4)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Extract rules from the RuleFit model\n",
    "rules = rf.get_rules()\n",
    "rules = rules[rules.coef != 0].sort_values(\"importance\", ascending=False)\n",
    "\n",
    "# Display the top rules\n",
    "print(\"Top Rules from RuleFit Model:\")\n",
    "print(rules.head())\n",
    "\n",
    "# Save rules to a text file\n",
    "output_file_path = 'E:/Abroad period research/new idea implementation codes/Second part of the paper/26 features results/rulefit_rules_on_9_statistical_features.txt'\n",
    "rules.to_csv(output_file_path, index=False)\n",
    "print(f\"Rules have been saved to {output_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting 6 most important features based on mutual importance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features based on training set: Index(['skewness', 'kurtosis', 'signal_to_noise', 'coef_of_var',\n",
      "       'harmonic_mean', 'geometric_mean'],\n",
      "      dtype='object')\n",
      "Feature selection completed and saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# Define the input directory where the selected statistical features are saved\n",
    "input_dir = 'E:/Abroad period research/new idea implementation codes/Second part of the paper/26 features results/filtered_statistical_features'\n",
    "\n",
    "# Define the output directory for saving selected features after CFS\n",
    "output_dir = 'E:/Abroad period research/new idea implementation codes/Second part of the paper/26 features results/6 best features'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Function to apply CFS and save selected features\n",
    "def apply_mic_and_save(data, selected_columns, output_name):\n",
    "    # Separate features and labels\n",
    "    X = data[selected_columns]  # Use only the selected features\n",
    "    y = data['label']  # Labels\n",
    "    \n",
    "    # Add the label back to the selected features\n",
    "    selected_data = pd.concat([X, y], axis=1)\n",
    "    \n",
    "    # Save the selected features with labels\n",
    "    selected_data.to_csv(os.path.join(output_dir, output_name), index=False)\n",
    "\n",
    "# Step 1: Perform feature selection on the training set\n",
    "train_data = pd.read_csv(os.path.join(input_dir, \"filtered_train_stat_features.csv\"))\n",
    "\n",
    "# Separate features and labels for the training set\n",
    "X_train = train_data.drop(columns=['label'])  # Features\n",
    "y_train = train_data['label']  # Labels\n",
    "\n",
    "# Use mutual information on training data to select top 3 features\n",
    "selector = SelectKBest(score_func=mutual_info_classif, k=6)  # Select top 3 features\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "# Get the selected feature names based on training set\n",
    "selected_columns = X_train.columns[selector.get_support()]\n",
    "print(f\"Selected features based on training set: {selected_columns}\")\n",
    "\n",
    "# Step 2: Apply the same selected features to the training, validation, and testing datasets\n",
    "\n",
    "# Apply CFS to the training set (using selected features)\n",
    "apply_mic_and_save(train_data, selected_columns, \"6_training_selected_features.csv\")\n",
    "\n",
    "# Apply the same selected features to the validation set\n",
    "val_data = pd.read_csv(os.path.join(input_dir, \"filtered_val_stat_features.csv\"))\n",
    "apply_mic_and_save(val_data, selected_columns, \"6_validation_selected_features.csv\")\n",
    "\n",
    "# Apply the same selected features to the testing set\n",
    "test_data = pd.read_csv(os.path.join(input_dir, \"filtered_test_stat_features.csv\"))\n",
    "apply_mic_and_save(test_data, selected_columns, \"6_testing_selected_features.csv\")\n",
    "\n",
    "print(\"Feature selection completed and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating decision tree on 6 important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Training Accuracy: 77.3202%\n",
      "Combined Training Confusion Matrix:\n",
      " [[7190 1212  273   19]\n",
      " [2879 5349  373   24]\n",
      " [ 191  276 6568 1608]\n",
      " [   7   27  970 7686]]\n",
      "Combined Training Classification Report:\n",
      "  Class 0: Precision: 0.7003, Recall: 0.8270, F1-Score: 0.7584\n",
      "  Class 1: Precision: 0.7793, Recall: 0.6202, F1-Score: 0.6907\n",
      "  Class 2: Precision: 0.8025, Recall: 0.7599, F1-Score: 0.7807\n",
      "  Class 3: Precision: 0.8232, Recall: 0.8845, F1-Score: 0.8527\n",
      "  Accuracy: 0.7732\n",
      "  Class macro avg: Precision: 0.7763, Recall: 0.7729, F1-Score: 0.7706\n",
      "  Class weighted avg: Precision: 0.7763, Recall: 0.7732, F1-Score: 0.7707\n",
      "\n",
      "Test Accuracy: 74.4441%\n",
      "Test Confusion Matrix:\n",
      " [[1149  209  127   13]\n",
      " [ 598  800  159   10]\n",
      " [   0    0 1469   80]\n",
      " [   4    8  355 1135]]\n",
      "Test Classification Report:\n",
      "  Class 0: Precision: 0.6562, Recall: 0.7670, F1-Score: 0.7073\n",
      "  Class 1: Precision: 0.7866, Recall: 0.5105, F1-Score: 0.6192\n",
      "  Class 2: Precision: 0.6962, Recall: 0.9484, F1-Score: 0.8030\n",
      "  Class 3: Precision: 0.9168, Recall: 0.7557, F1-Score: 0.8285\n",
      "  Accuracy: 0.7444\n",
      "  Class macro avg: Precision: 0.7640, Recall: 0.7454, F1-Score: 0.7395\n",
      "  Class weighted avg: Precision: 0.7637, Recall: 0.7444, F1-Score: 0.7387\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "stat_feature_dir = 'E:/Abroad period research/new idea implementation codes/Second part of the paper/26 features results/6 best features'\n",
    "\n",
    "# Load the CSV files\n",
    "train_df = pd.read_csv(os.path.join(stat_feature_dir, \"6_training_selected_features.csv\"))\n",
    "val_df = pd.read_csv(os.path.join(stat_feature_dir, \"6_validation_selected_features.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(stat_feature_dir, \"6_testing_selected_features.csv\"))\n",
    "\n",
    "# Separate features and labels\n",
    "train_stat_features = train_df.drop(columns=['label']).values\n",
    "train_labels = train_df['label'].values\n",
    "\n",
    "val_stat_features = val_df.drop(columns=['label']).values\n",
    "val_labels = val_df['label'].values\n",
    "\n",
    "test_stat_features = test_df.drop(columns=['label']).values\n",
    "test_labels = test_df['label'].values\n",
    "\n",
    "# Combine training and validation data for final training\n",
    "combined_features = np.vstack([train_stat_features, val_stat_features])\n",
    "combined_labels = np.hstack([train_labels, val_labels])\n",
    "\n",
    "# Train Decision Tree Classifier with specified hyperparameters on combined training and validation data\n",
    "clf = DecisionTreeClassifier(\n",
    "    random_state=0,\n",
    "    max_depth=10,\n",
    "    min_samples_leaf=4,\n",
    "    min_samples_split=2\n",
    ")\n",
    "clf.fit(combined_features, combined_labels)\n",
    "\n",
    "# Function to print classification report with four decimal points\n",
    "def print_classification_report(set_name, labels, predictions):\n",
    "    report = classification_report(labels, predictions, output_dict=True)\n",
    "    print(f\"{set_name} Classification Report:\")\n",
    "    for label, metrics in report.items():\n",
    "        if label == 'accuracy':\n",
    "            print(f\"  Accuracy: {metrics:.4f}\")\n",
    "        else:\n",
    "            print(f\"  Class {label}: Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, F1-Score: {metrics['f1-score']:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Evaluate on the combined training set\n",
    "train_predictions = clf.predict(combined_features)\n",
    "train_accuracy = accuracy_score(combined_labels, train_predictions)\n",
    "print(f\"Combined Training Accuracy: {train_accuracy * 100:.4f}%\")\n",
    "print(f\"Combined Training Confusion Matrix:\\n\", confusion_matrix(combined_labels, train_predictions))\n",
    "print_classification_report(\"Combined Training\", combined_labels, train_predictions)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_predictions = clf.predict(test_stat_features)\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.4f}%\")\n",
    "print(f\"Test Confusion Matrix:\\n\", confusion_matrix(test_labels, test_predictions))\n",
    "print_classification_report(\"Test\", test_labels, test_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating Rulefit on 6 important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features Shape: (28537, 6)\n",
      "Validation Features Shape: (6115, 6)\n",
      "Testing Features Shape: (6116, 6)\n",
      "Combined Training + Validation Features Shape: (34652, 6)\n",
      "Combined Training + Validation Labels Shape: (34652,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.402618188587439, tolerance: 2.905117181074409\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.421808559258352, tolerance: 2.905117181074409\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.672180243317598, tolerance: 2.905117181074409\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.542505670053288, tolerance: 2.905117181074409\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.808800245516977, tolerance: 2.905117181074409\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7.110796341340574, tolerance: 2.905117181074409\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.493412151121447, tolerance: 2.902724756504043\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9.319459058919165, tolerance: 2.902724756504043\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9.073105702837893, tolerance: 2.902724756504043\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7.015126274747672, tolerance: 2.902724756504043\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.287999623327778, tolerance: 2.902724756504043\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.883955628437434, tolerance: 2.902724756504043\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7.719377905320471, tolerance: 2.902724756504043\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.119531349842873, tolerance: 2.8783463596225434\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.059078424783365, tolerance: 2.8783463596225434\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.456625102619, tolerance: 2.8783463596225434\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.156688233443674, tolerance: 2.8783463596225434\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.092443068278953, tolerance: 2.8783463596225434\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.874164907961131, tolerance: 2.8783463596225434\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.640e+02, tolerance: 4.343e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.7871\n",
      "Confusion Matrix:\n",
      "[[1041  430   26    1]\n",
      " [ 348 1123   93    3]\n",
      " [   0    1 1516   32]\n",
      " [   0    5  363 1134]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7495    0.6949    0.7212      1498\n",
      "           1     0.7203    0.7167    0.7185      1567\n",
      "           2     0.7588    0.9787    0.8548      1549\n",
      "           3     0.9692    0.7550    0.8488      1502\n",
      "\n",
      "    accuracy                         0.7871      6116\n",
      "   macro avg     0.7994    0.7863    0.7858      6116\n",
      "weighted avg     0.7983    0.7871    0.7857      6116\n",
      "\n",
      "Combined Training Accuracy: 0.7368\n",
      "Combined Training Confusion Matrix:\n",
      "[[6143 2475   73    3]\n",
      " [2163 6211  241   10]\n",
      " [   7  891 5671 2074]\n",
      " [   0   39 1143 7508]]\n",
      "Combined Training Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7390    0.7066    0.7224      8694\n",
      "           1     0.6459    0.7201    0.6810      8625\n",
      "           2     0.7956    0.6561    0.7192      8643\n",
      "           3     0.7825    0.8640    0.8212      8690\n",
      "\n",
      "    accuracy                         0.7368     34652\n",
      "   macro avg     0.7407    0.7367    0.7359     34652\n",
      "weighted avg     0.7408    0.7368    0.7361     34652\n",
      "\n",
      "Top Rules from RuleFit Model:\n",
      "              rule    type          coef  support  importance\n",
      "0         skewness  linear -2.410242e+00      1.0    0.690682\n",
      "4    harmonic_mean  linear  2.676165e+06      1.0    0.572355\n",
      "2  signal_to_noise  linear -4.095684e+00      1.0    0.315256\n",
      "5   geometric_mean  linear -2.929662e+02      1.0    0.292597\n",
      "3      coef_of_var  linear  1.073112e+00      1.0    0.145019\n",
      "Rules have been saved to E:/Abroad period research/new idea implementation codes/Second part of the paper/26 features results/rulefit_rules_on_6_statistical_features.txt.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rulefit import RuleFit\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "input_dir = 'E:/Abroad period research/new idea implementation codes/Second part of the paper/26 features results/6 best features'\n",
    "\n",
    "# Load the CSV files\n",
    "train_df = pd.read_csv(os.path.join(input_dir, \"6_training_selected_features.csv\"))\n",
    "val_df = pd.read_csv(os.path.join(input_dir, \"6_validation_selected_features.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(input_dir, \"6_testing_selected_features.csv\"))\n",
    "\n",
    "# Separate features and labels for each dataset\n",
    "train_features = train_df.drop(columns=['label']).values\n",
    "train_labels = train_df['label'].values\n",
    "print(f\"Training Features Shape: {train_features.shape}\")\n",
    "\n",
    "val_features = val_df.drop(columns=['label']).values\n",
    "val_labels = val_df['label'].values\n",
    "print(f\"Validation Features Shape: {val_features.shape}\")\n",
    "\n",
    "test_features = test_df.drop(columns=['label']).values\n",
    "test_labels = test_df['label'].values\n",
    "print(f\"Testing Features Shape: {test_features.shape}\")\n",
    "\n",
    "# Combine training and validation data for final training\n",
    "train_val_features = np.vstack([train_features, val_features])\n",
    "train_val_labels = np.hstack([train_labels, val_labels])\n",
    "\n",
    "print(f\"Combined Training + Validation Features Shape: {train_val_features.shape}\")\n",
    "print(f\"Combined Training + Validation Labels Shape: {train_val_labels.shape}\")\n",
    "\n",
    "# Define feature names dynamically based on the training data\n",
    "feature_names = train_df.columns[:-1].tolist()  # Exclude the label column\n",
    "\n",
    "# Initialize RuleFit model\n",
    "rf = RuleFit(tree_size=3, sample_fract=0.7, max_rules=3000, random_state=42)\n",
    "\n",
    "# Fit the RuleFit model to combined training and validation data\n",
    "rf.fit(train_val_features, train_val_labels, feature_names=feature_names)\n",
    "\n",
    "# Predict on the test set (continuous values)\n",
    "test_predictions = rf.predict(test_features)\n",
    "\n",
    "# Convert continuous predictions to discrete class labels by rounding\n",
    "test_predictions_discrete = np.round(test_predictions).astype(int)\n",
    "\n",
    "# Ensure the predicted labels are within the valid range of classes\n",
    "test_predictions_discrete = np.clip(test_predictions_discrete, np.min(train_val_labels), np.max(train_val_labels))\n",
    "\n",
    "# Calculate testing accuracy\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions_discrete)\n",
    "print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(test_labels, test_predictions_discrete)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(test_labels, test_predictions_discrete, digits=4)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Evaluate on the combined training set\n",
    "train_predictions = rf.predict(train_val_features)\n",
    "train_predictions_discrete = np.round(train_predictions).astype(int)\n",
    "train_accuracy = accuracy_score(train_val_labels, train_predictions_discrete)\n",
    "\n",
    "print(f\"Combined Training Accuracy: {train_accuracy:.4f}\")\n",
    "train_conf_matrix = confusion_matrix(train_val_labels, train_predictions_discrete)\n",
    "print(\"Combined Training Confusion Matrix:\")\n",
    "print(train_conf_matrix)\n",
    "print(\"Combined Training Classification Report:\")\n",
    "print(classification_report(train_val_labels, train_predictions_discrete, digits=4))\n",
    "\n",
    "# Extract rules from the RuleFit model\n",
    "rules = rf.get_rules()\n",
    "rules = rules[rules.coef != 0].sort_values(\"importance\", ascending=False)\n",
    "\n",
    "# Display the top rules\n",
    "print(\"Top Rules from RuleFit Model:\")\n",
    "print(rules.head())\n",
    "\n",
    "# Save rules to a text file\n",
    "output_file_path = 'E:/Abroad period research/new idea implementation codes/Second part of the paper/26 features results/rulefit_rules_on_6_statistical_features.txt'\n",
    "rules.to_csv(output_file_path, index=False)\n",
    "print(f\"Rules have been saved to {output_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting 3 most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features based on training set: Index(['skewness', 'coef_of_var', 'harmonic_mean'], dtype='object')\n",
      "Feature selection completed and saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# Define the input directory where the selected statistical features are saved\n",
    "input_dir = 'E:/Abroad period research/new idea implementation codes/Second part of the paper/26 features results/filtered_statistical_features'\n",
    "\n",
    "# Define the output directory for saving selected features after CFS\n",
    "output_dir = 'E:/Abroad period research/new idea implementation codes/Second part of the paper/26 features results/3 best features'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Function to apply CFS and save selected features\n",
    "def apply_mic_and_save(data, selected_columns, output_name):\n",
    "    # Separate features and labels\n",
    "    X = data[selected_columns]  # Use only the selected features\n",
    "    y = data['label']  # Labels\n",
    "    \n",
    "    # Add the label back to the selected features\n",
    "    selected_data = pd.concat([X, y], axis=1)\n",
    "    \n",
    "    # Save the selected features with labels\n",
    "    selected_data.to_csv(os.path.join(output_dir, output_name), index=False)\n",
    "\n",
    "# Step 1: Perform feature selection on the training set\n",
    "train_data = pd.read_csv(os.path.join(input_dir, \"filtered_train_stat_features.csv\"))\n",
    "\n",
    "# Separate features and labels for the training set\n",
    "X_train = train_data.drop(columns=['label'])  # Features\n",
    "y_train = train_data['label']  # Labels\n",
    "\n",
    "# Use mutual information on training data to select top 3 features\n",
    "selector = SelectKBest(score_func=mutual_info_classif, k=3)  # Select top 3 features\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "# Get the selected feature names based on training set\n",
    "selected_columns = X_train.columns[selector.get_support()]\n",
    "print(f\"Selected features based on training set: {selected_columns}\")\n",
    "\n",
    "# Step 2: Apply the same selected features to the training, validation, and testing datasets\n",
    "\n",
    "# Apply CFS to the training set (using selected features)\n",
    "apply_mic_and_save(train_data, selected_columns, \"3_training_selected_features.csv\")\n",
    "\n",
    "# Apply the same selected features to the validation set\n",
    "val_data = pd.read_csv(os.path.join(input_dir, \"filtered_val_stat_features.csv\"))\n",
    "apply_mic_and_save(val_data, selected_columns, \"3_validation_selected_features.csv\")\n",
    "\n",
    "# Apply the same selected features to the testing set\n",
    "test_data = pd.read_csv(os.path.join(input_dir, \"filtered_test_stat_features.csv\"))\n",
    "apply_mic_and_save(test_data, selected_columns, \"3_testing_selected_features.csv\")\n",
    "\n",
    "print(\"Feature selection completed and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree evaluation on 3 most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Training Accuracy: 71.4533%\n",
      "Combined Training Confusion Matrix:\n",
      " [[7009 1125  510   50]\n",
      " [3548 4415  590   72]\n",
      " [ 417  179 5782 2265]\n",
      " [  32  105  999 7554]]\n",
      "Combined Training Classification Report:\n",
      "  Class 0: Precision: 0.6368, Recall: 0.8062, F1-Score: 0.7116\n",
      "  Class 1: Precision: 0.7581, Recall: 0.5119, F1-Score: 0.6111\n",
      "  Class 2: Precision: 0.7337, Recall: 0.6690, F1-Score: 0.6998\n",
      "  Class 3: Precision: 0.7599, Recall: 0.8693, F1-Score: 0.8109\n",
      "  Accuracy: 0.7145\n",
      "  Class macro avg: Precision: 0.7221, Recall: 0.7141, F1-Score: 0.7084\n",
      "  Class weighted avg: Precision: 0.7220, Recall: 0.7145, F1-Score: 0.7086\n",
      "\n",
      "Test Accuracy: 67.2008%\n",
      "Test Confusion Matrix:\n",
      " [[1025  144  309   20]\n",
      " [ 698  609  237   23]\n",
      " [   0    0 1307  242]\n",
      " [   9   20  304 1169]]\n",
      "Test Classification Report:\n",
      "  Class 0: Precision: 0.5918, Recall: 0.6842, F1-Score: 0.6347\n",
      "  Class 1: Precision: 0.7878, Recall: 0.3886, F1-Score: 0.5205\n",
      "  Class 2: Precision: 0.6059, Recall: 0.8438, F1-Score: 0.7053\n",
      "  Class 3: Precision: 0.8040, Recall: 0.7783, F1-Score: 0.7909\n",
      "  Accuracy: 0.6720\n",
      "  Class macro avg: Precision: 0.6974, Recall: 0.6737, F1-Score: 0.6629\n",
      "  Class weighted avg: Precision: 0.6977, Recall: 0.6720, F1-Score: 0.6617\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "stat_feature_dir = 'E:/Abroad period research/new idea implementation codes/Second part of the paper/26 features results/3 best features'\n",
    "\n",
    "# Load the CSV files\n",
    "train_df = pd.read_csv(os.path.join(stat_feature_dir, \"3_training_selected_features.csv\"))\n",
    "val_df = pd.read_csv(os.path.join(stat_feature_dir, \"3_validation_selected_features.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(stat_feature_dir, \"3_testing_selected_features.csv\"))\n",
    "\n",
    "# Separate features and labels\n",
    "train_stat_features = train_df.drop(columns=['label']).values\n",
    "train_labels = train_df['label'].values\n",
    "\n",
    "val_stat_features = val_df.drop(columns=['label']).values\n",
    "val_labels = val_df['label'].values\n",
    "\n",
    "test_stat_features = test_df.drop(columns=['label']).values\n",
    "test_labels = test_df['label'].values\n",
    "\n",
    "# Combine training and validation data for final training\n",
    "combined_features = np.vstack([train_stat_features, val_stat_features])\n",
    "combined_labels = np.hstack([train_labels, val_labels])\n",
    "\n",
    "# Train Decision Tree Classifier with specified hyperparameters on combined training and validation data\n",
    "clf = DecisionTreeClassifier(\n",
    "    random_state=0,\n",
    "    max_depth=10,\n",
    "    min_samples_leaf=4,\n",
    "    min_samples_split=2\n",
    ")\n",
    "clf.fit(combined_features, combined_labels)\n",
    "\n",
    "# Function to print classification report with four decimal points\n",
    "def print_classification_report(set_name, labels, predictions):\n",
    "    report = classification_report(labels, predictions, output_dict=True)\n",
    "    print(f\"{set_name} Classification Report:\")\n",
    "    for label, metrics in report.items():\n",
    "        if label == 'accuracy':\n",
    "            print(f\"  Accuracy: {metrics:.4f}\")\n",
    "        else:\n",
    "            print(f\"  Class {label}: Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, F1-Score: {metrics['f1-score']:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Evaluate on the combined training set\n",
    "train_predictions = clf.predict(combined_features)\n",
    "train_accuracy = accuracy_score(combined_labels, train_predictions)\n",
    "print(f\"Combined Training Accuracy: {train_accuracy * 100:.4f}%\")\n",
    "print(f\"Combined Training Confusion Matrix:\\n\", confusion_matrix(combined_labels, train_predictions))\n",
    "print_classification_report(\"Combined Training\", combined_labels, train_predictions)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_predictions = clf.predict(test_stat_features)\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.4f}%\")\n",
    "print(f\"Test Confusion Matrix:\\n\", confusion_matrix(test_labels, test_predictions))\n",
    "print_classification_report(\"Test\", test_labels, test_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rulefit evaluation using 3 most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features Shape: (28537, 3)\n",
      "Validation Features Shape: (6115, 3)\n",
      "Testing Features Shape: (6116, 3)\n",
      "Combined Training + Validation Features Shape: (34652, 3)\n",
      "Combined Training + Validation Labels Shape: (34652,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.781229238533342, tolerance: 2.905117181074409\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.254e+02, tolerance: 4.343e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.6938\n",
      "Confusion Matrix:\n",
      "[[ 812  543  138    5]\n",
      " [ 428  967  165    7]\n",
      " [   0    0 1313  236]\n",
      " [   1   16  334 1151]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6543    0.5421    0.5929      1498\n",
      "           1     0.6337    0.6171    0.6253      1567\n",
      "           2     0.6733    0.8476    0.7505      1549\n",
      "           3     0.8227    0.7663    0.7935      1502\n",
      "\n",
      "    accuracy                         0.6938      6116\n",
      "   macro avg     0.6960    0.6933    0.6906      6116\n",
      "weighted avg     0.6952    0.6938    0.6904      6116\n",
      "\n",
      "Top Rules from RuleFit Model:\n",
      "                                                  rule    type          coef  \\\n",
      "2                                             variance  linear -1.398644e+06   \n",
      "73   mean <= 1.0566121339797974 & mean <= 1.0397928...    rule -2.021831e-01   \n",
      "48   std_dev > 1.0096375346183777 & std_dev <= 1.20...    rule -1.592777e-01   \n",
      "75                        std_dev > 1.3185044527053833    rule  1.219510e-01   \n",
      "922  std_dev > 1.247450053691864 & mean > 0.8011328...    rule -1.161759e-01   \n",
      "\n",
      "      support  importance  \n",
      "2    1.000000    0.299130  \n",
      "73   0.599507    0.099069  \n",
      "48   0.305921    0.073395  \n",
      "75   0.429276    0.060362  \n",
      "922  0.523026    0.058026  \n",
      "Rules have been saved to E:/Abroad period research/new idea implementation codes/Second part of the paper/26 features results/rulefit_rules_on_3_statistical_features.txt.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rulefit import RuleFit\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "input_dir = 'E:/Abroad period research/new idea implementation codes/Second part of the paper/26 features results/3 best features'\n",
    "\n",
    "# Load the CSV files\n",
    "train_df = pd.read_csv(os.path.join(input_dir, \"3_training_selected_features.csv\"))\n",
    "val_df = pd.read_csv(os.path.join(input_dir, \"3_validation_selected_features.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(input_dir, \"3_testing_selected_features.csv\"))\n",
    "\n",
    "\n",
    "# Separate features and labels for each dataset\n",
    "train_features = train_df.drop(columns=['label']).values\n",
    "train_labels = train_df['label'].values\n",
    "print(f\"Training Features Shape: {train_features.shape}\")\n",
    "\n",
    "val_features = val_df.drop(columns=['label']).values\n",
    "val_labels = val_df['label'].values\n",
    "print(f\"Validation Features Shape: {val_features.shape}\")\n",
    "\n",
    "test_features = test_df.drop(columns=['label']).values\n",
    "test_labels = test_df['label'].values\n",
    "print(f\"Testing Features Shape: {test_features.shape}\")\n",
    "\n",
    "# Combine training and validation data for final training\n",
    "train_val_features = np.vstack([train_features, val_features])\n",
    "train_val_labels = np.hstack([train_labels, val_labels])\n",
    "\n",
    "print(f\"Combined Training + Validation Features Shape: {train_val_features.shape}\")\n",
    "print(f\"Combined Training + Validation Labels Shape: {train_val_labels.shape}\")\n",
    "\n",
    "# Define feature names dynamically based on the training data\n",
    "feature_names = train_data.columns[:-1].tolist()  # Exclude the label column\n",
    "\n",
    "# Initialize RuleFit model\n",
    "rf = RuleFit(tree_size=3, sample_fract=0.7, max_rules=3000, random_state=42)\n",
    "\n",
    "# Fit the RuleFit model to combined training and validation data\n",
    "rf.fit(train_val_features, train_val_labels, feature_names=feature_names)\n",
    "\n",
    "# Predict on the test set (continuous values)\n",
    "test_predictions = rf.predict(test_features)\n",
    "\n",
    "# Convert continuous predictions to discrete class labels by rounding\n",
    "test_predictions_discrete = np.round(test_predictions).astype(int)\n",
    "\n",
    "# Ensure the predicted labels are within the valid range of classes\n",
    "test_predictions_discrete = np.clip(test_predictions_discrete, np.min(train_val_labels), np.max(train_val_labels))\n",
    "\n",
    "# Calculate testing accuracy\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions_discrete)\n",
    "print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(test_labels, test_predictions_discrete)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(test_labels, test_predictions_discrete, digits=4)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Extract rules from the RuleFit model\n",
    "rules = rf.get_rules()\n",
    "rules = rules[rules.coef != 0].sort_values(\"importance\", ascending=False)\n",
    "\n",
    "# Display the top rules\n",
    "print(\"Top Rules from RuleFit Model:\")\n",
    "print(rules.head())\n",
    "\n",
    "# Save rules to a text file\n",
    "output_file_path = 'E:/Abroad period research/new idea implementation codes/Second part of the paper/26 features results/rulefit_rules_on_3_statistical_features.txt'\n",
    "rules.to_csv(output_file_path, index=False)\n",
    "print(f\"Rules have been saved to {output_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
