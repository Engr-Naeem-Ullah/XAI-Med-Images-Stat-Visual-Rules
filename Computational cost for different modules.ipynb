{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mobilenet model trainign computational cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import psutil\n",
    "\n",
    "# Track memory usage\n",
    "def memory_usage():\n",
    "    return psutil.Process().memory_info().rss / (1024 * 1024)  # Convert to MB\n",
    "\n",
    "# Track execution time\n",
    "def execution_time(start_time):\n",
    "    return round(time.time() - start_time, 2)\n",
    "\n",
    "# Define image directory and class labels\n",
    "image_dir = r\"E:\\Abroad period research\\Medical images analysis paper implementation codes\\Second part of the paper\\lung_colon_image_set\\dataset\"\n",
    "class_labels = os.listdir(image_dir)  # Auto-detect class folders\n",
    "\n",
    "# Load dataset\n",
    "image_paths, labels = [], []\n",
    "for label in class_labels:\n",
    "    class_path = os.path.join(image_dir, label)\n",
    "    for img in os.listdir(class_path):\n",
    "        image_paths.append(os.path.join(class_path, img))\n",
    "        labels.append(label)\n",
    "\n",
    "# Create DataFrame\n",
    "image_df = pd.DataFrame({\"image_path\": image_paths, \"label\": labels})\n",
    "\n",
    "# Ensure dataset is not empty\n",
    "if image_df.empty:\n",
    "    raise ValueError(\"Dataset is empty. Check image directory path!\")\n",
    "\n",
    "\n",
    "# Split dataset \n",
    "train_df, test_df = train_test_split(image_df, test_size=0.2, stratify=image_df['label'], random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, stratify=train_df['label'], random_state=42)\n",
    "\n",
    "# Image Data Generators\n",
    "data_gen = ImageDataGenerator(rescale=1./255)\n",
    "train_gen = data_gen.flow_from_dataframe(train_df, x_col='image_path', y_col='label', target_size=(224, 224), class_mode='categorical', batch_size=32)\n",
    "val_gen = data_gen.flow_from_dataframe(val_df, x_col='image_path', y_col='label', target_size=(224, 224), class_mode='categorical', batch_size=32)\n",
    "test_gen = data_gen.flow_from_dataframe(test_df, x_col='image_path', y_col='label', target_size=(224, 224), class_mode='categorical', batch_size=32, shuffle=False)\n",
    "\n",
    "# Load MobileNetV2 base model\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "base_model.trainable = False  # Freeze layers\n",
    "\n",
    "# Add custom layers\n",
    "x = GlobalAveragePooling2D()(base_model.output)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "out = Dense(len(class_labels), activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=out)\n",
    "\n",
    "# Display Model Parameters & FLOPs\n",
    "trainable_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "total_params = np.sum([np.prod(v.shape) for v in model.weights])\n",
    "flops = 300 * 10**6  # Approx FLOPs for MobileNetV2\n",
    "\n",
    "print(f\"Total Parameters: {total_params:,}\")\n",
    "print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "print(f\"MobileNetV2 FLOPs (Approx): {flops:,}\")\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model & track execution time\n",
    "start_time = time.time()\n",
    "train_start_memory = memory_usage()\n",
    "\n",
    "history = model.fit(train_gen, validation_data=val_gen, epochs=50)\n",
    "\n",
    "train_end_memory = memory_usage()\n",
    "train_time = execution_time(start_time)\n",
    "\n",
    "# Evaluate model\n",
    "y_true = test_df['label'].map(lambda x: class_labels.index(x)).values\n",
    "y_pred_probs = model.predict(test_gen)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=class_labels))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='d', xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Save model\n",
    "model.save(\"mobilenetv2_trained_model.h5\")\n",
    "\n",
    "# Display Computational Cost Summary\n",
    "print(\"\\n===== COMPUTATIONAL COST SUMMARY =====\")\n",
    "print(f\"Model Training Time: {train_time} seconds\")\n",
    "print(f\"Memory Used During Training: {train_end_memory - train_start_memory:.2f} MB\")\n",
    "print(f\"Total Model Parameters: {total_params:,}\")\n",
    "print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "print(f\"MobileNetV2 FLOPs (Approx): {flops:,}\")\n",
    "\n",
    "# Save computational cost to a file for paper\n",
    "with open(\"computational_cost_summary.txt\", \"w\") as f:\n",
    "    f.write(f\"Model Training Time: {train_time} seconds\\n\")\n",
    "    f.write(f\"Memory Used During Training: {train_end_memory - train_start_memory:.2f} MB\\n\")\n",
    "    f.write(f\"Total Model Parameters: {total_params:,}\\n\")\n",
    "    f.write(f\"Trainable Parameters: {trainable_params:,}\\n\")\n",
    "    f.write(f\"MobileNetV2 FLOPs (Approx): {flops:,}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time  # Track execution time\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Markdown\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, auc\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Start measuring total execution time\n",
    "start_time = time.time()\n",
    "\n",
    "# Function to print with Markdown (optional)\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "# -----------------------------------\n",
    "# Load and preprocess dataset\n",
    "# -----------------------------------\n",
    "image_dir = Path(r'E:\\Abroad period research\\Medical images analysis paper implementation codes\\Second part of the paper\\Results on just chest region segmented dataset\\justchest_Unet_Segmented_Dataset')\n",
    "\n",
    "# Make sure the path exists\n",
    "if not image_dir.exists():\n",
    "    raise FileNotFoundError(f\"The directory {image_dir} does not exist!\")\n",
    "\n",
    "# Load image file paths\n",
    "filepaths = list(image_dir.glob('**/*.png'))\n",
    "\n",
    "# Check if files are found\n",
    "if len(filepaths) == 0:\n",
    "    raise ValueError(f\"No .png files found in {image_dir}. Check the directory and files!\")\n",
    "\n",
    "print(f\"Total images found: {len(filepaths)}\")\n",
    "\n",
    "# Extract labels from directory structure\n",
    "labels = [path.parent.name for path in filepaths]\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "filepaths = pd.Series(filepaths, name='Filepath').astype(str)\n",
    "labels = pd.Series(labels, name='Label')\n",
    "\n",
    "# Combine into a dataframe\n",
    "image_df = pd.concat([filepaths, labels], axis=1)\n",
    "image_df = image_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Dataframe head:\\n{image_df.head()}\")\n",
    "\n",
    "# -----------------------------------\n",
    "# Split data into training, validation, and test sets\n",
    "# -----------------------------------\n",
    "train_df, temp_df = train_test_split(image_df, train_size=0.7, shuffle=True, random_state=1)\n",
    "val_df, test_df = train_test_split(temp_df, train_size=0.5, shuffle=True, random_state=1)\n",
    "\n",
    "print(f\"Train samples: {len(train_df)}, Val samples: {len(val_df)}, Test samples: {len(test_df)}\")\n",
    "\n",
    "# -----------------------------------\n",
    "# Create data generators with augmentation\n",
    "# -----------------------------------\n",
    "def create_gen():\n",
    "    train_generator = ImageDataGenerator(\n",
    "        preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    val_test_generator = ImageDataGenerator(\n",
    "        preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "    )\n",
    "\n",
    "    train_images = train_generator.flow_from_dataframe(\n",
    "        dataframe=train_df,\n",
    "        x_col='Filepath',\n",
    "        y_col='Label',\n",
    "        target_size=(224, 224),\n",
    "        color_mode='rgb',\n",
    "        class_mode='categorical',\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        seed=0\n",
    "    )\n",
    "\n",
    "    val_images = val_test_generator.flow_from_dataframe(\n",
    "        dataframe=val_df,\n",
    "        x_col='Filepath',\n",
    "        y_col='Label',\n",
    "        target_size=(224, 224),\n",
    "        color_mode='rgb',\n",
    "        class_mode='categorical',\n",
    "        batch_size=32,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    test_images = val_test_generator.flow_from_dataframe(\n",
    "        dataframe=test_df,\n",
    "        x_col='Filepath',\n",
    "        y_col='Label',\n",
    "        target_size=(224, 224),\n",
    "        color_mode='rgb',\n",
    "        class_mode='categorical',\n",
    "        batch_size=32,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    return train_images, val_images, test_images\n",
    "\n",
    "# -----------------------------------\n",
    "# Load the pre-trained model\n",
    "# -----------------------------------\n",
    "model_path = r'E:\\Abroad period research\\Medical images analysis paper implementation codes\\Second part of the paper\\Results on just chest region segmented dataset\\Mobilenetv2_finetuned_with_CLR_and_GradientAccum.h5'\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(f\"The model file was not found at {model_path}\")\n",
    "\n",
    "loaded_model = load_model(model_path)\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "# -----------------------------------\n",
    "# Define the feature extraction model\n",
    "# -----------------------------------\n",
    "feature_extractor = Model(inputs=loaded_model.input, outputs=loaded_model.layers[-4].output)\n",
    "print(\"Feature extractor model created.\")\n",
    "\n",
    "# -----------------------------------\n",
    "# Directory to save extracted features\n",
    "# -----------------------------------\n",
    "feature_dir = r'E:\\Abroad period research\\Medical images analysis paper implementation codes\\Second part of the paper\\Results on just chest region segmented dataset\\extracted_features_just_for_time_Calculation'\n",
    "os.makedirs(feature_dir, exist_ok=True)\n",
    "\n",
    "# -----------------------------------\n",
    "# Feature extraction function\n",
    "# -----------------------------------\n",
    "def extract_features(data_gen, set_name):\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    steps = data_gen.samples // data_gen.batch_size + 1\n",
    "    print(f\"Extracting features for {set_name} set...\")\n",
    "\n",
    "    for i in range(steps):\n",
    "        batch_images, batch_labels = next(data_gen)\n",
    "        batch_features = feature_extractor.predict(batch_images, verbose=0)\n",
    "        features.extend(batch_features)\n",
    "        labels.extend(batch_labels)\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    features = np.array(features)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Save features and labels\n",
    "    feature_file = os.path.join(feature_dir, f\"{set_name}_features.pkl\")\n",
    "    with open(feature_file, 'wb') as f:\n",
    "        pickle.dump((features, labels), f)\n",
    "\n",
    "    print(f\"{set_name.capitalize()} features saved to {feature_file}\")\n",
    "\n",
    "# -----------------------------------\n",
    "# Generate data\n",
    "# -----------------------------------\n",
    "train_images, val_images, test_images = create_gen()\n",
    "\n",
    "# -----------------------------------\n",
    "# Extract features for training, validation, and test sets\n",
    "# -----------------------------------\n",
    "extract_features(train_images, \"train\")\n",
    "extract_features(val_images, \"val\")\n",
    "extract_features(test_images, \"test\")\n",
    "\n",
    "# -----------------------------------\n",
    "# Total execution time\n",
    "# -----------------------------------\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"\\n Total Execution Time: {elapsed_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical Feature Caculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from scipy.stats import skew, entropy, kurtosis, variation, iqr\n",
    "import os\n",
    "import time  # Added for timing\n",
    "\n",
    "# Load extracted features\n",
    "feature_dir = r'E:\\Abroad period research\\Medical images analysis paper implementation codes\\Second part of the paper\\Results on just chest region segmented dataset\\extracted_features_just_for_time_Calculation'\n",
    "stat_feature_dir = r'E:\\Abroad period research\\Medical images analysis paper implementation codes\\Second part of the paper\\Results on just chest region segmented dataset\\statistical_features_just_for_time_Calculation'\n",
    "os.makedirs(stat_feature_dir, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(feature_dir, \"train_features.pkl\"), 'rb') as f:\n",
    "    train_features, train_labels = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(feature_dir, \"val_features.pkl\"), 'rb') as f:\n",
    "    val_features, val_labels = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(feature_dir, \"test_features.pkl\"), 'rb') as f:\n",
    "    test_features, test_labels = pickle.load(f)\n",
    "\n",
    "# Convert one-hot encoded labels to single class labels, if needed\n",
    "if len(train_labels.shape) > 1 and train_labels.shape[1] > 1:\n",
    "    train_labels = np.argmax(train_labels, axis=1)\n",
    "\n",
    "if len(val_labels.shape) > 1 and val_labels.shape[1] > 1:\n",
    "    val_labels = np.argmax(val_labels, axis=1)\n",
    "\n",
    "if len(test_labels.shape) > 1 and test_labels.shape[1] > 1:\n",
    "    test_labels = np.argmax(test_labels, axis=1)\n",
    "\n",
    "# Function to calculate signal-to-noise ratio\n",
    "def signal_to_noise(f):\n",
    "    mean = np.mean(f)\n",
    "    std = np.std(f)\n",
    "    return mean / (std + 1e-6)  # Adding small constant to avoid division by zero\n",
    "\n",
    "# Function to calculate more advanced statistical features from deep features\n",
    "def calculate_statistical_features(features):\n",
    "    stats_features = []\n",
    "    for f in features:\n",
    "        stats = {\n",
    "            'mean': np.mean(f),\n",
    "            'std_dev': np.std(f),\n",
    "            'variance': np.var(f),\n",
    "            'median': np.median(f),\n",
    "            'range': np.ptp(f),  # Peak-to-peak range\n",
    "            'skewness': skew(f),\n",
    "            'kurtosis': kurtosis(f),\n",
    "            'entropy': entropy(np.abs(f) + 1e-6),  # Add small constant to avoid log(0)\n",
    "            'energy': np.sum(f ** 2),  # Sum of squared elements\n",
    "            'contrast': np.std(f) ** 2,  # Contrast as variance\n",
    "            'mean_abs_dev': np.mean(np.abs(f - np.mean(f))),\n",
    "            'min_value': np.min(f),\n",
    "            'max_value': np.max(f),\n",
    "            'iqr': iqr(f),  # Interquartile range\n",
    "            'percentile_25': np.percentile(f, 25),\n",
    "            'percentile_50': np.percentile(f, 50),  # Median\n",
    "            'percentile_75': np.percentile(f, 75),\n",
    "            'signal_to_noise': signal_to_noise(f),\n",
    "            'coef_of_var': variation(f),  # Coefficient of variation\n",
    "            'autocorrelation': np.corrcoef(f[:-1], f[1:])[0, 1] if len(f) > 1 else 0,  # Lag-1 autocorrelation\n",
    "            'shannon_entropy': -np.sum(f * np.log2(f + 1e-6)),  # Shannon entropy for diversity measure\n",
    "            'root_mean_square': np.sqrt(np.mean(f ** 2)),  # Root mean square\n",
    "            'harmonic_mean': len(f) / np.sum(1.0 / (f + 1e-6)),  # Harmonic mean\n",
    "            'geometric_mean': np.exp(np.mean(np.log(f + 1e-6))),  # Geometric mean\n",
    "            'std_error_mean': np.std(f) / np.sqrt(len(f)),  # Standard error of the mean\n",
    "            'median_abs_dev': np.median(np.abs(f - np.median(f))),  # Median absolute deviation\n",
    "        }\n",
    "        stats_features.append(stats)\n",
    "    return stats_features\n",
    "\n",
    "# Function to save features and labels as CSV\n",
    "def save_statistical_features_as_csv(features, labels, set_name):\n",
    "    df = pd.DataFrame(features)\n",
    "    df['label'] = labels  # Ensure labels are a 1D array\n",
    "    df.to_csv(os.path.join(stat_feature_dir, f\"{set_name}_stat_features.csv\"), index=False)\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Calculate and save statistical features for each dataset\n",
    "for set_name, features, labels in [(\"train\", train_features, train_labels), \n",
    "                                   (\"val\", val_features, val_labels), \n",
    "                                   (\"test\", test_features, test_labels)]:\n",
    "    print(f\"Processing {set_name} features...\")\n",
    "    stats_features = calculate_statistical_features(features)\n",
    "    save_statistical_features_as_csv(stats_features, labels, set_name)\n",
    "    print(f\"{set_name} features done.\")\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate total elapsed time\n",
    "total_time_seconds = end_time - start_time\n",
    "total_time_minutes = total_time_seconds / 60\n",
    "\n",
    "print(\"\\nStatistical features calculated and saved in CSV format.\")\n",
    "print(f\"\\nTotal time taken: {total_time_seconds:.2f} seconds ({total_time_minutes:.2f} minutes)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ZFMIS Feature Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import time  # Import the time module to measure execution time\n",
    "\n",
    "# Start measuring execution time\n",
    "start_time = time.time()\n",
    "\n",
    "# Define directories\n",
    "input_dir = r'E:\\Abroad period research\\Medical images analysis paper implementation codes\\Second part of the paper\\Results on just chest region segmented dataset\\statistical_features_just_for_time_Calculation'\n",
    "output_dir = r'E:\\Abroad period research\\Medical images analysis paper implementation codes\\Second part of the paper\\Results on just chest region segmented dataset\\filtered_statistical_features_just_for_time_Calculation'\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load datasets\n",
    "train_data = pd.read_csv(os.path.join(input_dir, \"train_stat_features.csv\"))\n",
    "val_data = pd.read_csv(os.path.join(input_dir, \"val_stat_features.csv\"))\n",
    "test_data = pd.read_csv(os.path.join(input_dir, \"test_stat_features.csv\"))\n",
    "\n",
    "# Function to find features with more than 50% zeros, excluding the label column\n",
    "def find_zero_features(df):\n",
    "    zero_features = [col for col in df.columns if col != 'label' and (df[col] == 0).mean() > 0.5]\n",
    "    return zero_features\n",
    "\n",
    "# Identify features with more than 50% zeros across all datasets\n",
    "train_zero_features = find_zero_features(train_data)\n",
    "val_zero_features = find_zero_features(val_data)\n",
    "test_zero_features = find_zero_features(test_data)\n",
    "\n",
    "# Union of features with more than 50% zeros across all datasets\n",
    "all_zero_features = set(train_zero_features).union(set(val_zero_features)).union(set(test_zero_features))\n",
    "\n",
    "# Display features with more than 50% zeros in any dataset\n",
    "print(f\"Features with more than 50% zeros in any dataset: {list(all_zero_features)}\")\n",
    "\n",
    "# Drop these features from all datasets, keeping the label column\n",
    "train_filtered = train_data.drop(columns=all_zero_features)\n",
    "val_filtered = val_data.drop(columns=all_zero_features)\n",
    "test_filtered = test_data.drop(columns=all_zero_features)\n",
    "\n",
    "# Save the filtered datasets\n",
    "train_filtered.to_csv(os.path.join(output_dir, \"filtered_train_stat_features.csv\"), index=False)\n",
    "val_filtered.to_csv(os.path.join(output_dir, \"filtered_val_stat_features.csv\"), index=False)\n",
    "test_filtered.to_csv(os.path.join(output_dir, \"filtered_test_stat_features.csv\"), index=False)\n",
    "\n",
    "print(\"Filtered datasets with selected features and labels have been saved in the 'filtered_statistical_features' directory.\")\n",
    "\n",
    "# End measuring execution time\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "\n",
    "# Display the total execution time in seconds and minutes\n",
    "print(f\"\\nTotal Execution Time: {total_time:.2f} seconds\")\n",
    "print(f\"Total Execution Time: {total_time / 60:.2f} minutes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree Training Time on All Statistical Feature Set  aand \n",
    "Decision Tree Testing Time All Statistical Feature Set \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time  # Import time module for measuring execution time\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "stat_feature_dir = r'E:\\Abroad period research\\Medical images analysis paper implementation codes\\Second part of the paper\\Results on just chest region segmented dataset\\statistical_features_just_for_time_Calculation'\n",
    "\n",
    "# Load the CSV files\n",
    "train_df = pd.read_csv(os.path.join(stat_feature_dir, \"train_stat_features.csv\"))\n",
    "val_df = pd.read_csv(os.path.join(stat_feature_dir, \"val_stat_features.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(stat_feature_dir, \"test_stat_features.csv\"))\n",
    "\n",
    "# Separate features and labels\n",
    "train_stat_features = train_df.drop(columns=['label']).values\n",
    "train_labels = train_df['label'].values\n",
    "\n",
    "val_stat_features = val_df.drop(columns=['label']).values\n",
    "val_labels = val_df['label'].values\n",
    "\n",
    "test_stat_features = test_df.drop(columns=['label']).values\n",
    "test_labels = test_df['label'].values\n",
    "\n",
    "# Combine training and validation data for final training\n",
    "combined_features = np.vstack([train_stat_features, val_stat_features])\n",
    "combined_labels = np.hstack([train_labels, val_labels])\n",
    "\n",
    "# Initialize the Decision Tree Classifier with specified hyperparameters\n",
    "clf = DecisionTreeClassifier(\n",
    "    criterion='entropy',      # Use entropy for splitting nodes\n",
    "    max_depth=5,              # Limit the tree depth to 5 levels\n",
    "    min_samples_leaf=4,       # Minimum samples required for a leaf node\n",
    "    min_samples_split=2,      # Minimum samples required to split an internal node\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "# ====================== TRAINING PHASE ======================\n",
    "print(\"Training the classifier...\")\n",
    "\n",
    "# Start training timer\n",
    "train_start_time = time.time()\n",
    "\n",
    "# Train the classifier\n",
    "clf.fit(combined_features, combined_labels)\n",
    "\n",
    "# End training timer\n",
    "train_end_time = time.time()\n",
    "\n",
    "# Calculate total training time\n",
    "total_train_time = train_end_time - train_start_time\n",
    "\n",
    "print(f\"\\nTotal Training Time: {total_train_time:.4f} seconds ({total_train_time / 60:.4f} minutes)\")\n",
    "\n",
    "# ====================== EVALUATE ON COMBINED TRAINING SET ======================\n",
    "train_predictions = clf.predict(combined_features)\n",
    "train_accuracy = accuracy_score(combined_labels, train_predictions)\n",
    "\n",
    "print(f\"\\nCombined Training Accuracy: {train_accuracy * 100:.4f}%\")\n",
    "print(f\"Combined Training Confusion Matrix:\\n\", confusion_matrix(combined_labels, train_predictions))\n",
    "\n",
    "# Function to print classification report with four decimal points\n",
    "def print_classification_report(set_name, labels, predictions):\n",
    "    report = classification_report(labels, predictions, output_dict=True)\n",
    "    print(f\"{set_name} Classification Report:\")\n",
    "    for label, metrics in report.items():\n",
    "        if label == 'accuracy':\n",
    "            print(f\"  Accuracy: {metrics:.4f}\")\n",
    "        else:\n",
    "            print(f\"  Class {label}: Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, F1-Score: {metrics['f1-score']:.4f}\")\n",
    "    print()\n",
    "\n",
    "print_classification_report(\"Combined Training\", combined_labels, train_predictions)\n",
    "\n",
    "# ====================== TESTING PHASE ======================\n",
    "print(\"Testing the classifier...\")\n",
    "\n",
    "# Start testing timer\n",
    "test_start_time = time.time()\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_predictions = clf.predict(test_stat_features)\n",
    "\n",
    "# End testing timer\n",
    "test_end_time = time.time()\n",
    "\n",
    "# Calculate total testing time\n",
    "total_test_time = test_end_time - test_start_time\n",
    "\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "\n",
    "print(f\"\\nTest Accuracy: {test_accuracy * 100:.4f}%\")\n",
    "print(f\"Test Confusion Matrix:\\n\", confusion_matrix(test_labels, test_predictions))\n",
    "\n",
    "print_classification_report(\"Test\", test_labels, test_predictions)\n",
    "\n",
    "print(f\"\\nTotal Testing Time: {total_test_time:.4f} seconds ({total_test_time / 60:.4f} minutes)\")\n",
    "\n",
    "# ====================== SUMMARY ======================\n",
    "print(\"\\n========== SUMMARY ==========\")\n",
    "print(f\"Training Time   : {total_train_time:.4f} seconds ({total_train_time / 60:.4f} minutes)\")\n",
    "print(f\"Testing Time    : {total_test_time:.4f} seconds ({total_test_time / 60:.4f} minutes)\")\n",
    "print(\"================================\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RuleFit Training Time All Statistical Feature Set  and\n",
    "RuleFit Testing Time All Statistical Feature Set  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from rulefit import RuleFit\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# ==========================\n",
    "# Load Dataset CSVs\n",
    "# ==========================\n",
    "input_dir = r'E:\\Abroad period research\\Medical images analysis paper implementation codes\\Second part of the paper\\Results on just chest region segmented dataset\\statistical_features_just_for_time_Calculation'\n",
    "\n",
    "train_data = pd.read_csv(os.path.join(input_dir, \"train_stat_features.csv\"))\n",
    "val_data = pd.read_csv(os.path.join(input_dir, \"val_stat_features.csv\"))\n",
    "test_data = pd.read_csv(os.path.join(input_dir, \"test_stat_features.csv\"))\n",
    "\n",
    "# ==========================\n",
    "# Separate Features and Labels\n",
    "# ==========================\n",
    "train_features = train_data.drop(columns=['label']).values\n",
    "train_labels = train_data['label'].values\n",
    "\n",
    "val_features = val_data.drop(columns=['label']).values\n",
    "val_labels = val_data['label'].values\n",
    "\n",
    "test_features = test_data.drop(columns=['label']).values\n",
    "test_labels = test_data['label'].values\n",
    "\n",
    "# ==========================\n",
    "# Combine Training + Validation Data\n",
    "# ==========================\n",
    "train_val_features = np.vstack([train_features, val_features])\n",
    "train_val_labels = np.hstack([train_labels, val_labels])\n",
    "\n",
    "print(f\"\\nCombined Training + Validation Features Shape: {train_val_features.shape}\")\n",
    "print(f\"Combined Training + Validation Labels Shape: {train_val_labels.shape}\")\n",
    "\n",
    "# ==========================\n",
    "# Define Feature Names Dynamically\n",
    "# ==========================\n",
    "feature_names = train_data.columns[:-1].tolist()  # Exclude 'label'\n",
    "\n",
    "# ==========================\n",
    "# Initialize RuleFit Model\n",
    "# ==========================\n",
    "rf = RuleFit(tree_size=3, sample_fract=0.7, max_rules=2000, random_state=42)\n",
    "\n",
    "# ==========================\n",
    "# Train the RuleFit Model & Measure Training Time\n",
    "# ==========================\n",
    "train_start_time = time.perf_counter()\n",
    "\n",
    "rf.fit(train_val_features, train_val_labels, feature_names=feature_names)\n",
    "\n",
    "train_end_time = time.perf_counter()\n",
    "total_train_time = train_end_time - train_start_time\n",
    "\n",
    "# ==========================\n",
    "# Test the RuleFit Model & Measure Testing Time\n",
    "# ==========================\n",
    "test_start_time = time.perf_counter()\n",
    "\n",
    "# Predict on the test set (continuous values)\n",
    "test_predictions = rf.predict(test_features)\n",
    "\n",
    "test_end_time = time.perf_counter()\n",
    "total_test_time = test_end_time - test_start_time\n",
    "\n",
    "# ==========================\n",
    "# Convert Predictions to Discrete Class Labels\n",
    "# ==========================\n",
    "test_predictions_discrete = np.round(test_predictions).astype(int)\n",
    "\n",
    "# Ensure predicted labels are within the valid range of classes\n",
    "test_predictions_discrete = np.clip(test_predictions_discrete, np.min(train_val_labels), np.max(train_val_labels))\n",
    "\n",
    "# ==========================\n",
    "# Calculate Accuracy and Metrics\n",
    "# ==========================\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions_discrete)\n",
    "print(f\"\\nTesting Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "conf_matrix = confusion_matrix(test_labels, test_predictions_discrete)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "report = classification_report(test_labels, test_predictions_discrete, digits=4)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)\n",
    "\n",
    "# ==========================\n",
    "# Extract and Save Rules\n",
    "# ==========================\n",
    "rules = rf.get_rules()\n",
    "rules = rules[rules.coef != 0].sort_values(\"importance\", ascending=False)\n",
    "\n",
    "print(\"\\nTop Rules from RuleFit Model:\")\n",
    "print(rules.head())\n",
    "\n",
    "output_file_path = r'E:\\Abroad period research\\Medical images analysis paper implementation codes\\Second part of the paper\\lung_colon_image_set\\26 features results\\rulefit_rules_on_statistical_features_just_for_Time_Calculation.txt'\n",
    "rules.to_csv(output_file_path, index=False)\n",
    "print(f\"\\nRules have been saved to: {output_file_path}\")\n",
    "\n",
    "# ==========================\n",
    "# Display Training and Testing Time\n",
    "# ==========================\n",
    "print(\"\\n============================================\")\n",
    "print(f\"Total Training Time: {total_train_time:.8f} seconds ({total_train_time / 60:.8f} minutes)\")\n",
    "print(f\"Total Testing Time : {total_test_time:.8f} seconds ({total_test_time / 60:.8f} minutes)\")\n",
    "print(\"============================================\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SFMOV Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
    "from scipy.stats import skew\n",
    "from skimage.measure import shannon_entropy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# --------------------- Step 1: Load the MobileNetV2 Model ---------------------\n",
    "model_path = r\"E:\\Abroad period research\\Medical images analysis paper implementation codes\\Second part of the paper\\Ultrasound Breast Images for Breast Cancer\\Mobilenetv2_finetuned_on_breastcancer_dataset.h5\"\n",
    "\n",
    "print(\"Loading model...\")\n",
    "loaded_model = load_model(model_path)\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "# Define convolutional and dense layers for feature extraction\n",
    "last_conv_layer_name = 'Conv_1_bn'\n",
    "dense_layer_name = 'dense_1'\n",
    "\n",
    "conv_layer_model = Model(inputs=loaded_model.input, outputs=loaded_model.get_layer(last_conv_layer_name).output)\n",
    "dense_layer_model = Model(inputs=loaded_model.input, outputs=loaded_model.get_layer(dense_layer_name).output)\n",
    "\n",
    "# --------------------- Step 2: Define dataset and output directories ---------------------\n",
    "image_dir = r\"E:\\Abroad period research\\Medical images analysis paper implementation codes\\Second part of the paper\\Results on just chest region segmented dataset\\justchest_Unet_Segmented_Dataset\"\n",
    "output_dir = r\"E:\\Abroad period research\\Medical images analysis paper implementation codes\\Second part of the paper\\Results on just chest region segmented dataset\\eye Confusion Matrix visualization just for TC\"\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# --------------------- Step 3: Load and preprocess dataset ---------------------\n",
    "\n",
    "# Find all image paths\n",
    "print(\"Loading images from directory:\", image_dir)\n",
    "filepaths = list(Path(image_dir).glob('**/*.png'))\n",
    "\n",
    "# Check if images are found\n",
    "if not filepaths:\n",
    "    print(f\"No images found in {image_dir}. Please check the folder path.\")\n",
    "    exit()\n",
    "\n",
    "# Extract labels from folder names (parent folder of each file)\n",
    "labels = [Path(fp).parent.name for fp in filepaths]\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "filepaths = pd.Series([str(fp) for fp in filepaths], name='Filepath')\n",
    "labels = pd.Series(labels, name='Label')\n",
    "\n",
    "# Combine into a DataFrame\n",
    "image_df = pd.concat([filepaths, labels], axis=1)\n",
    "\n",
    "# Print dataset info\n",
    "print(f\"Total images found: {len(image_df)}\")\n",
    "print(image_df['Label'].value_counts())\n",
    "\n",
    "# Shuffle dataset\n",
    "image_df = image_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Check minimum sample size for splitting\n",
    "if len(image_df) < 3:\n",
    "    print(\"Not enough data to split. You need at least 3 images.\")\n",
    "    exit()\n",
    "\n",
    "# --------------------- Step 4: Split the dataset ---------------------\n",
    "train_df, temp_df = train_test_split(image_df, train_size=0.7, shuffle=True, random_state=1)\n",
    "val_df, test_df = train_test_split(temp_df, train_size=0.5, shuffle=True, random_state=1)\n",
    "\n",
    "print(f\"Train set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")\n",
    "\n",
    "# --------------------- Step 5: Create test data generator ---------------------\n",
    "def create_test_data_generator(image_df, batch_size=32):\n",
    "    datagen = ImageDataGenerator(rescale=1.0 / 255.0)\n",
    "    generator = datagen.flow_from_dataframe(\n",
    "        dataframe=image_df,\n",
    "        x_col='Filepath',\n",
    "        y_col=None,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False\n",
    "    )\n",
    "    return generator\n",
    "\n",
    "test_generator = create_test_data_generator(test_df, batch_size=32)\n",
    "\n",
    "# --------------------- Step 6: Make predictions on the test set ---------------------\n",
    "predictions = loaded_model.predict(test_generator, steps=len(test_generator), verbose=1)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# --------------------- Step 7: Create and save confusion matrix ---------------------\n",
    "# Folder mapping for class indices\n",
    "class_folders = {\n",
    "    0: \"COVID\",\n",
    "    1: \"Lung_Opacity\",\n",
    "    2: \"Normal\",\n",
    "    3: \"Viral Pneumonia\"\n",
    "}\n",
    "\n",
    "# Map true labels to class indices\n",
    "true_classes = test_df['Label'].map(lambda x: list(class_folders.values()).index(x)).values\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(true_classes, predicted_classes)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "\n",
    "tick_marks = np.arange(len(class_folders))\n",
    "plt.xticks(tick_marks, class_folders.values(), rotation=45)\n",
    "plt.yticks(tick_marks, class_folders.values())\n",
    "\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "thresh = cm_normalized.max() / 2.\n",
    "\n",
    "for i, j in np.ndindex(cm_normalized.shape):\n",
    "    plt.text(j, i, f\"{cm_normalized[i, j]:.2f}\",\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if cm_normalized[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the confusion matrix plot\n",
    "plt.savefig(os.path.join(output_dir, 'confusion_matrix.png'))\n",
    "plt.close()\n",
    "\n",
    "print(\"Confusion matrix saved to:\", output_dir)\n",
    "\n",
    "# --------------------- Step 8: Process and visualize a single image with timing ---------------------\n",
    "def process_image_with_timing(image_path, output_name):\n",
    "    start_time = time.time()\n",
    "\n",
    "    input_size = (224, 224)\n",
    "    original_image = load_img(image_path, target_size=input_size)\n",
    "    image_array = img_to_array(original_image) / 255.0\n",
    "    image_array = np.expand_dims(image_array, axis=0)\n",
    "\n",
    "    # Extract features\n",
    "    conv_features = np.squeeze(conv_layer_model.predict(image_array))\n",
    "    dense_features = dense_layer_model.predict(image_array)\n",
    "\n",
    "    dense_mean = np.mean(dense_features)\n",
    "    dense_skewness = skew(dense_features.ravel())\n",
    "    dense_entropy = shannon_entropy(dense_features)\n",
    "\n",
    "    # Statistical maps\n",
    "    mean_map = np.zeros(conv_features.shape[:2])\n",
    "    skewness_map = np.zeros(conv_features.shape[:2])\n",
    "    entropy_map = np.zeros(conv_features.shape[:2])\n",
    "\n",
    "    for i in range(conv_features.shape[-1]):\n",
    "        feature_map = conv_features[:, :, i]\n",
    "        mean_map += feature_map / conv_features.shape[-1]\n",
    "        skewness_map += skew(feature_map.ravel()) * feature_map\n",
    "        entropy_map += shannon_entropy(feature_map) * feature_map\n",
    "\n",
    "    # Normalize maps\n",
    "    mean_map = (mean_map - mean_map.min()) / (mean_map.max() - mean_map.min() + 1e-8)\n",
    "    skewness_map = (skewness_map - skewness_map.min()) / (skewness_map.max() - skewness_map.min() + 1e-8)\n",
    "    entropy_map = (entropy_map - entropy_map.min()) / (entropy_map.max() - entropy_map.min() + 1e-8)\n",
    "\n",
    "    # Combine maps\n",
    "    combined_map = (dense_mean * mean_map + dense_skewness * skewness_map + dense_entropy * entropy_map)\n",
    "    combined_map /= (dense_mean + dense_skewness + dense_entropy)\n",
    "    combined_map = (combined_map - combined_map.min()) / (combined_map.max() - combined_map.min() + 1e-8)\n",
    "\n",
    "    # Resize maps for plotting\n",
    "    mean_map_resized = cv2.resize(mean_map, input_size)\n",
    "    skewness_map_resized = cv2.resize(skewness_map, input_size)\n",
    "    entropy_map_resized = cv2.resize(entropy_map, input_size)\n",
    "    combined_map_resized = cv2.resize(combined_map, input_size)\n",
    "\n",
    "    # Plot and save\n",
    "    fig, ax = plt.subplots(1, 5, figsize=(22, 6))\n",
    "\n",
    "    ax[0].imshow(original_image)\n",
    "    ax[0].set_title('Original Image')\n",
    "\n",
    "    ax[1].imshow(original_image)\n",
    "    ax[1].imshow(mean_map_resized, cmap='jet', alpha=0.5)\n",
    "    ax[1].set_title('Mean Heatmap')\n",
    "\n",
    "    ax[2].imshow(original_image)\n",
    "    ax[2].imshow(skewness_map_resized, cmap='jet', alpha=0.5)\n",
    "    ax[2].set_title('Skewness Heatmap')\n",
    "\n",
    "    ax[3].imshow(original_image)\n",
    "    ax[3].imshow(entropy_map_resized, cmap='jet', alpha=0.5)\n",
    "    ax[3].set_title('Entropy Heatmap')\n",
    "\n",
    "    ax[4].imshow(original_image)\n",
    "    ax[4].imshow(combined_map_resized, cmap='jet', alpha=0.5)\n",
    "    ax[4].set_title('Combined Heatmap')\n",
    "\n",
    "    for a in ax:\n",
    "        a.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, output_name))\n",
    "    plt.close()\n",
    "\n",
    "    end_time = time.time()\n",
    "    time_taken = end_time - start_time\n",
    "    print(f\"Time taken to process and visualize the image '{output_name}': {time_taken:.2f} seconds\")\n",
    "\n",
    "# --------------------- Step 9: Run visualization on one image ---------------------\n",
    "if len(test_df) > 0:\n",
    "    sample_image_path = test_df.iloc[0]['Filepath']\n",
    "    print(f\"Processing image: {sample_image_path}\")\n",
    "    process_image_with_timing(sample_image_path, 'sample_image_visualization.png')\n",
    "else:\n",
    "    print(\"No images found in the test set to process.\")\n",
    "\n",
    "print(\"Visualization and processing completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning model Grid search optimization code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from pathlib import Path\n",
    "# import os\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers, Model\n",
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# import time  # <-- Added for timing the process\n",
    "\n",
    "# # Load and preprocess dataset\n",
    "# image_dir = Path(r'E:\\Abroad period research\\Medical images analysis paper implementation codes\\Second part of the paper\\testing on Generated Eye Dataset for Glaucoma Detection\\Acrima')\n",
    "# filepaths = list(image_dir.glob(r'**/*.png'))\n",
    "# labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], filepaths))\n",
    "\n",
    "# # Convert file paths and labels to a DataFrame\n",
    "# filepaths = pd.Series(filepaths, name='Filepath').astype(str)\n",
    "# labels = pd.Series(labels, name='Label')\n",
    "# image_df = pd.concat([filepaths, labels], axis=1).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# # Split data into train, validation, and test sets\n",
    "# train_df, temp_df = train_test_split(image_df, train_size=0.7, shuffle=True, random_state=1)\n",
    "# val_df, test_df = train_test_split(temp_df, train_size=0.5, shuffle=True, random_state=1)\n",
    "\n",
    "# # Create data generators\n",
    "# def create_data_generators(train_df, val_df, test_df, batch_size):\n",
    "#     train_gen = ImageDataGenerator(\n",
    "#         preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input,\n",
    "#         rotation_range=20, width_shift_range=0.2, height_shift_range=0.2,\n",
    "#         shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest'\n",
    "#     )\n",
    "#     val_test_gen = ImageDataGenerator(\n",
    "#         preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "#     )\n",
    "\n",
    "#     train_images = train_gen.flow_from_dataframe(train_df, x_col='Filepath', y_col='Label',\n",
    "#                                                  target_size=(224, 224), color_mode='rgb', class_mode='categorical',\n",
    "#                                                  batch_size=batch_size, shuffle=True, seed=0)\n",
    "#     val_images = val_test_gen.flow_from_dataframe(val_df, x_col='Filepath', y_col='Label',\n",
    "#                                                   target_size=(224, 224), color_mode='rgb', class_mode='categorical',\n",
    "#                                                   batch_size=batch_size, shuffle=False)\n",
    "#     test_images = val_test_gen.flow_from_dataframe(test_df, x_col='Filepath', y_col='Label',\n",
    "#                                                    target_size=(224, 224), color_mode='rgb', class_mode='categorical',\n",
    "#                                                    batch_size=batch_size, shuffle=False)\n",
    "#     return train_images, val_images, test_images\n",
    "\n",
    "# # Model-building function\n",
    "# def build_model(learning_rate=0.001):\n",
    "#     pretrained_model = tf.keras.applications.MobileNetV2(input_shape=(224, 224, 3), include_top=False, pooling='avg', weights='imagenet')\n",
    "#     pretrained_model.trainable = True\n",
    "#     for layer in pretrained_model.layers[:-50]:  # Fine-tune last 50 layers\n",
    "#         layer.trainable = False\n",
    "\n",
    "#     model = tf.keras.Sequential([\n",
    "#         pretrained_model,\n",
    "#         layers.Dense(256, activation='relu'),\n",
    "#         layers.BatchNormalization(),\n",
    "#         layers.Dropout(0.4),\n",
    "#         layers.Dense(128, activation='relu'),\n",
    "#         layers.BatchNormalization(),\n",
    "#         layers.Dropout(0.3),\n",
    "#         layers.Dense(len(train_df['Label'].unique()), activation='softmax')\n",
    "#     ])\n",
    "\n",
    "#     model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "#                   loss='categorical_crossentropy',\n",
    "#                   metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# # Hyperparameter grid\n",
    "# param_grid = {\n",
    "#     'learning_rate': [0.001, 0.005, 0.01, 0.02],\n",
    "#     'epochs': [10, 30, 50],\n",
    "#     'batch_size': [16, 32, 64, 128]\n",
    "# }\n",
    "\n",
    "# # Grid search\n",
    "# best_accuracy = 0\n",
    "# best_params = None\n",
    "\n",
    "# # Generate data once outside the loop for consistent train/val/test splits\n",
    "# train_images, val_images, test_images = create_data_generators(train_df, val_df, test_df, batch_size=32)\n",
    "\n",
    "# # Start timer\n",
    "# start_time = time.time()\n",
    "\n",
    "# # Hyperparameter tuning loop\n",
    "# for lr in param_grid['learning_rate']:\n",
    "#     for epochs in param_grid['epochs']:\n",
    "#         for batch_size in param_grid['batch_size']:\n",
    "#             print(f\"Training with lr={lr}, epochs={epochs}, batch_size={batch_size}\")\n",
    "\n",
    "#             # Re-create data generators for current batch_size\n",
    "#             train_images, val_images, test_images = create_data_generators(train_df, val_df, test_df, batch_size=batch_size)\n",
    "\n",
    "#             # Build and train model\n",
    "#             model = build_model(learning_rate=lr)\n",
    "#             history = model.fit(train_images, epochs=epochs, validation_data=val_images, verbose=0)\n",
    "\n",
    "#             # Evaluate on validation set\n",
    "#             val_loss, val_accuracy = model.evaluate(val_images, verbose=0)\n",
    "#             print(f\"Validation accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "#             # Track best parameters\n",
    "#             if val_accuracy > best_accuracy:\n",
    "#                 best_accuracy = val_accuracy\n",
    "#                 best_params = {'learning_rate': lr, 'epochs': epochs, 'batch_size': batch_size}\n",
    "\n",
    "# # End timer\n",
    "# end_time = time.time()\n",
    "# total_time = end_time - start_time\n",
    "\n",
    "# # Convert time to hours, minutes, seconds\n",
    "# hours, rem = divmod(total_time, 3600)\n",
    "# minutes, seconds = divmod(rem, 60)\n",
    "\n",
    "# print(f\"\\nGrid Search Completed!\")\n",
    "# print(f\"Total Time: {total_time:.2f} seconds ({int(hours)}h {int(minutes)}m {int(seconds)}s)\\n\")\n",
    "# print(f\"Best Hyperparameters: {best_params}\")\n",
    "# print(f\"Best Validation Accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "# # Final evaluation on test set\n",
    "# best_model = build_model(learning_rate=best_params['learning_rate'])\n",
    "# best_model.fit(train_images, epochs=best_params['epochs'], batch_size=best_params['batch_size'], verbose=0)\n",
    "# test_loss, test_acc = best_model.evaluate(test_images, verbose=0)\n",
    "# print(f\"Test Loss: {test_loss:.5f}, Test Accuracy: {test_acc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grid search for DT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time  # Import the time module\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Define parameter grid for Decision Tree\n",
    "dt_param_grid = {\n",
    "    'max_depth': [5, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "stat_feature_dir = r'E:\\Abroad period research\\Medical images analysis paper implementation codes\\Second part of the paper\\Results on just chest region segmented dataset\\statistical_features_just_for_time_Calculation'\n",
    "\n",
    "# Load the CSV files\n",
    "train_df = pd.read_csv(os.path.join(stat_feature_dir, \"train_stat_features.csv\"))\n",
    "\n",
    "# Separate features and labels\n",
    "train_stat_features = train_df.drop(columns=['label']).values\n",
    "train_labels = train_df['label'].values\n",
    "\n",
    "# Initialize Decision Tree Classifier\n",
    "dt_clf = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "# Use GridSearchCV for Decision Tree with a limited number of folds (e.g., 3-fold) to save time\n",
    "dt_grid_search = GridSearchCV(\n",
    "    estimator=dt_clf,\n",
    "    param_grid=dt_param_grid,\n",
    "    cv=3,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Track time before and after the grid search\n",
    "start_time = time.time()\n",
    "\n",
    "# Fit the grid search model on training data\n",
    "dt_grid_search.fit(train_stat_features, train_labels)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate total time taken in seconds\n",
    "total_time = end_time - start_time\n",
    "\n",
    "# Retrieve the best parameters and accuracy\n",
    "best_dt_params = dt_grid_search.best_params_\n",
    "best_dt_score = dt_grid_search.best_score_\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nBest Decision Tree Parameters: {best_dt_params}\")\n",
    "print(f\"Best Decision Tree Cross-Validation Accuracy: {best_dt_score:.4f}\")\n",
    "print(f\"\\nTotal time taken by Grid Search: {total_time:.2f} seconds\")\n",
    "\n",
    "# Optional: display time in minutes and seconds\n",
    "minutes = int(total_time // 60)\n",
    "seconds = int(total_time % 60)\n",
    "print(f\"Total time taken: {minutes} minute(s) and {seconds} second(s)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search Time for RuleFit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time  # <-- Added to track execution time\n",
    "from rulefit import RuleFit\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "stat_feature_dir = r'E:\\Abroad period research\\Medical images analysis paper implementation codes\\Second part of the paper\\Results on just chest region segmented dataset\\statistical_features_just_for_time_Calculation'\n",
    "\n",
    "# Load the CSV files\n",
    "train_df = pd.read_csv(os.path.join(stat_feature_dir, \"train_stat_features.csv\"))\n",
    "val_df = pd.read_csv(os.path.join(stat_feature_dir, \"val_stat_features.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(stat_feature_dir, \"test_stat_features.csv\"))\n",
    "\n",
    "# Extract feature names (column names except 'label')\n",
    "feature_names = train_df.columns.drop('label').tolist()\n",
    "\n",
    "# Separate features and labels\n",
    "train_stat_features = train_df.drop(columns=['label']).values\n",
    "train_labels = train_df['label'].values\n",
    "\n",
    "val_stat_features = val_df.drop(columns=['label']).values\n",
    "val_labels = val_df['label'].values\n",
    "\n",
    "test_stat_features = test_df.drop(columns=['label']).values\n",
    "test_labels = test_df['label'].values\n",
    "\n",
    "# Define parameter grid for RuleFit\n",
    "rulefit_param_grid = {\n",
    "    'tree_size': [3, 5, 7],\n",
    "    'sample_fract': [0.7, 0.8, 1.0],\n",
    "    'max_rules': [25, 100, 200, 500, 1000, 2000]\n",
    "}\n",
    "\n",
    "# Variables to store best parameters and score\n",
    "best_rf_params = None\n",
    "best_rf_score = 0\n",
    "\n",
    "# Start timing the grid search\n",
    "start_time = time.time()\n",
    "\n",
    "# Custom grid search for RuleFit\n",
    "for tree_size in rulefit_param_grid['tree_size']:\n",
    "    for sample_fract in rulefit_param_grid['sample_fract']:\n",
    "        for max_rules in rulefit_param_grid['max_rules']:\n",
    "            # Initialize RuleFit with the current set of parameters\n",
    "            rf = RuleFit(tree_size=tree_size, sample_fract=sample_fract, max_rules=max_rules, random_state=42)\n",
    "            \n",
    "            # Fit the model on training data\n",
    "            rf.fit(train_stat_features, train_labels, feature_names=feature_names)\n",
    "            \n",
    "            # Predict on the validation set\n",
    "            val_predictions = rf.predict(val_stat_features)\n",
    "            \n",
    "            # Convert continuous predictions to discrete labels\n",
    "            val_predictions_discrete = np.round(val_predictions).astype(int)\n",
    "            \n",
    "            # Clip predictions to ensure they stay within the label range\n",
    "            val_predictions_discrete = np.clip(val_predictions_discrete, np.min(train_labels), np.max(train_labels))\n",
    "            \n",
    "            # Calculate accuracy on the validation set\n",
    "            accuracy = accuracy_score(val_labels, val_predictions_discrete)\n",
    "            \n",
    "            # Print parameters and validation accuracy for current iteration\n",
    "            print(f\"Tree Size: {tree_size}, Sample Fract: {sample_fract}, Max Rules: {max_rules}, Validation Accuracy: {accuracy:.4f}\")\n",
    "            \n",
    "            # Update best score and parameters if the current is better\n",
    "            if accuracy > best_rf_score:\n",
    "                best_rf_score = accuracy\n",
    "                best_rf_params = {\n",
    "                    'tree_size': tree_size,\n",
    "                    'sample_fract': sample_fract,\n",
    "                    'max_rules': max_rules\n",
    "                }\n",
    "\n",
    "# End timing the grid search\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate total time taken in seconds\n",
    "total_time = end_time - start_time\n",
    "\n",
    "# Print best parameters and accuracy\n",
    "print(\"\\nBest RuleFit Parameters:\", best_rf_params)\n",
    "print(f\"Best RuleFit Validation Accuracy: {best_rf_score:.4f}\")\n",
    "\n",
    "# Print total time taken by the grid search\n",
    "print(f\"\\nTotal time taken by Grid Search: {total_time:.2f} seconds\")\n",
    "\n",
    "# Optional: display time in minutes and seconds\n",
    "minutes = int(total_time // 60)\n",
    "seconds = int(total_time % 60)\n",
    "print(f\"Total time taken: {minutes} minute(s) and {seconds} second(s)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RuleFit rule extraction Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time  # Added for timing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rulefit import RuleFit\n",
    "\n",
    "def simplify_rule(rule):\n",
    "    \"\"\"\n",
    "    Simplifies a rule string by removing logically redundant conditions\n",
    "    and ensuring the rule format is valid.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conditions = rule.split(\" & \")\n",
    "        parsed_conditions = []\n",
    "        for cond in conditions:\n",
    "            parts = cond.split()\n",
    "            if len(parts) < 3:\n",
    "                continue\n",
    "            feature, operator, value = parts[:3]\n",
    "            value = float(value)\n",
    "            parsed_conditions.append((feature, operator, value))\n",
    "\n",
    "        simplified_conditions = {}\n",
    "        for feature, operator, value in parsed_conditions:\n",
    "            if feature not in simplified_conditions:\n",
    "                simplified_conditions[feature] = (operator, value)\n",
    "            else:\n",
    "                current_operator, current_value = simplified_conditions[feature]\n",
    "                if operator == \">\" and value > current_value:\n",
    "                    simplified_conditions[feature] = (operator, value)\n",
    "                elif operator == \"<=\" and value < current_value:\n",
    "                    simplified_conditions[feature] = (operator, value)\n",
    "\n",
    "        return \" & \".join([f\"{feature} {operator} {value}\" for feature, (operator, value) in simplified_conditions.items()])\n",
    "    except Exception as e:\n",
    "        print(f\"Error simplifying rule: {rule}. Error: {e}\")\n",
    "        return rule\n",
    "\n",
    "# Start timer here!\n",
    "start_time = time.time()\n",
    "\n",
    "# Load Data\n",
    "input_dir = r'E:\\Abroad period research\\Medical images analysis paper implementation codes\\Second part of the paper\\26 features results\\3 best features'\n",
    "train_df = pd.read_csv(os.path.join(input_dir, \"3_training_selected_features.csv\"))\n",
    "val_df = pd.read_csv(os.path.join(input_dir, \"3_validation_selected_features.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(input_dir, \"3_testing_selected_features.csv\"))\n",
    "\n",
    "# Combine Training and Validation Data\n",
    "train_val_df = pd.concat([train_df, val_df], ignore_index=True)\n",
    "X_train_val = train_val_df.drop(columns=[\"label\"]).values\n",
    "y_train_val = train_val_df[\"label\"].values\n",
    "feature_names = train_val_df.columns[:-1].tolist()\n",
    "\n",
    "# Train RuleFit Model\n",
    "rulefit_model = RuleFit(tree_size=4, sample_fract=0.7, max_rules=200, random_state=42)\n",
    "rulefit_model.fit(X_train_val, y_train_val, feature_names=feature_names)\n",
    "\n",
    "# Extract Rules\n",
    "rules = rulefit_model.get_rules()\n",
    "rules = rules[rules.coef != 0]  # Filter rules with non-zero coefficients\n",
    "\n",
    "if rules.empty:\n",
    "    print(\"No rules were generated. Check your data or model configuration.\")\n",
    "else:\n",
    "    # Simplify Rules\n",
    "    rules[\"rule\"] = rules[\"rule\"].apply(simplify_rule)\n",
    "\n",
    "    # Get Unique Classes\n",
    "    unique_classes = sorted(set(y_train_val))\n",
    "\n",
    "    # Display and Save Rules\n",
    "    print(\"\\nSimplified Top Rules in If-Then Format for All Classes:\")\n",
    "    if_then_rules = []\n",
    "    \n",
    "    for label in unique_classes:\n",
    "        # Filter top rules for each class by importance scores and class association\n",
    "        class_rules = rules[rules.apply(lambda x: np.argmax(x['coef']) == label if isinstance(x['coef'], np.ndarray) else x['coef'] > 0, axis=1)]\n",
    "        class_rules = class_rules.sort_values(by=\"importance\", ascending=False).head(20)  # Top N rules per class\n",
    "\n",
    "        print(f\"\\nClass {label} Rules:\")\n",
    "        for _, row in class_rules.iterrows():\n",
    "            rule_str = f\"If ({row['rule']}) then Class = {label} (Importance: {row['importance']:.4f})\"\n",
    "            print(rule_str)\n",
    "            if_then_rules.append(rule_str)\n",
    "\n",
    "    # Save Rules to File\n",
    "    output_file_path = \"simplified_rulefit_top_if_then_rules_for_all_classes.txt\"\n",
    "    with open(output_file_path, 'w') as f:\n",
    "        for rule in if_then_rules:\n",
    "            f.write(rule + \"\\n\")\n",
    "    print(f\"\\nSimplified If-Then rules for all classes have been saved to {output_file_path}.\")\n",
    "\n",
    "# End timer here!\n",
    "end_time = time.time()\n",
    "total_time_seconds = end_time - start_time\n",
    "total_time_minutes = total_time_seconds / 60\n",
    "\n",
    "print(f\"\\n✅ Total time taken for RuleFit training, rule extraction, simplification, and saving: {total_time_seconds:.2f} seconds ({total_time_minutes:.2f} minutes).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree rules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time  # Added for measuring time\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "stat_feature_dir = r'E:\\Abroad period research\\Medical images analysis paper implementation codes\\Second part of the paper\\26 features results\\3 best features'\n",
    "\n",
    "# Load the CSV files\n",
    "train_df = pd.read_csv(os.path.join(stat_feature_dir, \"3_training_selected_features.csv\"))\n",
    "val_df = pd.read_csv(os.path.join(stat_feature_dir, \"3_validation_selected_features.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(stat_feature_dir, \"3_testing_selected_features.csv\"))\n",
    "\n",
    "# Separate features and labels\n",
    "train_stat_features = train_df.drop(columns=['label']).values\n",
    "train_labels = train_df['label'].values\n",
    "\n",
    "val_stat_features = val_df.drop(columns=['label']).values\n",
    "val_labels = val_df['label'].values\n",
    "\n",
    "test_stat_features = test_df.drop(columns=['label']).values\n",
    "test_labels = test_df['label'].values\n",
    "\n",
    "# Combine training and validation data for final training\n",
    "combined_features = np.vstack([train_stat_features, val_stat_features])\n",
    "combined_labels = np.hstack([train_labels, val_labels])\n",
    "\n",
    "# Start timing for decision tree training and rule extraction\n",
    "start_time = time.time()\n",
    "\n",
    "# Train Decision Tree Classifier with specified hyperparameters\n",
    "clf = DecisionTreeClassifier(\n",
    "    criterion=\"entropy\",\n",
    "    max_depth=10,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=4,\n",
    "    random_state=0\n",
    ")\n",
    "clf.fit(combined_features, combined_labels)\n",
    "\n",
    "# Extract rules in the form of \"If ... then ...\" with importance\n",
    "feature_names = list(train_df.columns[:-1])\n",
    "rules = []\n",
    "\n",
    "def traverse_tree(tree, feature_names, feature_importances, node=0, conditions=\"\", path_importance=0):\n",
    "    \"\"\"\n",
    "    Recursively traverse the decision tree to extract rules and calculate their importance.\n",
    "    \"\"\"\n",
    "    # Check if this is a leaf node\n",
    "    if tree.children_left[node] == -1 and tree.children_right[node] == -1:\n",
    "        # Leaf node, output the class prediction and importance\n",
    "        class_value = np.argmax(tree.value[node][0])\n",
    "        rule_importance = path_importance\n",
    "        rule = f\"If ({conditions.rstrip(' & ')}) then Class = {class_value} (Importance: {rule_importance:.4f})\"\n",
    "        rules.append(rule)\n",
    "    else:\n",
    "        # Internal node, calculate feature importance for this path\n",
    "        feature_index = tree.feature[node]\n",
    "        threshold = tree.threshold[node]\n",
    "\n",
    "        # Avoid invalid indices\n",
    "        if feature_index >= 0:\n",
    "            # Update path importance using the feature importance of the current feature\n",
    "            path_importance += feature_importances[feature_index]\n",
    "\n",
    "            # Left child (feature <= threshold)\n",
    "            left_conditions = conditions + f\"{feature_names[feature_index]} <= {threshold:.3f} & \"\n",
    "            traverse_tree(tree, feature_names, feature_importances, tree.children_left[node], left_conditions, path_importance)\n",
    "\n",
    "            # Right child (feature > threshold)\n",
    "            right_conditions = conditions + f\"{feature_names[feature_index]} > {threshold:.3f} & \"\n",
    "            traverse_tree(tree, feature_names, feature_importances, tree.children_right[node], right_conditions, path_importance)\n",
    "\n",
    "# Traverse the tree to extract rules\n",
    "try:\n",
    "    traverse_tree(clf.tree_, feature_names, clf.feature_importances_)\n",
    "except Exception as e:\n",
    "    print(f\"Error during tree traversal: {e}\")\n",
    "\n",
    "# End timing after rule extraction\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the total time taken\n",
    "total_time = end_time - start_time\n",
    "print(f\"\\nTotal time taken for Decision Tree training and rule extraction: {total_time:.4f} seconds\\n\")\n",
    "\n",
    "# Simplify a single rule by merging redundant conditions\n",
    "def simplify_rule(rule):\n",
    "    conditions_part, result_part = rule.split(\") then\")\n",
    "    conditions = conditions_part.replace(\"If (\", \"\").split(\" & \")\n",
    "    simplified_conditions = {}\n",
    "\n",
    "    # Extract feature and threshold for each condition\n",
    "    for condition in conditions:\n",
    "        feature, operator, threshold = condition.split(\" \")\n",
    "        threshold = float(threshold)\n",
    "\n",
    "        # Simplify by keeping the most restrictive range\n",
    "        if feature not in simplified_conditions:\n",
    "            simplified_conditions[feature] = {\"<=\": float('inf'), \">\": float('-inf')}\n",
    "        if operator == \"<=\":\n",
    "            simplified_conditions[feature][\"<=\"] = min(simplified_conditions[feature][\"<=\"], threshold)\n",
    "        elif operator == \">\":\n",
    "            simplified_conditions[feature][\">\"] = max(simplified_conditions[feature][\">\"], threshold)\n",
    "\n",
    "    # Reconstruct simplified conditions\n",
    "    final_conditions = []\n",
    "    for feature, thresholds in simplified_conditions.items():\n",
    "        if thresholds[\">\"] != float('-inf'):\n",
    "            final_conditions.append(f\"{feature} > {thresholds['>']:.3f}\")\n",
    "        if thresholds[\"<=\"] != float('inf'):\n",
    "            final_conditions.append(f\"{feature} <= {thresholds['<=']:.3f}\")\n",
    "\n",
    "    return f\"If ({' & '.join(final_conditions)}) then{result_part}\"\n",
    "\n",
    "# Simplify all rules\n",
    "simplified_rules = [simplify_rule(rule) for rule in rules]\n",
    "\n",
    "# Extract importance scores from simplified rules and sort them in descending order\n",
    "def extract_importance(rule):\n",
    "    importance_start = rule.find(\"(Importance: \") + len(\"(Importance: \")\n",
    "    importance_end = rule.find(\")\", importance_start)\n",
    "    return float(rule[importance_start:importance_end])\n",
    "\n",
    "# Sort rules by importance in descending order\n",
    "sorted_rules = sorted(simplified_rules, key=extract_importance, reverse=True)\n",
    "\n",
    "# Display sorted and simplified rules\n",
    "print(\"Simplified and Sorted Rules:\")\n",
    "for rule in sorted_rules:\n",
    "    print(rule)\n",
    "\n",
    "# Save sorted and simplified rules to a file\n",
    "output_file_path = r'E:\\Abroad period research\\Medical images analysis paper implementation codes\\Second part of the paper\\Ultrasound Breast Images for Breast Cancer\\26 features results\\3 best features\\simplified_sorted_rules.txt'\n",
    "with open(output_file_path, 'w') as f:\n",
    "    for rule in sorted_rules:\n",
    "        f.write(rule + \"\\n\")\n",
    "\n",
    "print(f\"\\nSimplified and sorted rules have been saved to {output_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
