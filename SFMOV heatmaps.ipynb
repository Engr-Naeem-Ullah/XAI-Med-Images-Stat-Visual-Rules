{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of skewness, mean and entropy on original images "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualization of folder of images without using masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from scipy.stats import skew\n",
    "from skimage.measure import shannon_entropy\n",
    "\n",
    "# Load your custom-trained model\n",
    "model_path = \"E:/Abroad period research/new idea implementation codes/Second part of the paper/10 features results/Mobilenetv2_finetuned_with_CLR_and_GradientAccum.h5\"\n",
    "loaded_model = load_model(model_path)\n",
    "\n",
    "# Identify the last convolutional layer and dense layer\n",
    "last_conv_layer_name = 'Conv_1_bn'  # Replace with the actual name of the last convolutional layer\n",
    "dense_layer_name = 'dense_1'  # Replace with the actual name of the dense layer\n",
    "conv_layer_model = Model(inputs=loaded_model.input, outputs=loaded_model.get_layer(last_conv_layer_name).output)\n",
    "dense_layer_model = Model(inputs=loaded_model.input, outputs=loaded_model.get_layer(dense_layer_name).output)\n",
    "\n",
    "# Define directories for images\n",
    "image_dir = \"E:/Abroad period research/new idea implementation codes/Second part of the paper/10 features results/dataset for visualization/images\"\n",
    "input_size = (224, 224)\n",
    "\n",
    "# Function to process a single image and visualize statistical features\n",
    "def process_image(image_path):\n",
    "    # Load and preprocess the image\n",
    "    original_image = load_img(image_path, target_size=input_size)\n",
    "    image_array = img_to_array(original_image) / 255.0\n",
    "    image_array = np.expand_dims(image_array, axis=0)\n",
    "\n",
    "    # Extract feature maps from the last convolutional layer\n",
    "    conv_features = conv_layer_model.predict(image_array)\n",
    "    conv_features = np.squeeze(conv_features)  # Shape: (H, W, C)\n",
    "\n",
    "    # Calculate statistical properties for each channel in the convolutional layer\n",
    "    mean_map, skewness_map, entropy_map = np.zeros(conv_features.shape[:2]), np.zeros(conv_features.shape[:2]), np.zeros(conv_features.shape[:2])\n",
    "    for i in range(conv_features.shape[-1]):\n",
    "        feature_map = conv_features[:, :, i]\n",
    "        mean_map += feature_map / conv_features.shape[-1]\n",
    "        skewness_map += skew(feature_map.ravel()) * feature_map\n",
    "        entropy_map += shannon_entropy(feature_map) * feature_map\n",
    "\n",
    "    # Normalize each map for better visualization\n",
    "    mean_map = (mean_map - mean_map.min()) / (mean_map.max() - mean_map.min() + 1e-8)\n",
    "    skewness_map = (skewness_map - skewness_map.min()) / (skewness_map.max() - skewness_map.min() + 1e-8)\n",
    "    entropy_map = (entropy_map - entropy_map.min()) / (entropy_map.max() - entropy_map.min() + 1e-8)\n",
    "\n",
    "    # Resize maps to match the original image size\n",
    "    mean_map_resized = cv2.resize(mean_map, (input_size[1], input_size[0]))\n",
    "    skewness_map_resized = cv2.resize(skewness_map, (input_size[1], input_size[0]))\n",
    "    entropy_map_resized = cv2.resize(entropy_map, (input_size[1], input_size[0]))\n",
    "\n",
    "    # Plot the original image with heatmap overlays\n",
    "    fig, ax = plt.subplots(1, 4, figsize=(18, 6))\n",
    "    ax[0].imshow(original_image)\n",
    "    ax[0].set_title('Original X-ray Image')\n",
    "    ax[1].imshow(original_image)\n",
    "    ax[1].imshow(mean_map_resized, cmap='jet', alpha=0.5)\n",
    "    ax[1].set_title('Mean Heatmap')\n",
    "    ax[2].imshow(original_image)\n",
    "    ax[2].imshow(skewness_map_resized, cmap='jet', alpha=0.5)\n",
    "    ax[2].set_title('Skewness Heatmap')\n",
    "    ax[3].imshow(original_image)\n",
    "    ax[3].imshow(entropy_map_resized, cmap='jet', alpha=0.5)\n",
    "    ax[3].set_title('Entropy Heatmap')\n",
    "\n",
    "    # Display the plot\n",
    "    for a in ax:\n",
    "        a.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Iterate over all images in the directory\n",
    "for image_file in os.listdir(image_dir):\n",
    "    image_path = os.path.join(image_dir, image_file)\n",
    "    print(f\"Processing {image_file}...\")\n",
    "    process_image(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from scipy.stats import skew\n",
    "from skimage.measure import shannon_entropy\n",
    "\n",
    "# Load your custom-trained model\n",
    "model_path = \"E:/Abroad period research/new idea implementation codes/Second part of the paper/10 features results/Mobilenetv2_finetuned_with_CLR_and_GradientAccum.h5\"\n",
    "loaded_model = load_model(model_path)\n",
    "\n",
    "# Identify the last convolutional layer and dense layer\n",
    "last_conv_layer_name = 'Conv_1_bn'  # Replace with the actual name of the last convolutional layer\n",
    "dense_layer_name = 'dense_1'  # Replace with the actual name of the dense layer\n",
    "conv_layer_model = Model(inputs=loaded_model.input, outputs=loaded_model.get_layer(last_conv_layer_name).output)\n",
    "dense_layer_model = Model(inputs=loaded_model.input, outputs=loaded_model.get_layer(dense_layer_name).output)\n",
    "\n",
    "# Define directories for images\n",
    "image_dir = \"E:/Abroad period research/new idea implementation codes/Second part of the paper/10 features results/dataset for visualization/images\"\n",
    "input_size = (224, 224)\n",
    "\n",
    "# Function to process a single image and visualize statistical features\n",
    "def process_image(image_path):\n",
    "    # Load and preprocess the image\n",
    "    original_image = load_img(image_path, target_size=input_size)\n",
    "    image_array = img_to_array(original_image) / 255.0\n",
    "    image_array = np.expand_dims(image_array, axis=0)\n",
    "\n",
    "    # Extract feature maps from the last convolutional layer\n",
    "    conv_features = conv_layer_model.predict(image_array)\n",
    "    conv_features = np.squeeze(conv_features)  # Shape: (H, W, C)\n",
    "\n",
    "    # Calculate statistical properties for each channel in the convolutional layer\n",
    "    mean_map, skewness_map, entropy_map = np.zeros(conv_features.shape[:2]), np.zeros(conv_features.shape[:2]), np.zeros(conv_features.shape[:2])\n",
    "    for i in range(conv_features.shape[-1]):\n",
    "        feature_map = conv_features[:, :, i]\n",
    "        mean_map += feature_map / conv_features.shape[-1]\n",
    "        skewness_map += skew(feature_map.ravel()) * feature_map\n",
    "        entropy_map += shannon_entropy(feature_map) * feature_map\n",
    "\n",
    "    # Normalize each map for better visualization\n",
    "    mean_map = (mean_map - mean_map.min()) / (mean_map.max() - mean_map.min() + 1e-8)\n",
    "    skewness_map = (skewness_map - skewness_map.min()) / (skewness_map.max() - skewness_map.min() + 1e-8)\n",
    "    entropy_map = (entropy_map - entropy_map.min()) / (entropy_map.max() - entropy_map.min() + 1e-8)\n",
    "\n",
    "    # Resize maps to match the original image size\n",
    "    mean_map_resized = cv2.resize(mean_map, (input_size[1], input_size[0]))\n",
    "    skewness_map_resized = cv2.resize(skewness_map, (input_size[1], input_size[0]))\n",
    "    entropy_map_resized = cv2.resize(entropy_map, (input_size[1], input_size[0]))\n",
    "\n",
    "    # Extract dense layer activations\n",
    "    dense_features = dense_layer_model.predict(image_array).squeeze()\n",
    "    dense_mean = np.mean(dense_features)\n",
    "    dense_skewness = skew(dense_features)\n",
    "    dense_entropy = shannon_entropy(dense_features)\n",
    "\n",
    "    # Apply dense layer statistical features as weights to convolutional maps\n",
    "    combined_map = (\n",
    "        dense_mean * mean_map_resized + \n",
    "        dense_skewness * skewness_map_resized + \n",
    "        dense_entropy * entropy_map_resized\n",
    "    )\n",
    "    combined_map = (combined_map - combined_map.min()) / (combined_map.max() - combined_map.min() + 1e-8)  # Normalize\n",
    "\n",
    "    # Plot the original image with heatmap overlays\n",
    "    fig, ax = plt.subplots(1, 5, figsize=(22, 6))\n",
    "    ax[0].imshow(original_image)\n",
    "    ax[0].set_title('Original Image')\n",
    "    ax[1].imshow(original_image)\n",
    "    ax[1].imshow(mean_map_resized, cmap='jet', alpha=0.5)\n",
    "    ax[1].set_title('Mean Heatmap')\n",
    "    ax[2].imshow(original_image)\n",
    "    ax[2].imshow(skewness_map_resized, cmap='jet', alpha=0.5)\n",
    "    ax[2].set_title('Skewness Heatmap')\n",
    "    ax[3].imshow(original_image)\n",
    "    ax[3].imshow(entropy_map_resized, cmap='jet', alpha=0.5)\n",
    "    ax[3].set_title('Entropy Heatmap')\n",
    "    ax[4].imshow(original_image)\n",
    "    ax[4].imshow(combined_map, cmap='jet', alpha=0.5)\n",
    "    ax[4].set_title('Combined Heatmap (Dense-weighted)')\n",
    "\n",
    "    # Display the plot\n",
    "    for a in ax:\n",
    "        a.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Iterate over all images in the directory\n",
    "for image_file in os.listdir(image_dir):\n",
    "    image_path = os.path.join(image_dir, image_file)\n",
    "    print(f\"Processing {image_file}...\")\n",
    "    process_image(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from scipy.stats import skew\n",
    "from skimage.measure import shannon_entropy\n",
    "\n",
    "# Load your custom-trained model\n",
    "model_path = \"E:/Abroad period research/new idea implementation codes/Second part of the paper/10 features results/Mobilenetv2_finetuned_with_CLR_and_GradientAccum.h5\"\n",
    "loaded_model = load_model(model_path)\n",
    "\n",
    "# Identify the last convolutional layer and dense layer\n",
    "last_conv_layer_name = 'Conv_1_bn'  # Replace with the actual name of the last convolutional layer\n",
    "dense_layer_name = 'dense_1'  # Replace with the actual name of the dense layer\n",
    "conv_layer_model = Model(inputs=loaded_model.input, outputs=loaded_model.get_layer(last_conv_layer_name).output)\n",
    "\n",
    "# Define directories for images and masks\n",
    "image_dir = \"E:/Abroad period research/new idea implementation codes/Second part of the paper/10 features results/dataset for visualization/images\"\n",
    "mask_dir = \"E:/Abroad period research/new idea implementation codes/Second part of the paper/10 features results/dataset for visualization/masks\"\n",
    "input_size = (224, 224)\n",
    "\n",
    "# Function to process a single image and visualize statistical features with mask overlay\n",
    "def process_image(image_path, mask_path):\n",
    "    # Load and preprocess the image\n",
    "    original_image = load_img(image_path, target_size=input_size)\n",
    "    image_array = img_to_array(original_image) / 255.0\n",
    "    image_array = np.expand_dims(image_array, axis=0)\n",
    "\n",
    "    # Load and preprocess the corresponding mask\n",
    "    mask = load_img(mask_path, target_size=input_size, color_mode=\"grayscale\")\n",
    "    mask_array = img_to_array(mask) / 255.0\n",
    "    mask_array = np.squeeze(mask_array)  # Shape: (H, W)\n",
    "\n",
    "    # Extract feature maps from the last convolutional layer\n",
    "    conv_features = conv_layer_model.predict(image_array)\n",
    "    conv_features = np.squeeze(conv_features)  # Shape: (H, W, C)\n",
    "\n",
    "    # Calculate statistical properties for each channel in the convolutional layer\n",
    "    mean_map, skewness_map, entropy_map = np.zeros(conv_features.shape[:2]), np.zeros(conv_features.shape[:2]), np.zeros(conv_features.shape[:2])\n",
    "    for i in range(conv_features.shape[-1]):\n",
    "        feature_map = conv_features[:, :, i]\n",
    "        mean_map += feature_map / conv_features.shape[-1]\n",
    "        skewness_map += skew(feature_map.ravel()) * feature_map\n",
    "        entropy_map += shannon_entropy(feature_map) * feature_map\n",
    "\n",
    "    # Normalize each map for better visualization\n",
    "    mean_map = (mean_map - mean_map.min()) / (mean_map.max() - mean_map.min() + 1e-8)\n",
    "    skewness_map = (skewness_map - skewness_map.min()) / (skewness_map.max() - skewness_map.min() + 1e-8)\n",
    "    entropy_map = (entropy_map - entropy_map.min()) / (entropy_map.max() - entropy_map.min() + 1e-8)\n",
    "\n",
    "    # Resize maps to match the original image size\n",
    "    mean_map_resized = cv2.resize(mean_map, (input_size[1], input_size[0])) * mask_array\n",
    "    skewness_map_resized = cv2.resize(skewness_map, (input_size[1], input_size[0])) * mask_array\n",
    "    entropy_map_resized = cv2.resize(entropy_map, (input_size[1], input_size[0])) * mask_array\n",
    "\n",
    "    # Plot the original image with heatmap overlays\n",
    "    fig, ax = plt.subplots(1, 4, figsize=(18, 6))\n",
    "    ax[0].imshow(original_image)\n",
    "    ax[0].set_title('Original X-ray Image')\n",
    "    ax[1].imshow(original_image)\n",
    "    ax[1].imshow(mean_map_resized, cmap='jet', alpha=0.5)\n",
    "    ax[1].set_title('Mean Heatmap')\n",
    "    ax[2].imshow(original_image)\n",
    "    ax[2].imshow(skewness_map_resized, cmap='jet', alpha=0.5)\n",
    "    ax[2].set_title('Skewness Heatmap')\n",
    "    ax[3].imshow(original_image)\n",
    "    ax[3].imshow(entropy_map_resized, cmap='jet', alpha=0.5)\n",
    "    ax[3].set_title('Entropy Heatmap')\n",
    "\n",
    "    # Display the plot\n",
    "    for a in ax:\n",
    "        a.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Iterate over all images in the directory and find corresponding masks\n",
    "for image_file in os.listdir(image_dir):\n",
    "    image_path = os.path.join(image_dir, image_file)\n",
    "    mask_path = os.path.join(mask_dir, image_file)  # Assuming masks have the same names as images\n",
    "    if os.path.exists(mask_path):\n",
    "        print(f\"Processing {image_file}...\")\n",
    "        process_image(image_path, mask_path)\n",
    "    else:\n",
    "        print(f\"Mask for {image_file} not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from scipy.stats import skew\n",
    "from skimage.measure import shannon_entropy\n",
    "\n",
    "# Load your custom-trained model\n",
    "model_path = \"E:/Abroad period research/new idea implementation codes/Second part of the paper/10 features results/Mobilenetv2_finetuned_with_CLR_and_GradientAccum.h5\"\n",
    "loaded_model = load_model(model_path)\n",
    "\n",
    "# Identify the last convolutional layer and dense layer\n",
    "last_conv_layer_name = 'Conv_1_bn'  # Replace with the actual name of the last convolutional layer\n",
    "dense_layer_name = 'dense_1'  # Replace with the actual name of the dense layer\n",
    "\n",
    "# Models to extract convolutional and dense layer features\n",
    "conv_layer_model = Model(inputs=loaded_model.input, outputs=loaded_model.get_layer(last_conv_layer_name).output)\n",
    "dense_layer_model = Model(inputs=loaded_model.input, outputs=loaded_model.get_layer(dense_layer_name).output)\n",
    "\n",
    "# Define directories for images and masks\n",
    "image_dir = \"E:/Abroad period research/new idea implementation codes/Second part of the paper/10 features results/Viral Pneumonia for visualization/Images\"\n",
    "mask_dir = \"E:/Abroad period research/new idea implementation codes/Second part of the paper/10 features results/Viral Pneumonia for visualization/Masks\"\n",
    "input_size = (224, 224)\n",
    "\n",
    "output_dir = \"E:/Abroad period research/new idea implementation codes/Second part of the paper/10 features results/Visualization output_images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Function to process a single image and visualize statistical features with mask overlay\n",
    "def process_image(image_path, mask_path):\n",
    "    # Load and preprocess the image\n",
    "    original_image = load_img(image_path, target_size=input_size)\n",
    "    image_array = img_to_array(original_image) / 255.0\n",
    "    image_array = np.expand_dims(image_array, axis=0)\n",
    "\n",
    "    # Load and preprocess the corresponding mask\n",
    "    mask = load_img(mask_path, target_size=input_size, color_mode=\"grayscale\")\n",
    "    mask_array = img_to_array(mask) / 255.0\n",
    "    mask_array = np.squeeze(mask_array)  # Shape: (H, W)\n",
    "\n",
    "    # Extract feature maps from the last convolutional layer\n",
    "    conv_features = conv_layer_model.predict(image_array)\n",
    "    conv_features = np.squeeze(conv_features)  # Shape: (H, W, C)\n",
    "\n",
    "    # Extract dense layer activations and calculate statistics\n",
    "    dense_features = dense_layer_model.predict(image_array)\n",
    "    dense_mean = np.mean(dense_features)\n",
    "    dense_skewness = skew(dense_features.ravel())\n",
    "    dense_entropy = shannon_entropy(dense_features)\n",
    "\n",
    "    # Calculate statistical properties for each channel in the convolutional layer\n",
    "    mean_map, skewness_map, entropy_map = np.zeros(conv_features.shape[:2]), np.zeros(conv_features.shape[:2]), np.zeros(conv_features.shape[:2])\n",
    "    for i in range(conv_features.shape[-1]):\n",
    "        feature_map = conv_features[:, :, i]\n",
    "        mean_map += feature_map / conv_features.shape[-1]\n",
    "        skewness_map += skew(feature_map.ravel()) * feature_map\n",
    "        entropy_map += shannon_entropy(feature_map) * feature_map\n",
    "\n",
    "    # Normalize each map for better visualization\n",
    "    mean_map = (mean_map - mean_map.min()) / (mean_map.max() - mean_map.min() + 1e-8)\n",
    "    skewness_map = (skewness_map - skewness_map.min()) / (skewness_map.max() - skewness_map.min() + 1e-8)\n",
    "    entropy_map = (entropy_map - entropy_map.min()) / (entropy_map.max() - entropy_map.min() + 1e-8)\n",
    "\n",
    "    # Calculate combined map based on dense layer statistics as weights\n",
    "    combined_map = (dense_mean * mean_map + dense_skewness * skewness_map + dense_entropy * entropy_map) / (dense_mean + dense_skewness + dense_entropy)\n",
    "    combined_map = (combined_map - combined_map.min()) / (combined_map.max() - combined_map.min() + 1e-8)\n",
    "\n",
    "    # Resize maps to match the original image size and apply mask\n",
    "    mean_map_resized = cv2.resize(mean_map, (input_size[1], input_size[0])) * mask_array\n",
    "    skewness_map_resized = cv2.resize(skewness_map, (input_size[1], input_size[0])) * mask_array\n",
    "    entropy_map_resized = cv2.resize(entropy_map, (input_size[1], input_size[0])) * mask_array\n",
    "    combined_map_resized = cv2.resize(combined_map, (input_size[1], input_size[0])) * mask_array\n",
    "\n",
    "    # Plot the original image with heatmap overlays\n",
    "    fig, ax = plt.subplots(1, 5, figsize=(22, 6))\n",
    "    ax[0].imshow(original_image)\n",
    "    ax[0].set_title('Original Image')\n",
    "    ax[1].imshow(original_image)\n",
    "    ax[1].imshow(mean_map_resized, cmap='jet', alpha=0.5)\n",
    "    ax[1].set_title('Mean Heatmap')\n",
    "    ax[2].imshow(original_image)\n",
    "    ax[2].imshow(skewness_map_resized, cmap='jet', alpha=0.5)\n",
    "    ax[2].set_title('Skewness Heatmap')\n",
    "    ax[3].imshow(original_image)\n",
    "    ax[3].imshow(entropy_map_resized, cmap='jet', alpha=0.5)\n",
    "    ax[3].set_title('Entropy Heatmap')\n",
    "    ax[4].imshow(original_image)\n",
    "    ax[4].imshow(combined_map_resized, cmap='jet', alpha=0.5)\n",
    "    ax[4].set_title('Combined Heatmap')\n",
    "\n",
    "# Save the plot to the output directory\n",
    "    image_name = os.path.basename(image_path).split('.')[0]\n",
    "    output_path = os.path.join(output_dir, f\"{image_name}_heatmaps.png\")\n",
    "    plt.savefig(output_path, bbox_inches='tight')\n",
    "    # Display the plot\n",
    "    for a in ax:\n",
    "        a.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Iterate over all images in the directory and find corresponding masks\n",
    "for image_file in os.listdir(image_dir):\n",
    "    image_path = os.path.join(image_dir, image_file)\n",
    "    mask_path = os.path.join(mask_dir, image_file)  # Assuming masks have the same names as images\n",
    "    if os.path.exists(mask_path):\n",
    "        print(f\"Processing {image_file}...\")\n",
    "        process_image(image_path, mask_path)\n",
    "    else:\n",
    "        print(f\"Mask for {image_file} not found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
